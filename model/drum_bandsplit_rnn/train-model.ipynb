{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b2a3bb-3d01-4f42-bc37-dff46522af7f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d686e6-8847-4861-88f0-58141c363862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "import plotly.graph_objects as go\n",
    "from torch.optim import lr_scheduler\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "import typing as tp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c82506-04c8-4200-9e63-26857aa176bd",
   "metadata": {},
   "source": [
    "# Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3acd3b-d177-44d3-a3f4-35140f45e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 3407\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182916a-aec1-4c05-8a08-4d2e44dfd979",
   "metadata": {},
   "source": [
    "# Construct Teh Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe5ef07-9ab9-4e3e-9334-048f67ae25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/Github/phd-drum-sep/Data/musdb18hq/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e101d3bb-e001-42ff-90cc-42384739d506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52ab99f-41d2-4b7f-a179-90681a8a0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(os.listdir(path+'train'))\n",
    "test = list(os.listdir(path+'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21aa39ee-b25f-4bdc-950a-9389eb93765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['drum', 'bass', 'other', 'vocals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07f7f9c-2d6b-42b9-b5ac-0f4bd9aeb6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 6372.87it/s]\n"
     ]
    }
   ],
   "source": [
    "all_scenes = {}\n",
    "counter = 0\n",
    "for idx, val in tqdm(enumerate(train)):\n",
    "    p = path + 'train/' + val + \"/\"\n",
    "    info = torchaudio.info(f\"{p}mixture.wav\")\n",
    "    seconds = info.num_frames // 44100\n",
    "    for i in range(0, seconds - 10, 10):\n",
    "        start_point = i * 44100\n",
    "        if start_point + (44100 * 10) < info.num_frames:\n",
    "            all_scenes[counter] = {'music_path': p, 'start_point': start_point, 'length': (44100 * 10), 'frames' : info.num_frames}\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d4eb9a-f08d-4452-a4ad-b9769a218278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_transcription_into_roll(transcription, frames):\n",
    "    # Determine your sampling frequency (frames per second)\n",
    "    fs = 44100\n",
    "    \n",
    "    piano_roll_length = int(frames)\n",
    "    \n",
    "    # Initialize the piano roll array\n",
    "    piano_roll = np.zeros((64, piano_roll_length))\n",
    "    \n",
    "    # Fill in the piano roll array\n",
    "    for note in transcription.instruments[0].notes:\n",
    "        # Convert start and end times to frame indices\n",
    "        start_frame = int(np.floor(note.start * fs))\n",
    "        end_frame = int(np.ceil(note.end * fs))\n",
    "        \n",
    "        # Set the corresponding frames to 1 (or note.velocity for a velocity-sensitive representation)\n",
    "        piano_roll[note.pitch, start_frame:end_frame] = 1  # Or use note.velocity\n",
    "        \n",
    "    roll = np.vstack([piano_roll[35:36, :], piano_roll[38:39, :], piano_roll[42:43, :], piano_roll[47:48, :], piano_roll[49:50, :]])\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfcdd0e-dc46-483f-b427-8e0657fae5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAD:\n",
    "    \"\"\"\n",
    "    SAD(Source Activity Detector)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            sr: int,\n",
    "            window_size_in_sec: int = 6,\n",
    "            overlap_ratio: float = 0.5,\n",
    "            n_chunks_per_segment: int = 10,\n",
    "            eps: float = 1e-5,\n",
    "            gamma: float = 1e-3,\n",
    "            threshold_max_quantile: float = 0.15,\n",
    "            threshold_segment: float = 0.5,\n",
    "    ):\n",
    "        self.sr = sr\n",
    "        self.n_chunks_per_segment = n_chunks_per_segment\n",
    "        self.eps = eps\n",
    "        self.gamma = gamma\n",
    "        self.threshold_max_quantile = threshold_max_quantile\n",
    "        self.threshold_segment = threshold_segment\n",
    "\n",
    "        self.window_size = sr * window_size_in_sec\n",
    "        self.step_size = int(self.window_size * overlap_ratio)\n",
    "\n",
    "    def chunk(self, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input shape: [n_channels, n_frames]\n",
    "        Output shape: []\n",
    "        \"\"\"\n",
    "        y = y.unfold(-1, self.window_size, self.step_size)\n",
    "        y = y.chunk(self.n_chunks_per_segment, dim=-1)\n",
    "        y = torch.stack(y, dim=-2)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_rms(y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        y = torch.mean(torch.square(y), dim=-1, keepdim=True)\n",
    "        y = torch.sqrt(y)\n",
    "        return y\n",
    "\n",
    "    def calculate_thresholds(self, rms: torch.Tensor):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        rms[rms == 0.] = self.eps\n",
    "        rms_threshold = torch.quantile(\n",
    "            rms,\n",
    "            self.threshold_max_quantile,\n",
    "            dim=-2,\n",
    "            keepdim=True,\n",
    "        )\n",
    "        rms_threshold[rms_threshold < self.gamma] = self.gamma\n",
    "        rms_percentage = torch.mean(\n",
    "            (rms > rms_threshold).float(),\n",
    "            dim=-2,\n",
    "            keepdim=True,\n",
    "        )\n",
    "        rms_mask = torch.all(rms_percentage > self.threshold_segment, dim=0).squeeze()\n",
    "        return rms_mask\n",
    "\n",
    "    def calculate_salient(self, y: torch.Tensor, mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        y = y[:, mask, ...]\n",
    "        C, D1, D2, D3 = y.shape\n",
    "        y = y.view(C, D1, D2*D3)\n",
    "        return y\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            y: torch.Tensor,\n",
    "            segment_saliency_mask: tp.Optional[torch.Tensor] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Stacks signal into segments and filters out silent segments.\n",
    "        :param y: Input signal.\n",
    "            Shape [n_channels, n_frames]\n",
    "               segment_saliency_mask: Optional precomputed mask\n",
    "            Shape [n_channels, n_segments, 1, 1]\n",
    "        :return: Salient signal folded into segments of length 'self.window_size' and step 'self.step_size'.\n",
    "            Shape [n_channels, n_segments, frames_in_segment]\n",
    "        \"\"\"\n",
    "        y = self.chunk(y)\n",
    "        rms = self.calculate_rms(y)\n",
    "        if segment_saliency_mask is None:\n",
    "            segment_saliency_mask = self.calculate_thresholds(rms)\n",
    "        y_salient = self.calculate_salient(y, segment_saliency_mask)\n",
    "        return y_salient, segment_saliency_mask\n",
    "\n",
    "    def calculate_salient_indices(\n",
    "            self,\n",
    "            y: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns start indices of salient regions of audio\n",
    "        \"\"\"\n",
    "        y = self.chunk(y)\n",
    "        rms = self.calculate_rms(y)\n",
    "        mask = self.calculate_thresholds(rms)\n",
    "        indices = torch.arange(mask.shape[-1])[mask] * self.step_size\n",
    "        return indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988d30-a95e-4dca-ac19-a6f7cea95717",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d11190-4e68-41f8-ae19-36d8199c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataGenerator(Dataset):\n",
    "    def __init__(self, data, sample_rate=HDEMUCS_HIGH_MUSDB.sample_rate, segment_length = 10):\n",
    "        self.data = data\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length = sample_rate * segment_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def load_audio(self, path, start_point, filename):\n",
    "        audio_tensors = []\n",
    "        file = filename\n",
    "        segment, _ = torchaudio.load(f\"{path}/{file}\", frame_offset=start_point, num_frames=self.segment_length)\n",
    "        audio_tensors.append(segment)\n",
    "        return torch.cat(audio_tensors, dim=0)\n",
    "\n",
    "    def load_roll(self, path, start_point, frames):\n",
    "        midi = path + '/mixture.wav.mid'\n",
    "        transcription = pretty_midi.PrettyMIDI(midi)\n",
    "        roll = turn_transcription_into_roll(transcription, frames)\n",
    "        # print(roll.shape)\n",
    "        roll = roll[:, start_point: start_point + self.segment_length]\n",
    "        return torch.from_numpy(roll).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # Load audio as a tensor\n",
    "        audio_path = sample['music_path']\n",
    "\n",
    "        start_point = sample['start_point']\n",
    "\n",
    "        mixture_tensor = self.load_audio(audio_path, start_point,'mixture.wav')\n",
    "        drum_tensor = self.load_audio(audio_path, start_point,'drums.wav')\n",
    "        roll_tensor = self.load_roll(audio_path, start_point, sample['frames'])\n",
    "        return mixture_tensor, drum_tensor, roll_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a74ca-f7f7-4c32-a229-8d43faf5e43f",
   "metadata": {},
   "source": [
    "## Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a521574-8b4f-4f85-88be-f97920050fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data, batch_size=32, num_workers=0, persistent_workers=False, shuffle=False):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.persistent_workers=persistent_workers\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Split your data here if necessary, e.g., into train, validation, test\n",
    "        self.dataset = AudioDataGenerator(self.data)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=self.shuffle, num_workers = self.num_workers, persistent_workers=self.persistent_workers)\n",
    "\n",
    "    # Implement val_dataloader() and test_dataloader() if you have validation and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c90603-3461-41ad-8707-28630df0c7ed",
   "metadata": {},
   "source": [
    "# making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01e086db-1c3c-4654-9487-3c66a8a8e9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN submodule of BandSequence module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim_size: int,\n",
    "            hidden_dim_size: int,\n",
    "            rnn_type: str = 'lstm',\n",
    "            bidirectional: bool = True\n",
    "    ):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.groupnorm = nn.GroupNorm(input_dim_size, input_dim_size)\n",
    "        self.rnn = getattr(nn, rnn_type)(\n",
    "            input_dim_size, hidden_dim_size, batch_first=True, bidirectional=bidirectional\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dim_size * 2 if bidirectional else hidden_dim_size,\n",
    "            input_dim_size\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Input shape:\n",
    "            across T - [batch_size, k_subbands, time, n_features]\n",
    "            OR\n",
    "            across K - [batch_size, time, k_subbands, n_features]\n",
    "        \"\"\"\n",
    "        B, K, T, N = x.shape  # across T      across K (keep in mind T->K, K->T)\n",
    "\n",
    "        out = x.view(B * K, T, N)  # [BK, T, N]    [BT, K, N]\n",
    "\n",
    "        out = self.groupnorm(\n",
    "            out.transpose(-1, -2)\n",
    "        ).transpose(-1, -2)  # [BK, T, N]    [BT, K, N]\n",
    "        out = self.rnn(out)[0]  # [BK, T, H]    [BT, K, H]\n",
    "        out = self.fc(out)  # [BK, T, N]    [BT, K, N]\n",
    "\n",
    "        x = out.view(B, K, T, N) + x  # [B, K, T, N]  [B, T, K, N]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  # [B, T, K, N]  [B, K, T, N]\n",
    "        return x\n",
    "\n",
    "\n",
    "class BandSequenceModelModule(nn.Module):\n",
    "    \"\"\"\n",
    "    BandSequence (2nd) Module of BandSplitRNN.\n",
    "    Runs input through n BiLSTMs in two dimensions - time and subbands.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim_size: int,\n",
    "            hidden_dim_size: int,\n",
    "            rnn_type: str = 'lstm',\n",
    "            bidirectional: bool = True,\n",
    "            num_layers: int = 12,\n",
    "    ):\n",
    "        super(BandSequenceModelModule, self).__init__()\n",
    "\n",
    "        self.bsrnn = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            rnn_across_t = RNNModule(\n",
    "                input_dim_size, hidden_dim_size, rnn_type, bidirectional\n",
    "            )\n",
    "            rnn_across_k = RNNModule(\n",
    "                input_dim_size, hidden_dim_size, rnn_type, bidirectional\n",
    "            )\n",
    "            self.bsrnn.append(\n",
    "                nn.Sequential(rnn_across_t, rnn_across_k)\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input shape: [batch_size, k_subbands, time, n_features]\n",
    "        Output shape: [batch_size, k_subbands, time, n_features]\n",
    "        \"\"\"\n",
    "        for i in range(len(self.bsrnn)):\n",
    "            x = self.bsrnn[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e5408c-1264-4400-8e14-d8fb7eb78d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandSplitModule(nn.Module):\n",
    "    \"\"\"\n",
    "    BandSplit (1st) Module of BandSplitRNN.\n",
    "    Separates input in k subbands and runs through LayerNorm+FC layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            sr: int,\n",
    "            n_fft: int,\n",
    "            bandsplits: tp.List[tp.Tuple[int, int]],\n",
    "            t_timesteps: int = 517,\n",
    "            fc_dim: int = 128,\n",
    "            complex_as_channel: bool = True,\n",
    "            is_mono: bool = False,\n",
    "    ):\n",
    "        super(BandSplitModule, self).__init__()\n",
    "\n",
    "        frequency_mul = 1\n",
    "        if complex_as_channel:\n",
    "            frequency_mul *= 2\n",
    "        if not is_mono:\n",
    "            frequency_mul *= 2\n",
    "\n",
    "        self.cac = complex_as_channel\n",
    "        self.is_mono = is_mono\n",
    "        self.bandwidth_indices = freq2bands(bandsplits, sr, n_fft)\n",
    "        self.layernorms = nn.ModuleList([\n",
    "            nn.LayerNorm([(e - s) * frequency_mul, t_timesteps])\n",
    "            for s, e in self.bandwidth_indices\n",
    "        ])\n",
    "        self.fcs = nn.ModuleList([\n",
    "            nn.Linear((e - s) * frequency_mul, fc_dim)\n",
    "            for s, e in self.bandwidth_indices\n",
    "        ])\n",
    "\n",
    "    def generate_subband(\n",
    "            self,\n",
    "            x: torch.Tensor\n",
    "    ) -> tp.Iterator[torch.Tensor]:\n",
    "        for start_index, end_index in self.bandwidth_indices:\n",
    "            yield x[:, :, start_index:end_index]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input: [batch_size, n_channels, freq, time]\n",
    "        Output: [batch_size, k_subbands, time, fc_output_shape]\n",
    "        \"\"\"\n",
    "        xs = []\n",
    "        for i, x in enumerate(self.generate_subband(x)):\n",
    "            B, C, F, T = x.shape\n",
    "            # view complex as channels\n",
    "            if x.dtype == torch.cfloat:\n",
    "                x = torch.view_as_real(x).permute(0, 1, 4, 2, 3)\n",
    "            # from channels to frequency\n",
    "            x = x.reshape(B, -1, T)\n",
    "            # run through model\n",
    "            x = self.layernorms[i](x)\n",
    "            x = x.transpose(-1, -2)\n",
    "            x = self.fcs[i](x)\n",
    "            xs.append(x)\n",
    "        return torch.stack(xs, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d26ae3fa-7686-4201-87dd-27863cd3cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer module based on Dual-Path Transformer paper [1].\n",
    "    Almost the same as in https://github.com/asteroid-team/asteroid/blob/master/asteroid/masknn/attention.py\n",
    "\n",
    "    References\n",
    "        [1] Chen, Jingjing, Qirong Mao, and Dong Liu. \"Dual-Path Transformer\n",
    "        Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation.\"\n",
    "        arXiv (2020).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim: int = 128,\n",
    "            dim_ff: int = 512,\n",
    "            n_heads: int = 4,\n",
    "            dropout: float = 0.0,\n",
    "            bidirectional: bool = True,\n",
    "    ):\n",
    "        super(TransformerModule, self).__init__()\n",
    "\n",
    "        self.groupnorm = nn.GroupNorm(embed_dim, embed_dim)\n",
    "        self.mha = MultiheadAttention(embed_dim, n_heads, dropout=dropout)\n",
    "        self.recurrent = nn.LSTM(embed_dim, dim_ff, bidirectional=bidirectional, batch_first=True)\n",
    "        self.linear = nn.Linear(\n",
    "            2 * dim_ff if bidirectional else dim_ff,\n",
    "            embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input shape:\n",
    "            across T - [batch_size, k_subbands, time, n_features]\n",
    "            OR\n",
    "            across K - [batch_size, time, k_subbands, n_features]\n",
    "        \"\"\"\n",
    "        B, K, T, N = x.shape  # across T, across K - keep in mind T->K, K->T\n",
    "\n",
    "        x = x.view(B * K, T, N)  # [BK, T, N] across T,      [BT, K, N] across K\n",
    "\n",
    "        # groupnorm\n",
    "        out = self.groupnorm(\n",
    "            x.transpose(-1, -2)\n",
    "        ).transpose(-1, -2)  # [BK, T, N]    [BT, K, N]\n",
    "\n",
    "        # Attention\n",
    "        mha_in = x.transpose(0, 1)\n",
    "        mha_out, _ = self.mha(mha_in, mha_in, mha_in)\n",
    "        x = mha_out.transpose(0, 1) + x\n",
    "\n",
    "        # RNN\n",
    "        rnn_out, _ = self.recurrent(x)\n",
    "        x = self.linear(rnn_out) + x\n",
    "\n",
    "        # returning to the initial shape\n",
    "        x = x.view(B, K, T, N)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BandTransformerModelModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Modified BandSequence (2nd) Module of BandSplitRNN.\n",
    "    Runs input through n Transformers in two dimensions - time and subbands.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim_size: int,\n",
    "            hidden_dim_size: int,\n",
    "            num_layers: int = 6,\n",
    "    ):\n",
    "        super(BandTransformerModelModule, self).__init__()\n",
    "\n",
    "        self.dptransformers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            transformer_across_t = TransformerModule(\n",
    "                input_dim_size, hidden_dim_size\n",
    "            )\n",
    "            transformer_across_k = TransformerModule(\n",
    "                input_dim_size, hidden_dim_size\n",
    "            )\n",
    "            self.dptransformers.append(\n",
    "                nn.Sequential(transformer_across_t, transformer_across_k)\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input shape: [batch_size, k_subbands, time, n_features]\n",
    "        Output shape: [batch_size, k_subbands, time, n_features]\n",
    "        \"\"\"\n",
    "        for i in range(len(self.dptransformers)):\n",
    "            x = self.dptransformers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b84e46e2-90f3-424f-8018-3d7a3c424f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    \"\"\"\n",
    "    GLU Activation Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int):\n",
    "        super(GLU, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.linear = nn.Linear(input_dim, input_dim * 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.linear(x)\n",
    "        x = x[..., :self.input_dim] * self.sigmoid(x[..., self.input_dim:])\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Just a simple MLP with tanh activation (by default).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim: int,\n",
    "            hidden_dim: int,\n",
    "            output_dim: int,\n",
    "            activation_type: str = 'tanh',\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            self.select_activation(activation_type)(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            GLU(output_dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def select_activation(activation_type: str) -> nn.modules.activation:\n",
    "        if activation_type == 'tanh':\n",
    "            return nn.Tanh\n",
    "        elif activation_type == 'relu':\n",
    "            return nn.ReLU\n",
    "        elif activation_type == 'gelu':\n",
    "            return nn.GELU\n",
    "        else:\n",
    "            raise ValueError(\"wrong activation function was selected\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskEstimationModule(nn.Module):\n",
    "    \"\"\"\n",
    "    MaskEstimation (3rd) Module of BandSplitRNN.\n",
    "    Recreates from input initial subband dimensionality via running through LayerNorms+MLPs and forms the T-F mask.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            sr: int,\n",
    "            n_fft: int,\n",
    "            bandsplits: tp.List[tp.Tuple[int, int]],\n",
    "            t_timesteps: int = 517,\n",
    "            fc_dim: int = 128,\n",
    "            mlp_dim: int = 512,\n",
    "            complex_as_channel: bool = True,\n",
    "            is_mono: bool = False,\n",
    "    ):\n",
    "        super(MaskEstimationModule, self).__init__()\n",
    "\n",
    "        frequency_mul = 1\n",
    "        if complex_as_channel:\n",
    "            frequency_mul *= 2\n",
    "        if not is_mono:\n",
    "            frequency_mul *= 2\n",
    "\n",
    "        self.cac = complex_as_channel\n",
    "        self.is_mono = is_mono\n",
    "        self.frequency_mul = frequency_mul\n",
    "\n",
    "        self.bandwidths = [(e - s) for s, e in freq2bands(bandsplits, sr, n_fft)]\n",
    "        self.layernorms = nn.ModuleList([\n",
    "            nn.LayerNorm([t_timesteps, fc_dim])\n",
    "            for _ in range(len(self.bandwidths))\n",
    "        ])\n",
    "        self.mlp = nn.ModuleList([\n",
    "            MLP(fc_dim, mlp_dim, bw * frequency_mul, activation_type='tanh')\n",
    "            for bw in self.bandwidths\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input: [batch_size, k_subbands, time, fc_shape]\n",
    "        Output: [batch_size, freq, time]\n",
    "        \"\"\"\n",
    "        outs = []\n",
    "        for i in range(x.shape[1]):\n",
    "            # run through model\n",
    "            out = self.layernorms[i](x[:, i])\n",
    "            out = self.mlp[i](out)\n",
    "            B, T, F = out.shape\n",
    "            # return to complex\n",
    "            if self.cac:\n",
    "                out = out.permute(0, 2, 1).contiguous()\n",
    "                out = out.view(B, -1, 2, F//self.frequency_mul, T).permute(0, 1, 3, 4, 2)\n",
    "                out = torch.view_as_complex(out.contiguous())\n",
    "            else:\n",
    "                out = out.view(B, -1, F//self.frequency_mul, T).contiguous()\n",
    "            outs.append(out)\n",
    "\n",
    "        # concat all subbands\n",
    "        outs = torch.cat(outs, dim=-2)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d0fda2f-eba0-4ae9-afb4-357e0ae241d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fftfreq(\n",
    "        sr: int = 44100,\n",
    "        n_fft: int = 2048\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Torch workaround of librosa.fft_frequencies\n",
    "    \"\"\"\n",
    "    out = sr * torch.fft.fftfreq(n_fft)[:n_fft // 2 + 1]\n",
    "    out[-1] = sr // 2\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_subband_indices(\n",
    "        freqs: torch.Tensor,\n",
    "        splits: tp.List[tp.Tuple[int, int]],\n",
    ") -> tp.List[tp.Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Computes subband frequency indices with given bandsplits\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    start_freq, start_index = 0, 0\n",
    "    for end_freq, step in splits:\n",
    "        bands = torch.arange(start_freq + step, end_freq + step, step)\n",
    "        start_freq = end_freq\n",
    "        for band in bands:\n",
    "            end_index = freqs[freqs < band].shape[0]\n",
    "            indices.append((start_index, end_index))\n",
    "            start_index = end_index\n",
    "    indices.append((start_index, freqs.shape[0]))\n",
    "    return indices\n",
    "\n",
    "\n",
    "def freq2bands(\n",
    "        bandsplits: tp.List[tp.Tuple[int, int]],\n",
    "        sr: int = 44100,\n",
    "        n_fft: int = 2048\n",
    ") -> tp.List[tp.Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Returns start and end FFT indices of given bandsplits\n",
    "    \"\"\"\n",
    "    freqs = get_fftfreq(sr=sr, n_fft=n_fft)\n",
    "    band_indices = get_subband_indices(freqs, bandsplits)\n",
    "    return band_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88132505-31dd-4f42-9c61-0cfeb140383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BandSplitRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    BandSplitRNN as described in paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            sr: int,\n",
    "            n_fft: int,\n",
    "            bandsplits: tp.List[tp.Tuple[int, int]],\n",
    "            complex_as_channel: bool,\n",
    "            is_mono: bool,\n",
    "            bottleneck_layer: str,\n",
    "            t_timesteps: int,\n",
    "            fc_dim: int,\n",
    "            rnn_dim: int,\n",
    "            rnn_type: str,\n",
    "            bidirectional: bool,\n",
    "            num_layers: int,\n",
    "            mlp_dim: int,\n",
    "            return_mask: bool = False\n",
    "    ):\n",
    "        super(BandSplitRNN, self).__init__()\n",
    "\n",
    "        # encoder layer\n",
    "        self.bandsplit = BandSplitModule(\n",
    "            sr=sr,\n",
    "            n_fft=n_fft,\n",
    "            bandsplits=bandsplits,\n",
    "            t_timesteps=t_timesteps,\n",
    "            fc_dim=fc_dim,\n",
    "            complex_as_channel=complex_as_channel,\n",
    "            is_mono=is_mono,\n",
    "        )\n",
    "\n",
    "        # bottleneck layer\n",
    "        if bottleneck_layer == 'rnn':\n",
    "            self.bandsequence = BandSequenceModelModule(\n",
    "                input_dim_size=fc_dim,\n",
    "                hidden_dim_size=rnn_dim,\n",
    "                rnn_type=rnn_type,\n",
    "                bidirectional=bidirectional,\n",
    "                num_layers=num_layers,\n",
    "            )\n",
    "        elif bottleneck_layer == 'att':\n",
    "            self.bandsequence = BandTransformerModelModule(\n",
    "                input_dim_size=fc_dim,\n",
    "                hidden_dim_size=rnn_dim,\n",
    "                num_layers=num_layers,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # decoder layer\n",
    "        self.maskest = MaskEstimationModule(\n",
    "            sr=sr,\n",
    "            n_fft=n_fft,\n",
    "            bandsplits=bandsplits,\n",
    "            t_timesteps=t_timesteps,\n",
    "            fc_dim=fc_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            complex_as_channel=complex_as_channel,\n",
    "            is_mono=is_mono,\n",
    "        )\n",
    "        self.cac = complex_as_channel\n",
    "        self.return_mask = return_mask\n",
    "\n",
    "    def wiener(self, x_hat: torch.Tensor, x_complex: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Wiener filtering of the input signal\n",
    "        \"\"\"\n",
    "        # TODO: add Wiener Filtering\n",
    "        return x_hat\n",
    "\n",
    "    def compute_mask(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes complex-valued T-F mask.\n",
    "        \"\"\"\n",
    "        x = self.bandsplit(x)  # [batch_size, k_subbands, time, fc_dim]\n",
    "        x = self.bandsequence(x)  # [batch_size, k_subbands, time, fc_dim]\n",
    "        x = self.maskest(x)  # [batch_size, freq, time]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Input and output are T-F complex-valued features.\n",
    "        Input shape: batch_size, n_channels, freq, time]\n",
    "        Output shape: batch_size, n_channels, freq, time]\n",
    "        \"\"\"\n",
    "        # use only magnitude if not using complex input\n",
    "        x_complex = None\n",
    "        if not self.cac:\n",
    "            x_complex = x\n",
    "            x = x.abs()\n",
    "        # normalize\n",
    "        # TODO: Try to normalize in bandsplit and denormalize in maskest\n",
    "        mean = x.mean(dim=(1, 2, 3), keepdim=True)\n",
    "        std = x.std(dim=(1, 2, 3), keepdim=True)\n",
    "        x = (x - mean) / (std + 1e-5)\n",
    "\n",
    "        # compute T-F mask\n",
    "        mask = self.compute_mask(x)\n",
    "\n",
    "        # multiply with original tensor\n",
    "        x = mask if self.return_mask else mask * x\n",
    "\n",
    "        # denormalize\n",
    "        x = x * std + mean\n",
    "\n",
    "        if not self.cac:\n",
    "            x = self.wiener(x, x_complex)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b72a57b5-7115-421d-b26e-3ca3d21fef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  class DrumBandSplit(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(DrumBandSplit, self).__init__()\n",
    "\n",
    "        self.loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
    "                    fft_sizes=[1024, 2048, 4096],\n",
    "                    hop_sizes=[256, 512, 1024],\n",
    "                    win_lengths=[1024, 2048, 4096],\n",
    "                    scale=\"mel\", \n",
    "                    n_bins=150,\n",
    "                    sample_rate=44100,\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "\n",
    "        self.loss_fn_2 = auraloss.time.SISDRLoss()\n",
    "\n",
    "        self.loss_fn_3 = torch.nn.L1Loss()\n",
    "\n",
    "        self.loss_used = 0\n",
    "\n",
    "        self.sad = SAD(sr=44100)\n",
    "        \n",
    "        batch_size, n_channels, freq, time = 2, 7, 1025, 259\n",
    "        \n",
    "        self.cfg = {\n",
    "            \"sr\": 44100,\n",
    "            \"n_fft\": 2048,\n",
    "            \"bandsplits\": [\n",
    "                (1000, 100),\n",
    "                (4000, 250),\n",
    "                (8000, 500),\n",
    "                (16000, 1000),\n",
    "                (20000, 2000),\n",
    "            ],\n",
    "            \"complex_as_channel\": True,\n",
    "            \"is_mono\": n_channels == 1,\n",
    "            \"bottleneck_layer\": 'rnn',\n",
    "            \"t_timesteps\": 259,\n",
    "            \"fc_dim\": 128,\n",
    "            \"rnn_dim\": 256,\n",
    "            \"rnn_type\": \"LSTM\",\n",
    "            \"bidirectional\": True,\n",
    "            \"num_layers\": 1,\n",
    "            \"mlp_dim\": 512,\n",
    "            \"return_mask\": False,\n",
    "        }\n",
    "        \n",
    "        self.conv_tasnet = BandSplitRNN(**self.cfg)\n",
    "\n",
    "        self.out = nn.Conv1d(4, 2, kernel_size=1)\n",
    "\n",
    "    def compute_loss(self, outputs, ref_signals):\n",
    "        loss = self.loss_fn(outputs, ref_signals) + self.loss_fn_2(outputs, ref_signals) +  self.loss_fn_3(outputs, ref_signals)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, audio, drumroll):\n",
    "        to_mix = torch.cat([audio, drumroll], axis=1)\n",
    "        out = self.conv_tasnet(to_mix)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        audio, drum, drumroll = batch\n",
    "        \n",
    "        outputs = self.forward(audio, drumroll)\n",
    "        # print(outputs.size())\n",
    "\n",
    "        if batch_idx % 64 == 0:\n",
    "            input_signal = audio[0].cpu().detach().numpy().T\n",
    "            generated_signal = outputs[0].cpu().detach().numpy().T\n",
    "            drum_signal = drum[0].cpu().detach().numpy().T \n",
    "            wandb.log({'audio_input': [wandb.Audio(input_signal, caption=\"Input\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_reference': [wandb.Audio(drum_signal, caption=\"Reference\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_output': [wandb.Audio(generated_signal, caption=\"Output\", sample_rate=44100)]})\n",
    "             \n",
    "            for i in range(5):\n",
    "                wandb.log({f'drum_{i + 1}': [wandb.Audio(drumroll[0].cpu().detach().numpy()[i, :], caption=\"Output\", sample_rate=44100)]})\n",
    "\n",
    "\n",
    "        loss = self.compute_loss(outputs, drum)         \n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer and optionally learning rate scheduler here\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fc86976-6a61-4249-83ce-cb5343fba6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2, 264600])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sad = SAD(44100)\n",
    "sad(torch.randn(7, 44100*10))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288abb41-d05d-46b5-9046-68a2e1a44e12",
   "metadata": {},
   "source": [
    "## Lightning Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b4c478-fbb1-4fc0-9913-d0b9cfe99457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModelEveryNSteps(pl.Callback):\n",
    "    def __init__(self, save_step_frequency=256,):\n",
    "        self.save_step_frequency = save_step_frequency\n",
    "        self.save_path = \"D://Github//phd-drum-sep//models//DrumConvTasnetModified//\"\n",
    "        os.makedirs(self.save_path , exist_ok=True)\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if (trainer.global_step + 1) % self.save_step_frequency == 0:\n",
    "            checkpoint_path = os.path.join(self.save_path, f\"step_{trainer.global_step + 1}.ckpt\")\n",
    "            trainer.save_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee77954-bf41-491a-b7bc-58e45391c21f",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f1230be-8d20-47b1-ad76-bb306dd17a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DrumConvTasnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4425a453-8bc8-4727-b262-633d18dd193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project='DrumConvTasnetModified', log_model='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "923baf49-1806-4355-9ece-601ef315ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_module = AudioDataModule(all_scenes, batch_size=2, num_workers=0, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "807069e3-0e09-4aaa-a050-f3ebb64a42c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1000,\n",
    "    accelerator=\"gpu\", \n",
    "    devices=-1,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[SaveModelEveryNSteps()],\n",
    "    accumulate_grad_batches=2,\n",
    "    gradient_clip_val=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2842558-de38-4276-b174-7484c01f8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhephyrius\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240311_172243-4ftoz1tv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hephyrius/DrumConvTasnetModified/runs/4ftoz1tv' target=\"_blank\">proud-brook-1</a></strong> to <a href='https://wandb.ai/hephyrius/DrumConvTasnetModified' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hephyrius/DrumConvTasnetModified' target=\"_blank\">https://wandb.ai/hephyrius/DrumConvTasnetModified</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hephyrius/DrumConvTasnetModified/runs/4ftoz1tv' target=\"_blank\">https://wandb.ai/hephyrius/DrumConvTasnetModified/runs/4ftoz1tv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type                    | Params\n",
      "--------------------------------------------------------\n",
      "0 | loss_fn     | MultiResolutionSTFTLoss | 0     \n",
      "1 | loss_fn_2   | SISDRLoss               | 0     \n",
      "2 | loss_fn_3   | L1Loss                  | 0     \n",
      "3 | conv_tasnet | ConvTasNet              | 5.0 M \n",
      "4 | out         | Conv1d                  | 10    \n",
      "--------------------------------------------------------\n",
      "5.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.0 M     Total params\n",
      "20.169    Total estimated model params size (MB)\n",
      "C:\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c58e7893a504c579d2f5674ae5474cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, audio_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be66c1-ae4c-4620-94d5-00604306b9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d35cf6a-3ab4-4e8e-a6fb-772d891300ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
