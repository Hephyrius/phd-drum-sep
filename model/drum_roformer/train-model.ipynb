{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b2a3bb-3d01-4f42-bc37-dff46522af7f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d686e6-8847-4861-88f0-58141c363862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "import plotly.graph_objects as go\n",
    "from torch.optim import lr_scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c82506-04c8-4200-9e63-26857aa176bd",
   "metadata": {},
   "source": [
    "# Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3acd3b-d177-44d3-a3f4-35140f45e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 3407\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182916a-aec1-4c05-8a08-4d2e44dfd979",
   "metadata": {},
   "source": [
    "# Construct Teh Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe5ef07-9ab9-4e3e-9334-048f67ae25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/Github/phd-drum-sep/Data/musdb18hq/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e101d3bb-e001-42ff-90cc-42384739d506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52ab99f-41d2-4b7f-a179-90681a8a0a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(os.listdir(path+'train'))\n",
    "test = list(os.listdir(path+'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21aa39ee-b25f-4bdc-950a-9389eb93765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['drum', 'bass', 'other', 'vocals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07f7f9c-2d6b-42b9-b5ac-0f4bd9aeb6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:00, 7876.33it/s]\n"
     ]
    }
   ],
   "source": [
    "all_scenes = {}\n",
    "counter = 0\n",
    "for idx, val in tqdm(enumerate(train)):\n",
    "    p = path + 'train/' + val + \"/\"\n",
    "    info = torchaudio.info(f\"{p}mixture.wav\")\n",
    "    seconds = info.num_frames // 44100\n",
    "    for i in range(0, seconds - 10, 10):\n",
    "        start_point = i * 44100\n",
    "        if start_point + 441000 < info.num_frames:\n",
    "            all_scenes[counter] = {'music_path': p, 'start_point': start_point, 'length': 441000, 'frames' : info.num_frames}\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d4eb9a-f08d-4452-a4ad-b9769a218278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_transcription_into_roll(transcription, frames):\n",
    "    # Determine your sampling frequency (frames per second)\n",
    "    fs = 44100\n",
    "    \n",
    "    piano_roll_length = int(frames)\n",
    "    \n",
    "    # Initialize the piano roll array\n",
    "    piano_roll = np.zeros((64, piano_roll_length))\n",
    "    \n",
    "    # Fill in the piano roll array\n",
    "    for note in transcription.instruments[0].notes:\n",
    "        # Convert start and end times to frame indices\n",
    "        start_frame = int(np.floor(note.start * fs))\n",
    "        end_frame = int(np.ceil(note.end * fs))\n",
    "        \n",
    "        # Set the corresponding frames to 1 (or note.velocity for a velocity-sensitive representation)\n",
    "        piano_roll[note.pitch, start_frame:end_frame] = 1  # Or use note.velocity\n",
    "        \n",
    "    roll = np.vstack([piano_roll[35:36, :], piano_roll[38:39, :], piano_roll[42:43, :], piano_roll[47:48, :], piano_roll[49:50, :]])\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988d30-a95e-4dca-ac19-a6f7cea95717",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79d11190-4e68-41f8-ae19-36d8199c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataGenerator(Dataset):\n",
    "    def __init__(self, data, sample_rate=HDEMUCS_HIGH_MUSDB.sample_rate, segment_length = 10):\n",
    "        self.data = data\n",
    "        self.sample_rate = sample_rate\n",
    "        self.segment_length = sample_rate * segment_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def load_audio(self, path, start_point, filename):\n",
    "        audio_tensors = []\n",
    "        file = filename\n",
    "        segment, _ = torchaudio.load(f\"{path}/{file}\", frame_offset=start_point, num_frames=self.segment_length)\n",
    "        audio_tensors.append(segment)\n",
    "        return torch.cat(audio_tensors, dim=0)\n",
    "\n",
    "    def load_roll(self, path, start_point, frames):\n",
    "        midi = path + '/mixture.wav.mid'\n",
    "        transcription = pretty_midi.PrettyMIDI(midi)\n",
    "        roll = turn_transcription_into_roll(transcription, frames)\n",
    "        # print(roll.shape)\n",
    "        roll = roll[:, start_point: start_point + self.segment_length]\n",
    "        return torch.from_numpy(roll).float()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        # Load audio as a tensor\n",
    "        audio_path = sample['music_path']\n",
    "\n",
    "        start_point = sample['start_point']\n",
    "\n",
    "        mixture_tensor = self.load_audio(audio_path, start_point,'mixture.wav')\n",
    "        drum_tensor = self.load_audio(audio_path, start_point,'drums.wav')\n",
    "        roll_tensor = self.load_roll(audio_path, start_point, sample['frames'])\n",
    "        return mixture_tensor, drum_tensor, roll_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a74ca-f7f7-4c32-a229-8d43faf5e43f",
   "metadata": {},
   "source": [
    "## Lightning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a521574-8b4f-4f85-88be-f97920050fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data, batch_size=32, num_workers=0, persistent_workers=False, shuffle=False):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.persistent_workers=persistent_workers\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Split your data here if necessary, e.g., into train, validation, test\n",
    "        self.dataset = AudioDataGenerator(self.data)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=self.shuffle, num_workers = self.num_workers, persistent_workers=self.persistent_workers)\n",
    "\n",
    "    # Implement val_dataloader() and test_dataloader() if you have validation and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c90603-3461-41ad-8707-28630df0c7ed",
   "metadata": {},
   "source": [
    "# making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b870439-8030-4cbe-9e2f-763aed739dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum, Tensor\n",
    "from torch.nn import Module, ModuleList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from bs_roformer.attend import Attend\n",
    "\n",
    "from beartype.typing import Tuple, Optional, List, Callable\n",
    "from beartype import beartype\n",
    "\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "from einops import rearrange, pack, unpack\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(v, d):\n",
    "    return v if exists(v) else d\n",
    "\n",
    "def pack_one(t, pattern):\n",
    "    return pack([t], pattern)\n",
    "\n",
    "def unpack_one(t, ps, pattern):\n",
    "    return unpack(t, ps, pattern)[0]\n",
    "\n",
    "# norm\n",
    "\n",
    "class RMSNorm(Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** 0.5\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim = -1) * self.scale * self.gamma\n",
    "\n",
    "# attention\n",
    "\n",
    "class FeedForward(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        mult = 4,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_inner = int(dim * mult)\n",
    "        self.net = nn.Sequential(\n",
    "            RMSNorm(dim),\n",
    "            nn.Linear(dim, dim_inner),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_inner, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        rotary_embed = None,\n",
    "        flash = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head **-0.5\n",
    "        dim_inner = heads * dim_head\n",
    "\n",
    "        self.rotary_embed = rotary_embed\n",
    "\n",
    "        self.attend = Attend(flash = flash, dropout = dropout)\n",
    "\n",
    "        self.norm = RMSNorm(dim)\n",
    "        self.to_qkv = nn.Linear(dim, dim_inner * 3, bias = False)\n",
    "\n",
    "        self.to_gates = nn.Linear(dim, heads)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(dim_inner, dim, bias = False),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        q, k, v = rearrange(self.to_qkv(x), 'b n (qkv h d) -> qkv b h n d', qkv = 3, h = self.heads)\n",
    "\n",
    "        if exists(self.rotary_embed):\n",
    "            q = self.rotary_embed.rotate_queries_or_keys(q)\n",
    "            k = self.rotary_embed.rotate_queries_or_keys(k)\n",
    "\n",
    "        out = self.attend(q, k, v)\n",
    "\n",
    "        gates = self.to_gates(x)\n",
    "        out = out * rearrange(gates, 'b n h -> b h n 1').sigmoid()\n",
    "\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        ff_mult = 4,\n",
    "        norm_output = True,\n",
    "        rotary_embed = None,\n",
    "        flash_attn = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(ModuleList([\n",
    "                Attention(dim = dim, dim_head = dim_head, heads = heads, dropout = attn_dropout, rotary_embed = rotary_embed, flash = flash_attn),\n",
    "                FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout)\n",
    "            ]))\n",
    "\n",
    "        self.norm = RMSNorm(dim) if norm_output else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "# bandsplit module\n",
    "\n",
    "class BandSplit(Module):\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_inputs: Tuple[int, ...]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_inputs = dim_inputs\n",
    "        self.to_features = ModuleList([])\n",
    "\n",
    "        for dim_in in dim_inputs:\n",
    "            net = nn.Sequential(\n",
    "                RMSNorm(dim_in),\n",
    "                nn.Linear(dim_in, dim)\n",
    "            )\n",
    "\n",
    "            self.to_features.append(net)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.split(self.dim_inputs, dim = -1)\n",
    "\n",
    "        outs = []\n",
    "        for split_input, to_feature in zip(x, self.to_features):\n",
    "            split_output = to_feature(split_input)\n",
    "            outs.append(split_output)\n",
    "\n",
    "        return torch.stack(outs, dim = -2)\n",
    "\n",
    "def MLP(\n",
    "    dim_in,\n",
    "    dim_out,\n",
    "    dim_hidden = None,\n",
    "    depth = 1,\n",
    "    activation = nn.Tanh\n",
    "):\n",
    "    dim_hidden = default(dim_hidden, dim_in)\n",
    "\n",
    "    net = []\n",
    "    dims = (dim_in, *((dim_hidden,) * (depth - 1)), dim_out)\n",
    "\n",
    "    for ind, (layer_dim_in, layer_dim_out) in enumerate(zip(dims[:-1], dims[1:])):\n",
    "        is_last = ind == (len(dims) - 2)\n",
    "\n",
    "        net.append(nn.Linear(layer_dim_in, layer_dim_out))\n",
    "\n",
    "        if is_last:\n",
    "            continue\n",
    "\n",
    "        net.append(activation())\n",
    "\n",
    "    return nn.Sequential(*net)\n",
    "\n",
    "class MaskEstimator(Module):\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_inputs: Tuple[int, ...],\n",
    "        depth,\n",
    "        mlp_expansion_factor = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_inputs = dim_inputs\n",
    "        self.to_freqs = ModuleList([])\n",
    "        dim_hidden = dim * mlp_expansion_factor\n",
    "\n",
    "        for dim_in in dim_inputs:\n",
    "            net = []\n",
    "\n",
    "            mlp = nn.Sequential(\n",
    "                MLP(dim, dim_in * 2, dim_hidden = dim_hidden, depth = depth),\n",
    "                nn.GLU(dim = -1)\n",
    "            )\n",
    "\n",
    "            self.to_freqs.append(mlp)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unbind(dim = -2)\n",
    "\n",
    "        outs = []\n",
    "\n",
    "        for band_features, mlp in zip(x, self.to_freqs):\n",
    "            freq_out = mlp(band_features)\n",
    "            outs.append(freq_out)\n",
    "\n",
    "        return torch.cat(outs, dim = -1)\n",
    "\n",
    "# main class\n",
    "\n",
    "DEFAULT_FREQS_PER_BANDS = (\n",
    "  2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "  2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "  2, 2, 2, 2,\n",
    "  4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
    "  12, 12, 12, 12, 12, 12, 12, 12,\n",
    "  24, 24, 24, 24, 24, 24, 24, 24,\n",
    "  48, 48, 48, 48, 48, 48, 48, 48,\n",
    "  128, 129,\n",
    ")\n",
    "\n",
    "class BSRoformer(Module):\n",
    "\n",
    "    @beartype\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        depth,\n",
    "        num_stems = 1,\n",
    "        time_transformer_depth = 2,\n",
    "        freq_transformer_depth = 2,\n",
    "        freqs_per_bands: Tuple[int, ...] = DEFAULT_FREQS_PER_BANDS,  # in the paper, they divide into ~60 bands, test with 1 for starters\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        flash_attn = True,\n",
    "        dim_freqs_in = 1025,\n",
    "        stft_n_fft = 2048,\n",
    "        stft_hop_length = 512, # 10ms at 44100Hz, from sections 4.1, 4.4 in the paper - @faroit recommends // 2 or // 4 for better reconstruction\n",
    "        stft_win_length = 2048,\n",
    "        stft_normalized = False,\n",
    "        stft_window_fn: Optional[Callable] = None,\n",
    "        mask_estimator_depth = 2,\n",
    "        multi_stft_resolution_loss_weight = 1.,\n",
    "        multi_stft_resolutions_window_sizes: Tuple[int, ...] = (4096, 2048, 1024, 512, 256),\n",
    "        multi_stft_hop_size = 147,\n",
    "        multi_stft_normalized = False,\n",
    "        multi_stft_window_fn: Callable = torch.hann_window\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.audio_channels = 7\n",
    "        self.num_stems = num_stems\n",
    "\n",
    "        self.layers = ModuleList([])\n",
    "\n",
    "        transformer_kwargs = dict(\n",
    "            dim = dim,\n",
    "            heads = heads,\n",
    "            dim_head = dim_head,\n",
    "            attn_dropout = attn_dropout,\n",
    "            ff_dropout = ff_dropout,\n",
    "            flash_attn = flash_attn,\n",
    "            norm_output = False\n",
    "        )\n",
    "\n",
    "        time_rotary_embed = RotaryEmbedding(dim = dim_head)\n",
    "        freq_rotary_embed = RotaryEmbedding(dim = dim_head)\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Transformer(depth = time_transformer_depth, rotary_embed = time_rotary_embed, **transformer_kwargs),\n",
    "                Transformer(depth = freq_transformer_depth, rotary_embed = freq_rotary_embed, **transformer_kwargs)\n",
    "            ]))\n",
    "\n",
    "        self.final_norm = RMSNorm(dim)\n",
    "\n",
    "        self.stft_kwargs = dict(\n",
    "            n_fft = stft_n_fft,\n",
    "            hop_length = stft_hop_length,\n",
    "            win_length = stft_win_length,\n",
    "            normalized = stft_normalized\n",
    "        )\n",
    "\n",
    "        self.stft_window_fn = partial(default(stft_window_fn, torch.hann_window), stft_win_length)\n",
    "\n",
    "        freqs = torch.stft(torch.randn(1, 4096), **self.stft_kwargs, return_complex = True).shape[1]\n",
    "\n",
    "        assert len(freqs_per_bands) > 1\n",
    "        assert sum(freqs_per_bands) == freqs, f'the number of freqs in the bands must equal {freqs} based on the STFT settings, but got {sum(freqs_per_bands)}'\n",
    "\n",
    "        freqs_per_bands_with_complex = tuple(2 * f * self.audio_channels for f in freqs_per_bands)\n",
    "\n",
    "        self.band_split = BandSplit(\n",
    "            dim = dim,\n",
    "            dim_inputs = freqs_per_bands_with_complex\n",
    "        )\n",
    "\n",
    "        self.mask_estimators = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(num_stems):\n",
    "            mask_estimator = MaskEstimator(\n",
    "                dim = dim,\n",
    "                dim_inputs = freqs_per_bands_with_complex,\n",
    "                depth = mask_estimator_depth\n",
    "            )\n",
    "\n",
    "            self.mask_estimators.append(mask_estimator)\n",
    "\n",
    "        # for the multi-resolution stft loss\n",
    "\n",
    "        self.multi_stft_resolution_loss_weight = multi_stft_resolution_loss_weight\n",
    "        self.multi_stft_resolutions_window_sizes = multi_stft_resolutions_window_sizes\n",
    "        self.multi_stft_n_fft = stft_n_fft\n",
    "        self.multi_stft_window_fn = multi_stft_window_fn\n",
    "\n",
    "        self.multi_stft_kwargs = dict(\n",
    "            hop_length = multi_stft_hop_size,\n",
    "            normalized = multi_stft_normalized\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        raw_audio,\n",
    "        target = None,\n",
    "        return_loss_breakdown = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        einops\n",
    "\n",
    "        b - batch\n",
    "        f - freq\n",
    "        t - time\n",
    "        s - audio channel (1 for mono, 2 for stereo)\n",
    "        n - number of 'stems'\n",
    "        c - complex (2)\n",
    "        d - feature dimension\n",
    "        \"\"\"\n",
    "\n",
    "        device = raw_audio.device\n",
    "\n",
    "        if raw_audio.ndim == 2:\n",
    "            raw_audio = rearrange(raw_audio, 'b t -> b 1 t')\n",
    "\n",
    "        channels = raw_audio.shape[1]\n",
    "\n",
    "        # to stft\n",
    "\n",
    "        raw_audio, batch_audio_channel_packed_shape = pack_one(raw_audio, '* t')\n",
    "\n",
    "        stft_window = self.stft_window_fn(device = device)\n",
    "\n",
    "        stft_repr = torch.stft(raw_audio, **self.stft_kwargs, window = stft_window, return_complex = True)\n",
    "        stft_repr = torch.view_as_real(stft_repr)\n",
    "\n",
    "        stft_repr = unpack_one(stft_repr, batch_audio_channel_packed_shape, '* f t c')\n",
    "        stft_repr = rearrange(stft_repr, 'b s f t c -> b (f s) t c') # merge stereo / mono into the frequency, with frequency leading dimension, for band splitting\n",
    "\n",
    "        x = rearrange(stft_repr, 'b f t c -> b t (f c)')\n",
    "\n",
    "        x = self.band_split(x)\n",
    "\n",
    "        # axial / hierarchical attention\n",
    "\n",
    "        for time_transformer, freq_transformer in self.layers:\n",
    "\n",
    "            x = rearrange(x, 'b t f d -> b f t d')\n",
    "            x, ps = pack([x], '* t d')\n",
    "\n",
    "            x = time_transformer(x)\n",
    "\n",
    "            x, = unpack(x, ps, '* t d')\n",
    "            x = rearrange(x, 'b f t d -> b t f d')\n",
    "            x, ps = pack([x], '* f d')\n",
    "\n",
    "            x = freq_transformer(x)\n",
    "\n",
    "            x, = unpack(x, ps, '* f d')\n",
    "\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        num_stems = len(self.mask_estimators)\n",
    "\n",
    "        mask = torch.stack([fn(x) for fn in self.mask_estimators], dim = 1)\n",
    "        mask = rearrange(mask, 'b n t (f c) -> b n f t c', c = 2)\n",
    "\n",
    "        # modulate frequency representation\n",
    "\n",
    "        stft_repr = rearrange(stft_repr, 'b f t c -> b 1 f t c')\n",
    "\n",
    "        # complex number multiplication\n",
    "\n",
    "        stft_repr = torch.view_as_complex(stft_repr)\n",
    "        mask = torch.view_as_complex(mask)\n",
    "\n",
    "        stft_repr = stft_repr * mask\n",
    "\n",
    "        # istft\n",
    "\n",
    "        stft_repr = rearrange(stft_repr, 'b n (f s) t -> (b n s) f t', s = self.audio_channels)\n",
    "\n",
    "        recon_audio = torch.istft(stft_repr, **self.stft_kwargs, window = stft_window, return_complex = False)\n",
    "\n",
    "        recon_audio = rearrange(recon_audio, '(b n s) t -> b n s t', s = self.audio_channels, n = num_stems)\n",
    "\n",
    "        if num_stems == 1:\n",
    "            recon_audio = rearrange(recon_audio, 'b 1 s t -> b s t')\n",
    "\n",
    "        # if a target is passed in, calculate loss for learning\n",
    "\n",
    "        if not exists(target):\n",
    "            return recon_audio\n",
    "\n",
    "        if self.num_stems > 1:\n",
    "            assert target.ndim == 4 and target.shape[1] == self.num_stems\n",
    "        \n",
    "        if target.ndim == 2:\n",
    "            target = rearrange(target, '... t -> ... 1 t')\n",
    "\n",
    "        target = target[..., :recon_audio.shape[-1]] # protect against lost length on istft\n",
    "\n",
    "        loss = F.l1_loss(recon_audio, target)\n",
    "\n",
    "        multi_stft_resolution_loss = 0.\n",
    "\n",
    "        for window_size in self.multi_stft_resolutions_window_sizes:\n",
    "\n",
    "            res_stft_kwargs = dict(\n",
    "                n_fft = max(window_size, self.multi_stft_n_fft),  # not sure what n_fft is across multi resolution stft\n",
    "                win_length = window_size,\n",
    "                return_complex = True,\n",
    "                window = self.multi_stft_window_fn(window_size, device = device),\n",
    "                **self.multi_stft_kwargs,\n",
    "            )\n",
    "\n",
    "            recon_Y = torch.stft(rearrange(recon_audio, '... s t -> (... s) t'), **res_stft_kwargs)\n",
    "            target_Y = torch.stft(rearrange(target, '... s t -> (... s) t'), **res_stft_kwargs)\n",
    "\n",
    "            multi_stft_resolution_loss = multi_stft_resolution_loss + F.l1_loss(recon_Y, target_Y)\n",
    "\n",
    "        weighted_multi_resolution_loss = multi_stft_resolution_loss * self.multi_stft_resolution_loss_weight\n",
    "\n",
    "        total_loss =  loss + weighted_multi_resolution_loss\n",
    "\n",
    "        if not return_loss_breakdown:\n",
    "            return total_loss\n",
    "\n",
    "        return total_loss, (loss, multi_stft_resolution_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b72a57b5-7115-421d-b26e-3ca3d21fef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class DrumRoformer(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(DrumRoformer, self).__init__()\n",
    "\n",
    "        self.loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
    "                    fft_sizes=[1024, 2048, 4096],\n",
    "                    hop_sizes=[256, 512, 1024],\n",
    "                    win_lengths=[1024, 2048, 4096],\n",
    "                    scale=\"mel\", \n",
    "                    n_bins=150,\n",
    "                    sample_rate=44100,\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "\n",
    "        self.loss_fn_2 = auraloss.time.SISDRLoss()\n",
    "\n",
    "        self.loss_fn_3 = torch.nn.L1Loss()\n",
    "\n",
    "        self.loss_used = 0\n",
    "\n",
    "        sources = ['drum',\n",
    "                   'noise',\n",
    "                   ]\n",
    "        \n",
    "        self.roformer =  BSRoformer(\n",
    "            dim = 512,\n",
    "            depth = 12,\n",
    "            time_transformer_depth = 1,\n",
    "            freq_transformer_depth = 1\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Conv1d(in_channels=7, out_channels=2, kernel_size=1)\n",
    "        self.out = nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1)      \n",
    "\n",
    "\n",
    "    def compute_loss(self, outputs, ref_signals):\n",
    "        loss = self.loss_fn(outputs, ref_signals) + self.loss_fn_2(outputs, ref_signals) +  self.loss_fn_3(outputs, ref_signals)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, audio, drumroll):\n",
    "        to_mix = torch.cat([audio, drumroll], axis=1)\n",
    "        out = self.roformer(to_mix)\n",
    "        out_2 = self.out_conv(out[:, 0, :, :])\n",
    "        out_2 = self.out(out_2)\n",
    "        # out_2 = torch.tanh(out_2)\n",
    "\n",
    "        return out_2\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        audio, drum, drumroll = batch\n",
    "        \n",
    "        outputs = self.forward(audio, drumroll)\n",
    "        # print(outputs.size())\n",
    "\n",
    "        if batch_idx % 64 == 0:\n",
    "            input_signal = audio[0].cpu().detach().numpy().T\n",
    "            generated_signal = outputs[0].cpu().detach().numpy().T\n",
    "            drum_signal = drum[0].cpu().detach().numpy().T \n",
    "            wandb.log({'audio_input': [wandb.Audio(input_signal, caption=\"Input\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_reference': [wandb.Audio(drum_signal, caption=\"Reference\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_output': [wandb.Audio(generated_signal, caption=\"Output\", sample_rate=44100)]})\n",
    "             \n",
    "            for i in range(5):\n",
    "                wandb.log({f'drum_{i + 1}': [wandb.Audio(drumroll[0].cpu().detach().numpy()[i, :], caption=\"Output\", sample_rate=44100)]})\n",
    "\n",
    "\n",
    "        loss = self.compute_loss(outputs, drum)         \n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer and optionally learning rate scheduler here\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288abb41-d05d-46b5-9046-68a2e1a44e12",
   "metadata": {},
   "source": [
    "## Lightning Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee77954-bf41-491a-b7bc-58e45391c21f",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1230be-8d20-47b1-ad76-bb306dd17a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DrumDemucs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4425a453-8bc8-4727-b262-633d18dd193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project='DrumRoformer', log_model='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "923baf49-1806-4355-9ece-601ef315ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_module = AudioDataModule(all_scenes, batch_size=2, num_workers=0, persistent_workers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "807069e3-0e09-4aaa-a050-f3ebb64a42c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1000,\n",
    "    accelerator=\"gpu\", \n",
    "    devices=-1,\n",
    "    logger=wandb_logger,\n",
    "    accumulate_grad_batches=2,\n",
    "    gradient_clip_val=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2842558-de38-4276-b174-7484c01f8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhephyrius\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20240216_190137-5yohzsos</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hephyrius/DrumDemucs/runs/5yohzsos' target=\"_blank\">flashing-lantern-76</a></strong> to <a href='https://wandb.ai/hephyrius/DrumDemucs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hephyrius/DrumDemucs' target=\"_blank\">https://wandb.ai/hephyrius/DrumDemucs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hephyrius/DrumDemucs/runs/5yohzsos' target=\"_blank\">https://wandb.ai/hephyrius/DrumDemucs/runs/5yohzsos</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                    | Params\n",
      "---------------------------------------------------------\n",
      "0 | loss_fn      | MultiResolutionSTFTLoss | 0     \n",
      "1 | loss_fn_2    | SISDRLoss               | 0     \n",
      "2 | loss_fn_3    | L1Loss                  | 0     \n",
      "3 | demucs_mixer | HDemucs                 | 83.7 M\n",
      "4 | out_conv     | Conv1d                  | 16    \n",
      "5 | out          | Conv1d                  | 6     \n",
      "---------------------------------------------------------\n",
      "83.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "83.7 M    Total params\n",
      "334.608   Total estimated model params size (MB)\n",
      "C:\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1125c956017449b5a2591b81a6ef6c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, audio_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be66c1-ae4c-4620-94d5-00604306b9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d35cf6a-3ab4-4e8e-a6fb-772d891300ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
