{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b2a3bb-3d01-4f42-bc37-dff46522af7f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d686e6-8847-4861-88f0-58141c363862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "import plotly.graph_objects as go\n",
    "from torch.optim import lr_scheduler\n",
    "from IPython.display import Audio\n",
    "from torchaudio.transforms import Fade\n",
    "import musdb\n",
    "import museval\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ddd3f-aa3d-4444-8ddd-c8874bac940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import subprocess\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "from torch.optim import lr_scheduler\n",
    "import pretty_midi\n",
    "from typing import Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091eea44-52d4-44e1-ab82-863b9954851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, name, midi_path, drum_path, mix_path):\n",
    "        self.name = name\n",
    "        self.midi_path = midi_path\n",
    "        self.drum_path = drum_path\n",
    "        self.mix_path = mix_path\n",
    "        self.targets = {'drums': '', 'bass': ''}\n",
    "        self.rate = 44100\n",
    "        self.subset = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c82506-04c8-4200-9e63-26857aa176bd",
   "metadata": {},
   "source": [
    "# Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3acd3b-d177-44d3-a3f4-35140f45e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 3407\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49cdd4-f6c9-4df4-adc8-f4e98f24e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/full_mix/'\n",
    "mixes = os.listdir(mix_folder)\n",
    "mixes = [mix_folder + m for m in mixes]\n",
    "\n",
    "drum_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/drum_only/'\n",
    "drum = os.listdir(drum_folder)\n",
    "drum = [drum_folder + d for d in drum]\n",
    "\n",
    "beats_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/beats/'\n",
    "beats = os.listdir(beats_folder)\n",
    "beats = [beats_folder + b for b in beats]#\n",
    "\n",
    "class_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/'\n",
    "classes = os.listdir(class_folder)\n",
    "classes = [class_folder + c for c in classes]\n",
    "\n",
    "midi_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/midi/'\n",
    "midis = os.listdir(midi_folder)\n",
    "midis = [midi_folder + m for m in midis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac376c2d-6c64-4901-a1f6-687c59d7a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tracks = []\n",
    "for idx, val in tqdm(enumerate(classes)):\n",
    "\n",
    "    name = val.replace('D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/', '')\n",
    "    name = name.replace('_subclass.txt', '')\n",
    "\n",
    "    t = Track(name, midis[idx], drum[idx], mixes[idx])\n",
    "    all_tracks.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182916a-aec1-4c05-8a08-4d2e44dfd979",
   "metadata": {},
   "source": [
    "# Construct Teh Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4eb9a-f08d-4452-a4ad-b9769a218278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_transcription_into_roll(transcription, frames):\n",
    "    # Determine your sampling frequency (frames per second)\n",
    "    fs = 44100\n",
    "    \n",
    "    piano_roll_length = int(frames)\n",
    "    \n",
    "    # Initialize the piano roll array\n",
    "    piano_roll = np.zeros((64, piano_roll_length))\n",
    "    \n",
    "    # Fill in the piano roll array\n",
    "    for note in transcription.instruments[0].notes:\n",
    "        # Convert start and end times to frame indices\n",
    "        start_frame = int(np.floor(note.start * fs))\n",
    "        end_frame = int(np.ceil(note.end * fs))\n",
    "        \n",
    "        # Set the corresponding frames to 1 (or note.velocity for a velocity-sensitive representation)\n",
    "        piano_roll[note.pitch, start_frame:end_frame] = 1  # Or use note.velocity\n",
    "        \n",
    "    roll = np.vstack([piano_roll[35:36, :], piano_roll[38:39, :], piano_roll[42:43, :], piano_roll[47:48, :], piano_roll[49:50, :]])\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988d30-a95e-4dca-ac19-a6f7cea95717",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a57b5-7115-421d-b26e-3ca3d21fef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    \"\"\"1D Convolutional block.\n",
    "\n",
    "    Args:\n",
    "        io_channels (int): The number of input/output channels, <B, Sc>\n",
    "        hidden_channels (int): The number of channels in the internal layers, <H>.\n",
    "        kernel_size (int): The convolution kernel size of the middle layer, <P>.\n",
    "        padding (int): Padding value of the convolution in the middle layer.\n",
    "        dilation (int, optional): Dilation value of the convolution in the middle layer.\n",
    "        no_redisual (bool, optional): Disable residual block/output.\n",
    "\n",
    "    Note:\n",
    "        This implementation corresponds to the \"non-causal\" setting in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        io_channels: int,\n",
    "        hidden_channels: int,\n",
    "        kernel_size: int,\n",
    "        padding: int,\n",
    "        dilation: int = 1,\n",
    "        no_residual: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=io_channels, out_channels=hidden_channels, kernel_size=1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.GroupNorm(num_groups=1, num_channels=hidden_channels, eps=1e-08),\n",
    "            torch.nn.Conv1d(\n",
    "                in_channels=hidden_channels,\n",
    "                out_channels=hidden_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                dilation=dilation,\n",
    "                groups=hidden_channels,\n",
    "            ),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.GroupNorm(num_groups=1, num_channels=hidden_channels, eps=1e-08),\n",
    "        )\n",
    "\n",
    "        self.res_out = (\n",
    "            None\n",
    "            if no_residual\n",
    "            else torch.nn.Conv1d(in_channels=hidden_channels, out_channels=io_channels, kernel_size=1)\n",
    "        )\n",
    "        self.skip_out = torch.nn.Conv1d(in_channels=hidden_channels, out_channels=io_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> Tuple[Optional[torch.Tensor], torch.Tensor]:\n",
    "        feature = self.conv_layers(input)\n",
    "        if self.res_out is None:\n",
    "            residual = None\n",
    "        else:\n",
    "            residual = self.res_out(feature)\n",
    "        skip_out = self.skip_out(feature)\n",
    "        return residual, skip_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "class MaskGenerator(torch.nn.Module):\n",
    "    \"\"\"TCN (Temporal Convolution Network) Separation Module\n",
    "\n",
    "    Generates masks for separation.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Input feature dimension, <N>.\n",
    "        num_sources (int): The number of sources to separate.\n",
    "        kernel_size (int): The convolution kernel size of conv blocks, <P>.\n",
    "        num_featrs (int): Input/output feature dimenstion of conv blocks, <B, Sc>.\n",
    "        num_hidden (int): Intermediate feature dimention of conv blocks, <H>\n",
    "        num_layers (int): The number of conv blocks in one stack, <X>.\n",
    "        num_stacks (int): The number of conv block stacks, <R>.\n",
    "        msk_activate (str): The activation function of the mask output.\n",
    "\n",
    "    Note:\n",
    "        This implementation corresponds to the \"non-causal\" setting in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_sources: int,\n",
    "        kernel_size: int,\n",
    "        num_feats: int,\n",
    "        num_hidden: int,\n",
    "        num_layers: int,\n",
    "        num_stacks: int,\n",
    "        msk_activate: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_sources = num_sources\n",
    "\n",
    "        self.input_norm = torch.nn.GroupNorm(num_groups=1, num_channels=input_dim, eps=1e-8)\n",
    "        self.input_conv = torch.nn.Conv1d(in_channels=input_dim, out_channels=num_feats, kernel_size=1)\n",
    "\n",
    "        self.receptive_field = 0\n",
    "        self.conv_layers = torch.nn.ModuleList([])\n",
    "        for s in range(num_stacks):\n",
    "            for l in range(num_layers):\n",
    "                multi = 2**l\n",
    "                self.conv_layers.append(\n",
    "                    ConvBlock(\n",
    "                        io_channels=num_feats,\n",
    "                        hidden_channels=num_hidden,\n",
    "                        kernel_size=kernel_size,\n",
    "                        dilation=multi,\n",
    "                        padding=multi,\n",
    "                        # The last ConvBlock does not need residual\n",
    "                        no_residual=(l == (num_layers - 1) and s == (num_stacks - 1)),\n",
    "                    )\n",
    "                )\n",
    "                self.receptive_field += kernel_size if s == 0 and l == 0 else (kernel_size - 1) * multi\n",
    "        self.output_prelu = torch.nn.PReLU()\n",
    "        self.output_conv = torch.nn.Conv1d(\n",
    "            in_channels=num_feats,\n",
    "            out_channels=input_dim * num_sources,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        if msk_activate == \"sigmoid\":\n",
    "            self.mask_activate = torch.nn.Sigmoid()\n",
    "        elif msk_activate == \"relu\":\n",
    "            self.mask_activate = torch.nn.ReLU()\n",
    "        elif msk_activate == \"prelu\":\n",
    "            self.mask_activate = torch.nn.PReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation {msk_activate}\")\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Generate separation mask.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): 3D Tensor with shape [batch, features, frames]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: shape [batch, num_sources, features, frames]\n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        feats = self.input_norm(input)\n",
    "        feats = self.input_conv(feats)\n",
    "        output = 0.0\n",
    "        for layer in self.conv_layers:\n",
    "            residual, skip = layer(feats)\n",
    "            if residual is not None:  # the last conv layer does not produce residual\n",
    "                feats = feats + residual\n",
    "            output = output + skip\n",
    "        output = self.output_prelu(output)\n",
    "        output = self.output_conv(output)\n",
    "        output = self.mask_activate(output)\n",
    "        return output.view(batch_size, self.num_sources, self.input_dim, -1)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "class ConvTasNet(torch.nn.Module):\n",
    "    \"\"\"Conv-TasNet architecture introduced in\n",
    "    *Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation*\n",
    "    :cite:`Luo_2019`.\n",
    "\n",
    "    Note:\n",
    "        This implementation corresponds to the \"non-causal\" setting in the paper.\n",
    "\n",
    "    See Also:\n",
    "        * :class:`torchaudio.pipelines.SourceSeparationBundle`: Source separation pipeline with pre-trained models.\n",
    "\n",
    "    Args:\n",
    "        num_sources (int, optional): The number of sources to split.\n",
    "        enc_kernel_size (int, optional): The convolution kernel size of the encoder/decoder, <L>.\n",
    "        enc_num_feats (int, optional): The feature dimensions passed to mask generator, <N>.\n",
    "        msk_kernel_size (int, optional): The convolution kernel size of the mask generator, <P>.\n",
    "        msk_num_feats (int, optional): The input/output feature dimension of conv block in the mask generator, <B, Sc>.\n",
    "        msk_num_hidden_feats (int, optional): The internal feature dimension of conv block of the mask generator, <H>.\n",
    "        msk_num_layers (int, optional): The number of layers in one conv block of the mask generator, <X>.\n",
    "        msk_num_stacks (int, optional): The numbr of conv blocks of the mask generator, <R>.\n",
    "        msk_activate (str, optional): The activation function of the mask output (Default: ``sigmoid``).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_sources: int = 2,\n",
    "        # encoder/decoder parameters\n",
    "        enc_kernel_size: int = 16,\n",
    "        enc_num_feats: int = 512,\n",
    "        # mask generator parameters\n",
    "        msk_kernel_size: int = 3,\n",
    "        msk_num_feats: int = 128,\n",
    "        msk_num_hidden_feats: int = 512,\n",
    "        msk_num_layers: int = 8,\n",
    "        msk_num_stacks: int = 3,\n",
    "        msk_activate: str = \"sigmoid\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_sources = num_sources\n",
    "        self.enc_num_feats = enc_num_feats\n",
    "        self.enc_kernel_size = enc_kernel_size\n",
    "        self.enc_stride = enc_kernel_size // 2\n",
    "\n",
    "        self.encoder = torch.nn.Conv1d(\n",
    "            in_channels=7,\n",
    "            out_channels=enc_num_feats,\n",
    "            kernel_size=enc_kernel_size,\n",
    "            stride=self.enc_stride,\n",
    "            padding=self.enc_stride,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.mask_generator = MaskGenerator(\n",
    "            input_dim=enc_num_feats,\n",
    "            num_sources=num_sources,\n",
    "            kernel_size=msk_kernel_size,\n",
    "            num_feats=msk_num_feats,\n",
    "            num_hidden=msk_num_hidden_feats,\n",
    "            num_layers=msk_num_layers,\n",
    "            num_stacks=msk_num_stacks,\n",
    "            msk_activate=msk_activate,\n",
    "        )\n",
    "        self.decoder = torch.nn.ConvTranspose1d(\n",
    "            in_channels=enc_num_feats,\n",
    "            out_channels=2,\n",
    "            kernel_size=enc_kernel_size,\n",
    "            stride=self.enc_stride,\n",
    "            padding=self.enc_stride,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def _align_num_frames_with_strides(self, input: torch.Tensor) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Pad input Tensor so that the end of the input tensor corresponds with\n",
    "\n",
    "        1. (if kernel size is odd) the center of the last convolution kernel\n",
    "        or 2. (if kernel size is even) the end of the first half of the last convolution kernel\n",
    "\n",
    "        Assumption:\n",
    "            The resulting Tensor will be padded with the size of stride (== kernel_width // 2)\n",
    "            on the both ends in Conv1D\n",
    "\n",
    "        |<--- k_1 --->|\n",
    "        |      |            |<-- k_n-1 -->|\n",
    "        |      |                  |  |<--- k_n --->|\n",
    "        |      |                  |         |      |\n",
    "        |      |                  |         |      |\n",
    "        |      v                  v         v      |\n",
    "        |<---->|<--- input signal --->|<--->|<---->|\n",
    "         stride                         PAD  stride\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): 3D Tensor with shape (batch_size, channels==1, frames)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Padded Tensor\n",
    "            int: Number of paddings performed\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, num_frames = input.shape\n",
    "        is_odd = self.enc_kernel_size % 2\n",
    "        num_strides = (num_frames - is_odd) // self.enc_stride\n",
    "        num_remainings = num_frames - (is_odd + num_strides * self.enc_stride)\n",
    "        if num_remainings == 0:\n",
    "            return input, 0\n",
    "\n",
    "        num_paddings = self.enc_stride - num_remainings\n",
    "        pad = torch.zeros(\n",
    "            batch_size,\n",
    "            num_channels,\n",
    "            num_paddings,\n",
    "            dtype=input.dtype,\n",
    "            device=input.device,\n",
    "        )\n",
    "        return torch.cat([input, pad], 2), num_paddings\n",
    "    \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Perform source separation. Generate audio source waveforms.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): 3D Tensor with shape [batch, channel==1, frames]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: 3D Tensor with shape [batch, channel==num_sources, frames]\n",
    "        \"\"\"\n",
    "\n",
    "        # B: batch size\n",
    "        # L: input frame length\n",
    "        # L': padded input frame length\n",
    "        # F: feature dimension\n",
    "        # M: feature frame length\n",
    "        # S: number of sources\n",
    "\n",
    "        padded, num_pads = self._align_num_frames_with_strides(input)  # B, 1, L'\n",
    "        batch_size, num_padded_frames = padded.shape[0], padded.shape[2]\n",
    "        feats = self.encoder(padded)  # B, F, M\n",
    "        masked = self.mask_generator(feats) * feats.unsqueeze(1)  # B, S, F, M\n",
    "        masked = masked.view(batch_size * self.num_sources, self.enc_num_feats, -1)  # B*S, F, M\n",
    "        decoded = self.decoder(masked)  # B*S, 1, L'\n",
    "        out = decoded.reshape(batch_size, 4, -1)\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "class DrumConvTasnet(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(DrumConvTasnet, self).__init__()\n",
    "\n",
    "        self.loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
    "                    fft_sizes=[1024, 2048, 4096],\n",
    "                    hop_sizes=[256, 512, 1024],\n",
    "                    win_lengths=[1024, 2048, 4096],\n",
    "                    scale=\"mel\", \n",
    "                    n_bins=150,\n",
    "                    sample_rate=44100,\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "\n",
    "        self.loss_fn_2 = auraloss.time.SISDRLoss()\n",
    "\n",
    "        self.loss_fn_3 = torch.nn.L1Loss()\n",
    "\n",
    "        self.loss_used = 0\n",
    "        \n",
    "        self.conv_tasnet =  ConvTasNet(\n",
    "            num_sources=2,\n",
    "            enc_kernel_size=16,\n",
    "            enc_num_feats=512,\n",
    "            msk_kernel_size=3,\n",
    "            msk_num_feats=128,\n",
    "            msk_num_hidden_feats=512,\n",
    "            msk_num_layers=8,\n",
    "            msk_num_stacks=3,\n",
    "            msk_activate=\"prelu\",\n",
    "        )\n",
    "\n",
    "        self.out = nn.Conv1d(4, 2, kernel_size=1)\n",
    "\n",
    "    def compute_loss(self, outputs, ref_signals):\n",
    "        loss = self.loss_fn(outputs, ref_signals) + self.loss_fn_2(outputs, ref_signals) +  self.loss_fn_3(outputs, ref_signals)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, audio, drumroll):\n",
    "        to_mix = torch.cat([audio, drumroll], axis=1)\n",
    "        out = self.conv_tasnet(to_mix)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        audio, drum, drumroll = batch\n",
    "        \n",
    "        outputs = self.forward(audio, drumroll)\n",
    "        # print(outputs.size())\n",
    "\n",
    "        if batch_idx % 64 == 0:\n",
    "            input_signal = audio[0].cpu().detach().numpy().T\n",
    "            generated_signal = outputs[0].cpu().detach().numpy().T\n",
    "            drum_signal = drum[0].cpu().detach().numpy().T \n",
    "            wandb.log({'audio_input': [wandb.Audio(input_signal, caption=\"Input\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_reference': [wandb.Audio(drum_signal, caption=\"Reference\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_output': [wandb.Audio(generated_signal, caption=\"Output\", sample_rate=44100)]})\n",
    "             \n",
    "            for i in range(5):\n",
    "                wandb.log({f'drum_{i + 1}': [wandb.Audio(drumroll[0].cpu().detach().numpy()[i, :], caption=\"Output\", sample_rate=44100)]})\n",
    "\n",
    "\n",
    "        loss = self.compute_loss(outputs, drum)         \n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer and optionally learning rate scheduler here\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d756ac7-1290-4063-b7fc-aa4c6fe99249",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='D:/Github/phd-drum-sep/data/adtof/test/Al James - Schoolboy Facination/adtof.mid'\n",
    "transcription = pretty_midi.PrettyMIDI(path)\n",
    "for note in transcription.instruments[0].notes:\n",
    "    print(note.end-note.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761dda3-88da-4175-ad3a-f67ff9ac431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45654e9b-e0ba-4177-bfd1-9860bfc2f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = pretty_midi.PrettyMIDI(midis[0])\n",
    "for note in transcription.instruments[0].notes:\n",
    "    print(note.end-note.start)\n",
    "plt.plot(transcription.get_piano_roll(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d11190-4e68-41f8-ae19-36d8199c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    audio_tensors = []\n",
    "    waveform, _ = torchaudio.load(path)\n",
    "    return waveform\n",
    "\n",
    "def load_roll(path, frames):\n",
    "    transcription = pretty_midi.PrettyMIDI(path)\n",
    "    roll = turn_transcription_into_roll(transcription, frames)\n",
    "\n",
    "    return torch.from_numpy(roll).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a00457-307d-4743-9784-9f4cdae73730",
   "metadata": {},
   "source": [
    "# SISNR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "681a5fde-395e-4acd-9d85-a2a34a4cb9e2",
   "metadata": {},
   "source": [
    "def separate_sources(\n",
    "    model,\n",
    "    mix,\n",
    "    drumroll,\n",
    "    drums,\n",
    "    segment=4.0,\n",
    "    device=None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "    chunk_len = int(44100 * segment)\n",
    "    final = torch.zeros(batch, channels, length, device=device)\n",
    "    final_roll = torch.zeros_like(drumroll, device=device)\n",
    "    loss_fn = nn.L1Loss(reduction='sum')\n",
    "\n",
    "    for start in tqdm(range(0, length, chunk_len)):\n",
    "        end = start + chunk_len\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            chunk = mix[:, :, start:end]\n",
    "            drum_chunk = drums[:, :, start:end]\n",
    "            \n",
    "            start_tensor = torch.zeros_like(drumroll[:, :, start:end])\n",
    "            sep = model(chunk, start_tensor)\n",
    "            s = (sep[:, 0, :] + sep[:, 1, :]) / 2   \n",
    "            start_loss = loss_fn(s.squeeze(1), drum_chunk).item()\n",
    "\n",
    "            size = 4410\n",
    "            for channel in range(5):\n",
    "                for start_ in range(0, (44100 * 4) - size, size):\n",
    "                    candidate = start_tensor.detach().clone()\n",
    "                    candidate[:, channel, start_:start_+size] = 1\n",
    "                    sep = model(chunk, candidate)\n",
    "                    s = (sep[:, 0, :] + sep[:, 1, :]) / 2\n",
    "                        \n",
    "                    loss = loss_fn(s.squeeze(1), drum_chunk, ).item()\n",
    "                    print(start_loss, loss)\n",
    "        \n",
    "                    if loss < start_loss:\n",
    "                        start_loss = loss\n",
    "                        start_tensor = candidate\n",
    "    \n",
    "            roll = start_tensor\n",
    "            out = model.forward(chunk, roll)\n",
    "        \n",
    "        final_roll[:, :, start:end] += roll\n",
    "        final[:, :, start:end] += out\n",
    "\n",
    "        \n",
    "    return final, final_roll\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8d23919-87ad-47ae-88ba-f414ecf53582",
   "metadata": {},
   "source": [
    "def separate_sources(\n",
    "    model,\n",
    "    mix,\n",
    "    drumroll,\n",
    "    drums,\n",
    "    segment=4.0,\n",
    "    device=None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "    chunk_len = int(44100 * segment)\n",
    "    final = torch.zeros(batch, channels, length, device=device)\n",
    "    final_roll = torch.zeros_like(drumroll, device=device)\n",
    "    loss_fn = nn.L1Loss(reduction='sum')\n",
    "\n",
    "    for start in tqdm(range(0, length, chunk_len)):\n",
    "        end = start + chunk_len\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            chunk = mix[:, :, start:end]\n",
    "            drum_chunk = drums[:, :, start:end]\n",
    "            \n",
    "            start_tensor = torch.zeros_like(drumroll[:, :, start:end])\n",
    "            sep = model(chunk, start_tensor)\n",
    "            s = (sep[:, 0, :] + sep[:, 1, :]) / 2   \n",
    "            start_loss = loss_fn(s.squeeze(1), drum_chunk).item()\n",
    "\n",
    "            size = 4410\n",
    "            for channel in range(5):\n",
    "                for start_ in range(0, (44100 * 4) - size, size):\n",
    "                    candidate = start_tensor.detach().clone()\n",
    "                    candidate[:, channel, start_:start_+size] = 1\n",
    "                    sep = model(chunk, candidate)\n",
    "                    s = (sep[:, 0, :] + sep[:, 1, :]) / 2\n",
    "                        \n",
    "                    loss = loss_fn(s.squeeze(1), drum_chunk, ).item()\n",
    "                    print(start_loss, loss)\n",
    "        \n",
    "                    if loss < start_loss:\n",
    "                        start_loss = loss\n",
    "                        start_tensor = candidate\n",
    "    \n",
    "            roll = start_tensor\n",
    "            out = model.forward(chunk, roll)\n",
    "        \n",
    "        final_roll[:, :, start:end] += roll\n",
    "        final[:, :, start:end] += out\n",
    "\n",
    "        \n",
    "    return final, final_roll"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b89c32e-a732-4cbb-bc23-13059f9bcce2",
   "metadata": {},
   "source": [
    "name = 'epoch_200'\n",
    "#try:\n",
    "out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_conv_{name}/\"\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = DrumConvTasnet.load_from_checkpoint(f'D:/Github/phd-drum-sep/analysis/conv_tasnet_model_analysis/checkpoint/{name}.ckpt')\n",
    "# model.to('cpu')\n",
    "model = model.eval()\n",
    "\n",
    "results = museval.EvalStore(frames_agg='median', tracks_agg='median')\n",
    "for track in tqdm(all_tracks):\n",
    "\n",
    "    mixture_tensor = load_audio(track.mix_path).unsqueeze(0).to(model.device)\n",
    "    snippet_length = (mixture_tensor.shape[2] // (44100 * 16)) * (44100 * 4)\n",
    "    mixture_tensor = mixture_tensor[:,:, :snippet_length]\n",
    "\n",
    "    drum_tensor = load_audio(track.drum_path).unsqueeze(0).to(model.device)\n",
    "    drum_tensor = drum_tensor[:,:, :snippet_length]\n",
    "\n",
    "    shape = mixture_tensor.shape[2]\n",
    "    roll_tensor = load_roll(track.midi_path, shape).unsqueeze(0).to(model.device)\n",
    "    roll_tensor = roll_tensor[:,:, :snippet_length]\n",
    "\n",
    "    seperated, final_roll = separate_sources(model, mixture_tensor, roll_tensor, drum_tensor, device='cuda')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36a6ea-fdf1-49f4-a1b1-d5971933eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(x, out_size=44100*4, step=4410):\n",
    "    output_tensor = torch.zeros((5, out_size))\n",
    "    for i in range(x.shape[1]):  # Iterate over the second dimension\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        output_tensor[:, start_idx:end_idx] = x[:, i].unsqueeze(1)\n",
    "    return output_tensor\n",
    "\n",
    "def compress(x, original_shape=(5, 40), step=4410):\n",
    "    \"\"\"\n",
    "    Compresses a tensor from a larger size to its original smaller size by averaging blocks of values.\n",
    "    \n",
    "    Args:\n",
    "    - x (Tensor): The input tensor to be compressed, expected to have the shape (5, 44100) or similar.\n",
    "    - original_shape (tuple): The shape of the output tensor, default is (5, 40).\n",
    "    - step (int): The size of the block to average over, default is 4410.\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor: The compressed tensor with shape specified by `original_shape`.\n",
    "    \"\"\"\n",
    "    output_tensor = torch.zeros(original_shape)\n",
    "    for i in range(original_shape[1]):  # Iterate over the second dimension of the target shape\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        # Take the mean of each block and assign it to the corresponding position in the output tensor\n",
    "        output_tensor[:, i] = x[:, start_idx:end_idx].mean(dim=1)\n",
    "    return output_tensor\n",
    "\n",
    "def tournament_selection(population, losses, tournament_size=3):\n",
    "    \"\"\"\n",
    "    Selects two parents using tournament selection.\n",
    "\n",
    "    Args:\n",
    "    - population (list of Tensors): The population from which to select parents.\n",
    "    - losses (list of floats): The loss associated with each individual in the population, serving as a measure of fitness.\n",
    "    - tournament_size (int): The number of individuals to sample for each tournament.\n",
    "\n",
    "    Returns:\n",
    "    - parent1, parent2 (tuple of Tensors): Two selected parents from the population.\n",
    "    \"\"\"\n",
    "    population_size = len(population)\n",
    "\n",
    "    # Tournament 1\n",
    "    indices = np.random.choice(range(population_size), size=tournament_size, replace=False)\n",
    "    tournament_losses = [losses[i] for i in indices]\n",
    "    winner_index = indices[np.argmin(tournament_losses)]\n",
    "    parent1 = population[winner_index]\n",
    "\n",
    "    # Tournament 2\n",
    "    indices = np.random.choice(range(population_size), size=tournament_size, replace=False)\n",
    "    tournament_losses = [losses[i] for i in indices]\n",
    "    winner_index = indices[np.argmin(tournament_losses)]\n",
    "    parent2 = population[winner_index]\n",
    "\n",
    "    return parent1, parent2\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    mask = torch.randint(0, 2, size=parent1.shape, dtype=torch.bool)\n",
    "    offspring = torch.where(mask, parent1, parent2)\n",
    "    return offspring\n",
    "\n",
    "def adaptive_mutation_rate(current_iteration, max_iterations, start_rate=0.75, end_rate=0.25):\n",
    "    \"\"\"\n",
    "    Calculates an adaptive mutation rate that decreases from start_rate to end_rate over time.\n",
    "\n",
    "    Args:\n",
    "    - current_iteration (int): The current iteration number (should start from 0).\n",
    "    - max_iterations (int): The total number of iterations the algorithm will run.\n",
    "    - start_rate (float): The initial mutation rate at the start of the algorithm.\n",
    "    - end_rate (float): The final mutation rate at the end of the algorithm.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated mutation rate for the current iteration.\n",
    "    \"\"\"\n",
    "    # Linear decay\n",
    "    rate = start_rate - ((start_rate - end_rate) * (current_iteration / max_iterations))\n",
    "    \n",
    "    # Ensure the rate never falls below the end_rate\n",
    "    return max(rate, end_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a63a5-d58e-47db-9768-e063efd5da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioData:\n",
    "    def __init__(self, audio):\n",
    "        self.audio = audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6345c-e063-4fe5-9bb3-15b218c080e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(mixture_tensor_, drum_tensor_):\n",
    "    with torch.no_grad():\n",
    "        n_iters = 100\n",
    "        population_size = 32\n",
    "        batch_size = population_size\n",
    "        elite_size = 2  # Number of elites to carry over to the next generation\n",
    "        shape = (5,40)\n",
    "    \n",
    "        solution = torch.randn(shape).clamp(0, 1)\n",
    "        solution = torch.where(solution < 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    \n",
    "        population = []\n",
    "    \n",
    "        for i in range(population_size - len(population)):\n",
    "            candidates = torch.randint_like(solution, low=0, high=1)\n",
    "            population.append(candidates)\n",
    "            \n",
    "        best_loss = 10000000000\n",
    "        best_solution = []\n",
    "    \n",
    "        for iteration in tqdm(range(n_iters)):\n",
    "            \n",
    "            losses = []\n",
    "            batch = []\n",
    "            mix = []\n",
    "            drums = []\n",
    "    \n",
    "            for j in range(batch_size):\n",
    "                try:\n",
    "                    proposed = torch.where(population[j] < 0.5, torch.tensor(1), torch.tensor(0))\n",
    "                    proposed = expand(proposed).unsqueeze(0)\n",
    "                    batch.append(proposed)\n",
    "                    mix.append(mixture_tensor_)\n",
    "                    drums.append(drum_tensor_)\n",
    "                except Exception as e:\n",
    "                    print('error', e)\n",
    "            \n",
    "            batch_candidates = torch.cat(batch, axis=0).to(model.device)\n",
    "            mix = torch.cat(mix, axis=0)\n",
    "            drums = torch.cat(drums, axis=0)\n",
    "            sep = model(mix, batch_candidates)\n",
    "    \n",
    "            for j in range(batch_size):\n",
    "                # try:\n",
    "                    # print(loss)\n",
    "                loss_item = nn.L1Loss()(sep[j, : ,:].unsqueeze(0), drum_tensor_).item()\n",
    "                losses.append(loss_item)\n",
    "                # except Exception as e:\n",
    "                #     print('error', e) \n",
    "    \n",
    "            sorted_indices = np.argsort(losses)\n",
    "            sorted_population = [population[i] for i in sorted_indices]\n",
    "            sorted_losses = [losses[i] for i in sorted_indices]\n",
    "    \n",
    "            # Update best solution if found\n",
    "            if sorted_losses[0] < best_loss:\n",
    "                best_loss = sorted_losses[0]\n",
    "                best_solution = sorted_population[0]\n",
    "                print(f\"Iteration {iteration}, Loss: {best_loss}\")\n",
    "    \n",
    "    \n",
    "            # Elitism: Carry over the best solutions unchanged\n",
    "            new_population = sorted_population[:elite_size]\n",
    "            \n",
    "            # Fill the rest of the new population\n",
    "            while len(new_population) < population_size:\n",
    "                # Tournament selection for parent selection\n",
    "                parent1, parent2 = tournament_selection(sorted_population, sorted_losses)\n",
    "    \n",
    "                # Crossover to produce offspring\n",
    "                offspring1 = crossover(parent1, parent2)\n",
    "                offspring2 = crossover(parent2, parent1)\n",
    "    \n",
    "                # Adaptive mutation rate\n",
    "                mutation_rate = adaptive_mutation_rate(iteration, n_iters)\n",
    "    \n",
    "                # Mutation for offspring\n",
    "                for offspring in [offspring1, offspring2]:\n",
    "                    if len(new_population) < population_size:  # Check if there's still space in the new population\n",
    "                        if torch.rand(1) < mutation_rate:\n",
    "                            mutation = torch.randint(-1, 2, size=offspring.shape)\n",
    "                            mutated_offspring = offspring + mutation\n",
    "                            mutated_offspring = mutated_offspring.clamp(0, 1)\n",
    "                            new_population.append(mutated_offspring)\n",
    "    \n",
    "            # Update population for the next iteration\n",
    "            population = new_population\n",
    "    \n",
    "        return population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ada2d-30b5-4c4f-b03f-3b3c069aa549",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'epoch_200'\n",
    "#try:\n",
    "out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_conv_{name}/\"\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = DrumConvTasnet.load_from_checkpoint(f'D:/Github/phd-drum-sep/analysis/conv_tasnet_model_analysis/checkpoint/{name}.ckpt')\n",
    "model = model.eval()\n",
    "\n",
    "results = museval.EvalStore(frames_agg='median', tracks_agg='median')\n",
    "for track in tqdm(all_tracks):\n",
    "\n",
    "    mixture_tensor = load_audio(track.mix_path).unsqueeze(0).to(model.device)\n",
    "    snippet_length = (mixture_tensor.shape[2] // (44100 * 16)) * (44100 * 4)\n",
    "    mixture_tensor = mixture_tensor[:,:, :snippet_length]\n",
    "\n",
    "    drum_tensor = load_audio(track.drum_path).unsqueeze(0)\n",
    "    drum_tensor = torch.cat([drum_tensor, drum_tensor], dim=1).to(model.device)\n",
    "    drum_tensor = drum_tensor[:,:, :snippet_length]\n",
    "\n",
    "    shape = mixture_tensor.shape[2]\n",
    "    roll_tensor = load_roll(track.midi_path, shape).unsqueeze(0).to(model.device)\n",
    "    roll_tensor = roll_tensor[:,:, :snippet_length]\n",
    "\n",
    "    proposed_answers = []\n",
    "    \n",
    "    device = mixture_tensor.device\n",
    "    batch, channels, length = mixture_tensor.shape\n",
    "    chunk_len = int(44100 * 4)\n",
    "\n",
    "    for start in tqdm(range(0, length, chunk_len)):\n",
    "        end = start + chunk_len\n",
    "        answer = find_best(mixture_tensor[:,:,start:end], drum_tensor[:,:, start:end])\n",
    "        proposed_answers.append(answer)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2165f-3a25-4ffb-90f2-d8c1ad6879ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_torch(transcription, prediction):\n",
    "    TPs = torch.sum((transcription == 1) & (prediction == 1), dim=1)\n",
    "    FPs = torch.sum((transcription == 0) & (prediction == 1), dim=1)\n",
    "    FNs = torch.sum((transcription == 1) & (prediction == 0), dim=1)\n",
    "\n",
    "    precision = TPs.float() / (TPs + FPs).float()\n",
    "    recall = TPs.float() / (TPs + FNs).float()\n",
    "\n",
    "    # Handle potential division by zero for precision and recall\n",
    "    precision[torch.isnan(precision)] = 0\n",
    "    recall[torch.isnan(recall)] = 0\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9974d7-61e7-483f-83d0-733710827e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f_measure(precision, recall, beta=1):\n",
    "    \"\"\"\n",
    "    Calculate the F-measure for each class and the average F-measure.\n",
    "\n",
    "    Parameters:\n",
    "    - precision: Tensor of precision values per class.\n",
    "    - recall: Tensor of recall values per class.\n",
    "    - beta: Weight of recall in the harmonic mean.\n",
    "\n",
    "    Returns:\n",
    "    - f_measure: Tensor of F-measure for each class.\n",
    "    - average_f_measure: Scalar, average F-measure across all classes.\n",
    "    \"\"\"\n",
    "    numerator = (1 + beta**2) * precision * recall\n",
    "    denominator = (beta**2 * precision) + recall\n",
    "\n",
    "    # Avoid division by zero\n",
    "    denominator[denominator == 0] = 1\n",
    "\n",
    "    f_measure = numerator / denominator\n",
    "\n",
    "    # Handle potential NaN values\n",
    "    f_measure[torch.isnan(f_measure)] = 0\n",
    "\n",
    "    average_f_measure = torch.mean(f_measure)\n",
    "\n",
    "    return f_measure, average_f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dcacba-b189-4c22-b5f7-7af87c55b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded = [expand(p).unsqueeze(0) for p in proposed_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286af2c-ad08-4bfd-be6d-c62e149073c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pres = [[],[],[],[],[]]\n",
    "recs = [[],[],[],[],[]]\n",
    "for idx, val in enumerate(expanded):\n",
    "    segment = 44100 * 4\n",
    "    start = idx * segment\n",
    "    end = start + segment\n",
    "    slice = roll_tensor[:, :, start:end].to(model.device).squeeze(0)\n",
    "    pred = val.to(model.device).squeeze(0)\n",
    "    pre, rec = calculate_precision_recall_torch(slice, pred)\n",
    "\n",
    "    for drum in range(5):\n",
    "        pres[drum].append(pre[drum].unsqueeze(0))\n",
    "        recs[drum].append(rec[drum].unsqueeze(0))\n",
    "    print(idx, pre, rec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e43f19-0727-4a7b-b131-f97c4530bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.cat(pres[0])), torch.mean(torch.cat(recs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a78b5-9038-4cd5-8900-f900295ca669",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_f_measure(torch.mean(torch.cat(pres[0])), torch.mean(torch.cat(recs[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a455e-9663-4cf6-876a-70ff780c5950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
