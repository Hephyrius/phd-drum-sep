{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b2a3bb-3d01-4f42-bc37-dff46522af7f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d686e6-8847-4861-88f0-58141c363862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "import plotly.graph_objects as go\n",
    "from torch.optim import lr_scheduler\n",
    "from IPython.display import Audio\n",
    "from torchaudio.transforms import Fade\n",
    "import musdb\n",
    "import museval\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275ddd3f-aa3d-4444-8ddd-c8874bac940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import subprocess\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "from torch.optim import lr_scheduler\n",
    "import pretty_midi\n",
    "from typing import Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091eea44-52d4-44e1-ab82-863b9954851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, name, midi_path, drum_path, mix_path):\n",
    "        self.name = name\n",
    "        self.midi_path = midi_path\n",
    "        self.drum_path = drum_path\n",
    "        self.mix_path = mix_path\n",
    "        self.targets = {'drums': '', 'bass': ''}\n",
    "        self.rate = 44100\n",
    "        self.subset = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c82506-04c8-4200-9e63-26857aa176bd",
   "metadata": {},
   "source": [
    "# Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3acd3b-d177-44d3-a3f4-35140f45e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 3407\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf49cdd4-f6c9-4df4-adc8-f4e98f24e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/full_mix/'\n",
    "mixes = os.listdir(mix_folder)\n",
    "mixes = [mix_folder + m for m in mixes]\n",
    "\n",
    "drum_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/drum_only/'\n",
    "drum = os.listdir(drum_folder)\n",
    "drum = [drum_folder + d for d in drum]\n",
    "\n",
    "beats_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/beats/'\n",
    "beats = os.listdir(beats_folder)\n",
    "beats = [beats_folder + b for b in beats]#\n",
    "\n",
    "class_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/'\n",
    "classes = os.listdir(class_folder)\n",
    "classes = [class_folder + c for c in classes]\n",
    "\n",
    "midi_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/midi/'\n",
    "midis = os.listdir(midi_folder)\n",
    "midis = [midi_folder + m for m in midis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac376c2d-6c64-4901-a1f6-687c59d7a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tracks = []\n",
    "for idx, val in tqdm(enumerate(classes)):\n",
    "\n",
    "    name = val.replace('D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/', '')\n",
    "    name = name.replace('_subclass.txt', '')\n",
    "\n",
    "    t = Track(name, midis[idx], drum[idx], mixes[idx])\n",
    "    all_tracks.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182916a-aec1-4c05-8a08-4d2e44dfd979",
   "metadata": {},
   "source": [
    "# Construct Teh Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d4eb9a-f08d-4452-a4ad-b9769a218278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_transcription_into_roll(transcription, frames):\n",
    "    # Determine your sampling frequency (frames per second)\n",
    "    fs = 44100\n",
    "    \n",
    "    piano_roll_length = int(frames)\n",
    "    \n",
    "    # Initialize the piano roll array\n",
    "    piano_roll = np.zeros((64, piano_roll_length))\n",
    "    \n",
    "    # Fill in the piano roll array\n",
    "    for note in transcription.instruments[0].notes:\n",
    "        # Convert start and end times to frame indices\n",
    "        start_frame = int(np.floor(note.start * fs))\n",
    "        end_frame = int(np.ceil(note.end * fs))\n",
    "        \n",
    "        # Set the corresponding frames to 1 (or note.velocity for a velocity-sensitive representation)\n",
    "        piano_roll[note.pitch, start_frame:end_frame] = 1  # Or use note.velocity\n",
    "        \n",
    "    roll = np.vstack([piano_roll[35:36, :], piano_roll[38:39, :], piano_roll[42:43, :], piano_roll[47:48, :], piano_roll[49:50, :]])\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988d30-a95e-4dca-ac19-a6f7cea95717",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72a57b5-7115-421d-b26e-3ca3d21fef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    \"\"\"1D Convolutional block.\n",
    "\n",
    "    Args:\n",
    "        io_channels (int): The number of input/output channels, <B, Sc>\n",
    "        hidden_channels (int): The number of channels in the internal layers, <H>.\n",
    "        kernel_size (int): The convolution kernel size of the middle layer, <P>.\n",
    "        padding (int): Padding value of the convolution in the middle layer.\n",
    "        dilation (int, optional): Dilation value of the convolution in the middle layer.\n",
    "        no_redisual (bool, optional): Disable residual block/output.\n",
    "\n",
    "    Note:\n",
    "        This implementation corresponds to the \"non-causal\" setting in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        io_channels: int,\n",
    "        hidden_channels: int,\n",
    "        kernel_size: int,\n",
    "        padding: int,\n",
    "        dilation: int = 1,\n",
    "        no_residual: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=io_channels, out_channels=hidden_channels, kernel_size=1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.GroupNorm(num_groups=1, num_channels=hidden_channels, eps=1e-08),\n",
    "            torch.nn.Conv1d(\n",
    "                in_channels=hidden_channels,\n",
    "                out_channels=hidden_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                dilation=dilation,\n",
    "                groups=hidden_channels,\n",
    "            ),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.GroupNorm(num_groups=1, num_channels=hidden_channels, eps=1e-08),\n",
    "        )\n",
    "\n",
    "        self.res_out = (\n",
    "            None\n",
    "            if no_residual\n",
    "            else torch.nn.Conv1d(in_channels=hidden_channels, out_channels=io_channels, kernel_size=1)\n",
    "        )\n",
    "        self.skip_out = torch.nn.Conv1d(in_channels=hidden_channels, out_channels=io_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> Tuple[Optional[torch.Tensor], torch.Tensor]:\n",
    "        feature = self.conv_layers(input)\n",
    "        if self.res_out is None:\n",
    "            residual = None\n",
    "        else:\n",
    "            residual = self.res_out(feature)\n",
    "        skip_out = self.skip_out(feature)\n",
    "        return residual, skip_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "class MaskGenerator(torch.nn.Module):\n",
    "    \"\"\"TCN (Temporal Convolution Network) Separation Module\n",
    "\n",
    "    Generates masks for separation.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Input feature dimension, <N>.\n",
    "        num_sources (int): The number of sources to separate.\n",
    "        kernel_size (int): The convolution kernel size of conv blocks, <P>.\n",
    "        num_featrs (int): Input/output feature dimenstion of conv blocks, <B, Sc>.\n",
    "        num_hidden (int): Intermediate feature dimention of conv blocks, <H>\n",
    "        num_layers (int): The number of conv blocks in one stack, <X>.\n",
    "        num_stacks (int): The number of conv block stacks, <R>.\n",
    "        msk_activate (str): The activation function of the mask output.\n",
    "\n",
    "    Note:\n",
    "        This implementation corresponds to the \"non-causal\" setting in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_sources: int,\n",
    "        kernel_size: int,\n",
    "        num_feats: int,\n",
    "        num_hidden: int,\n",
    "        num_layers: int,\n",
    "        num_stacks: int,\n",
    "        msk_activate: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_sources = num_sources\n",
    "\n",
    "        self.input_norm = torch.nn.GroupNorm(num_groups=1, num_channels=input_dim, eps=1e-8)\n",
    "        self.input_conv = torch.nn.Conv1d(in_channels=input_dim, out_channels=num_feats, kernel_size=1)\n",
    "\n",
    "        self.receptive_field = 0\n",
    "        self.conv_layers = torch.nn.ModuleList([])\n",
    "        for s in range(num_stacks):\n",
    "            for l in range(num_layers):\n",
    "                multi = 2**l\n",
    "                self.conv_layers.append(\n",
    "                    ConvBlock(\n",
    "                        io_channels=num_feats,\n",
    "                        hidden_channels=num_hidden,\n",
    "                        kernel_size=kernel_size,\n",
    "                        dilation=multi,\n",
    "                        padding=multi,\n",
    "                        # The last ConvBlock does not need residual\n",
    "                        no_residual=(l == (num_layers - 1) and s == (num_stacks - 1)),\n",
    "                    )\n",
    "                )\n",
    "                self.receptive_field += kernel_size if s == 0 and l == 0 else (kernel_size - 1) * multi\n",
    "        self.output_prelu = torch.nn.PReLU()\n",
    "        self.output_conv = torch.nn.Conv1d(\n",
    "            in_channels=num_feats,\n",
    "            out_channels=input_dim * num_sources,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        if msk_activate == \"sigmoid\":\n",
    "            self.mask_activate = torch.nn.Sigmoid()\n",
    "        elif msk_activate == \"relu\":\n",
    "            self.mask_activate = torch.nn.ReLU()\n",
    "        elif msk_activate == \"prelu\":\n",
    "            self.mask_activate = torch.nn.PReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation {msk_activate}\")\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Generate separation mask.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): 3D Tensor with shape [batch, features, frames]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: shape [batch, num_sources, features, frames]\n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        feats = self.input_norm(input)\n",
    "        feats = self.input_conv(feats)\n",
    "        output = 0.0\n",
    "        for layer in self.conv_layers:\n",
    "            residual, skip = layer(feats)\n",
    "            if residual is not None:  # the last conv layer does not produce residual\n",
    "                feats = feats + residual\n",
    "            output = output + skip\n",
    "        output = self.output_prelu(output)\n",
    "        output = self.output_conv(output)\n",
    "        output = self.mask_activate(output)\n",
    "        return output.view(batch_size, self.num_sources, self.input_dim, -1)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "class ConvTasNet(torch.nn.Module):\n",
    "    \"\"\"Conv-TasNet architecture introduced in\n",
    "    *Conv-TasNet: Surpassing Ideal Timeâ€“Frequency Magnitude Masking for Speech Separation*\n",
    "    :cite:`Luo_2019`.\n",
    "\n",
    "    Note:\n",
    "        This implementation corresponds to the \"non-causal\" setting in the paper.\n",
    "\n",
    "    See Also:\n",
    "        * :class:`torchaudio.pipelines.SourceSeparationBundle`: Source separation pipeline with pre-trained models.\n",
    "\n",
    "    Args:\n",
    "        num_sources (int, optional): The number of sources to split.\n",
    "        enc_kernel_size (int, optional): The convolution kernel size of the encoder/decoder, <L>.\n",
    "        enc_num_feats (int, optional): The feature dimensions passed to mask generator, <N>.\n",
    "        msk_kernel_size (int, optional): The convolution kernel size of the mask generator, <P>.\n",
    "        msk_num_feats (int, optional): The input/output feature dimension of conv block in the mask generator, <B, Sc>.\n",
    "        msk_num_hidden_feats (int, optional): The internal feature dimension of conv block of the mask generator, <H>.\n",
    "        msk_num_layers (int, optional): The number of layers in one conv block of the mask generator, <X>.\n",
    "        msk_num_stacks (int, optional): The numbr of conv blocks of the mask generator, <R>.\n",
    "        msk_activate (str, optional): The activation function of the mask output (Default: ``sigmoid``).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_sources: int = 2,\n",
    "        # encoder/decoder parameters\n",
    "        enc_kernel_size: int = 16,\n",
    "        enc_num_feats: int = 512,\n",
    "        # mask generator parameters\n",
    "        msk_kernel_size: int = 3,\n",
    "        msk_num_feats: int = 128,\n",
    "        msk_num_hidden_feats: int = 512,\n",
    "        msk_num_layers: int = 8,\n",
    "        msk_num_stacks: int = 3,\n",
    "        msk_activate: str = \"sigmoid\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_sources = num_sources\n",
    "        self.enc_num_feats = enc_num_feats\n",
    "        self.enc_kernel_size = enc_kernel_size\n",
    "        self.enc_stride = enc_kernel_size // 2\n",
    "\n",
    "        self.encoder = torch.nn.Conv1d(\n",
    "            in_channels=7,\n",
    "            out_channels=enc_num_feats,\n",
    "            kernel_size=enc_kernel_size,\n",
    "            stride=self.enc_stride,\n",
    "            padding=self.enc_stride,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.mask_generator = MaskGenerator(\n",
    "            input_dim=enc_num_feats,\n",
    "            num_sources=num_sources,\n",
    "            kernel_size=msk_kernel_size,\n",
    "            num_feats=msk_num_feats,\n",
    "            num_hidden=msk_num_hidden_feats,\n",
    "            num_layers=msk_num_layers,\n",
    "            num_stacks=msk_num_stacks,\n",
    "            msk_activate=msk_activate,\n",
    "        )\n",
    "        self.decoder = torch.nn.ConvTranspose1d(\n",
    "            in_channels=enc_num_feats,\n",
    "            out_channels=2,\n",
    "            kernel_size=enc_kernel_size,\n",
    "            stride=self.enc_stride,\n",
    "            padding=self.enc_stride,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def _align_num_frames_with_strides(self, input: torch.Tensor) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Pad input Tensor so that the end of the input tensor corresponds with\n",
    "\n",
    "        1. (if kernel size is odd) the center of the last convolution kernel\n",
    "        or 2. (if kernel size is even) the end of the first half of the last convolution kernel\n",
    "\n",
    "        Assumption:\n",
    "            The resulting Tensor will be padded with the size of stride (== kernel_width // 2)\n",
    "            on the both ends in Conv1D\n",
    "\n",
    "        |<--- k_1 --->|\n",
    "        |      |            |<-- k_n-1 -->|\n",
    "        |      |                  |  |<--- k_n --->|\n",
    "        |      |                  |         |      |\n",
    "        |      |                  |         |      |\n",
    "        |      v                  v         v      |\n",
    "        |<---->|<--- input signal --->|<--->|<---->|\n",
    "         stride                         PAD  stride\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): 3D Tensor with shape (batch_size, channels==1, frames)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Padded Tensor\n",
    "            int: Number of paddings performed\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, num_frames = input.shape\n",
    "        is_odd = self.enc_kernel_size % 2\n",
    "        num_strides = (num_frames - is_odd) // self.enc_stride\n",
    "        num_remainings = num_frames - (is_odd + num_strides * self.enc_stride)\n",
    "        if num_remainings == 0:\n",
    "            return input, 0\n",
    "\n",
    "        num_paddings = self.enc_stride - num_remainings\n",
    "        pad = torch.zeros(\n",
    "            batch_size,\n",
    "            num_channels,\n",
    "            num_paddings,\n",
    "            dtype=input.dtype,\n",
    "            device=input.device,\n",
    "        )\n",
    "        return torch.cat([input, pad], 2), num_paddings\n",
    "    \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Perform source separation. Generate audio source waveforms.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): 3D Tensor with shape [batch, channel==1, frames]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: 3D Tensor with shape [batch, channel==num_sources, frames]\n",
    "        \"\"\"\n",
    "\n",
    "        # B: batch size\n",
    "        # L: input frame length\n",
    "        # L': padded input frame length\n",
    "        # F: feature dimension\n",
    "        # M: feature frame length\n",
    "        # S: number of sources\n",
    "\n",
    "        padded, num_pads = self._align_num_frames_with_strides(input)  # B, 1, L'\n",
    "        batch_size, num_padded_frames = padded.shape[0], padded.shape[2]\n",
    "        feats = self.encoder(padded)  # B, F, M\n",
    "        masked = self.mask_generator(feats) * feats.unsqueeze(1)  # B, S, F, M\n",
    "        masked = masked.view(batch_size * self.num_sources, self.enc_num_feats, -1)  # B*S, F, M\n",
    "        decoded = self.decoder(masked)  # B*S, 1, L'\n",
    "        out = decoded.reshape(batch_size, 4, -1)\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "class DrumConvTasnet(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(DrumConvTasnet, self).__init__()\n",
    "\n",
    "        self.loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
    "                    fft_sizes=[1024, 2048, 4096],\n",
    "                    hop_sizes=[256, 512, 1024],\n",
    "                    win_lengths=[1024, 2048, 4096],\n",
    "                    scale=\"mel\", \n",
    "                    n_bins=150,\n",
    "                    sample_rate=44100,\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "\n",
    "        self.loss_fn_2 = auraloss.time.SISDRLoss()\n",
    "\n",
    "        self.loss_fn_3 = torch.nn.L1Loss()\n",
    "\n",
    "        self.loss_used = 0\n",
    "        \n",
    "        self.conv_tasnet =  ConvTasNet(\n",
    "            num_sources=2,\n",
    "            enc_kernel_size=16,\n",
    "            enc_num_feats=512,\n",
    "            msk_kernel_size=3,\n",
    "            msk_num_feats=128,\n",
    "            msk_num_hidden_feats=512,\n",
    "            msk_num_layers=8,\n",
    "            msk_num_stacks=3,\n",
    "            msk_activate=\"prelu\",\n",
    "        )\n",
    "\n",
    "        self.out = nn.Conv1d(4, 2, kernel_size=1)\n",
    "\n",
    "    def compute_loss(self, outputs, ref_signals):\n",
    "        loss = self.loss_fn(outputs, ref_signals) + self.loss_fn_2(outputs, ref_signals) +  self.loss_fn_3(outputs, ref_signals)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, audio, drumroll):\n",
    "        to_mix = torch.cat([audio, drumroll], axis=1)\n",
    "        out = self.conv_tasnet(to_mix)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        audio, drum, drumroll = batch\n",
    "        \n",
    "        outputs = self.forward(audio, drumroll)\n",
    "        # print(outputs.size())\n",
    "\n",
    "        if batch_idx % 64 == 0:\n",
    "            input_signal = audio[0].cpu().detach().numpy().T\n",
    "            generated_signal = outputs[0].cpu().detach().numpy().T\n",
    "            drum_signal = drum[0].cpu().detach().numpy().T \n",
    "            wandb.log({'audio_input': [wandb.Audio(input_signal, caption=\"Input\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_reference': [wandb.Audio(drum_signal, caption=\"Reference\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_output': [wandb.Audio(generated_signal, caption=\"Output\", sample_rate=44100)]})\n",
    "             \n",
    "            for i in range(5):\n",
    "                wandb.log({f'drum_{i + 1}': [wandb.Audio(drumroll[0].cpu().detach().numpy()[i, :], caption=\"Output\", sample_rate=44100)]})\n",
    "\n",
    "\n",
    "        loss = self.compute_loss(outputs, drum)         \n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer and optionally learning rate scheduler here\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d756ac7-1290-4063-b7fc-aa4c6fe99249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999987\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.09999999999999964\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000053\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n"
     ]
    }
   ],
   "source": [
    "path='D:/Github/phd-drum-sep/data/adtof/test/Al James - Schoolboy Facination/adtof.mid'\n",
    "transcription = pretty_midi.PrettyMIDI(path)\n",
    "for note in transcription.instruments[0].notes:\n",
    "    print(note.end-note.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6761dda3-88da-4175-ad3a-f67ff9ac431c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_PrettyMIDI__tick_to_time',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_load_instruments',\n",
       " '_load_metadata',\n",
       " '_load_tempo_changes',\n",
       " '_tick_scales',\n",
       " '_update_tick_to_time',\n",
       " 'adjust_times',\n",
       " 'estimate_beat_start',\n",
       " 'estimate_tempi',\n",
       " 'estimate_tempo',\n",
       " 'fluidsynth',\n",
       " 'get_beats',\n",
       " 'get_chroma',\n",
       " 'get_downbeats',\n",
       " 'get_end_time',\n",
       " 'get_onsets',\n",
       " 'get_piano_roll',\n",
       " 'get_pitch_class_histogram',\n",
       " 'get_pitch_class_transition_matrix',\n",
       " 'get_tempo_changes',\n",
       " 'instruments',\n",
       " 'key_signature_changes',\n",
       " 'lyrics',\n",
       " 'remove_invalid_notes',\n",
       " 'resolution',\n",
       " 'synthesize',\n",
       " 'text_events',\n",
       " 'tick_to_time',\n",
       " 'time_signature_changes',\n",
       " 'time_to_tick',\n",
       " 'write']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45654e9b-e0ba-4177-bfd1-9860bfc2f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999999\n",
      "0.09999999999999998\n",
      "0.09999999999999998\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000053\n",
      "0.10000000000000053\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000053\n",
      "0.10000000000000053\n",
      "0.10000000000000053\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x234fc989390>,\n",
       " <matplotlib.lines.Line2D at 0x234fadaaf90>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbc650>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbca50>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbce10>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbd290>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbd890>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbdb90>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbdf90>,\n",
       " <matplotlib.lines.Line2D at 0x234fcba5410>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbe710>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbec90>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbf0d0>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbf510>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbfa50>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbbfdd0>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc6710>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc6fd0>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc4850>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc4b50>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc5090>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc5490>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc5790>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc5e90>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc6290>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc6790>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc6d10>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc7050>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc75d0>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc7a50>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc7f90>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbc6010>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbcc2d0>,\n",
       " <matplotlib.lines.Line2D at 0x234fcbcc890>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transcription = pretty_midi.PrettyMIDI(midis[0])\n",
    "for note in transcription.instruments[0].notes:\n",
    "    print(note.end-note.start)\n",
    "plt.plot(transcription.get_piano_roll(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d11190-4e68-41f8-ae19-36d8199c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    audio_tensors = []\n",
    "    waveform, _ = torchaudio.load(path)\n",
    "    return waveform\n",
    "\n",
    "def load_roll(path, frames):\n",
    "    transcription = pretty_midi.PrettyMIDI(path)\n",
    "    roll = turn_transcription_into_roll(transcription, frames)\n",
    "\n",
    "    return torch.from_numpy(roll).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a00457-307d-4743-9784-9f4cdae73730",
   "metadata": {},
   "source": [
    "# SISNR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "681a5fde-395e-4acd-9d85-a2a34a4cb9e2",
   "metadata": {},
   "source": [
    "def separate_sources(\n",
    "    model,\n",
    "    mix,\n",
    "    drumroll,\n",
    "    drums,\n",
    "    segment=4.0,\n",
    "    device=None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "    chunk_len = int(44100 * segment)\n",
    "    final = torch.zeros(batch, channels, length, device=device)\n",
    "    final_roll = torch.zeros_like(drumroll, device=device)\n",
    "    loss_fn = nn.L1Loss(reduction='sum')\n",
    "\n",
    "    for start in tqdm(range(0, length, chunk_len)):\n",
    "        end = start + chunk_len\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            chunk = mix[:, :, start:end]\n",
    "            drum_chunk = drums[:, :, start:end]\n",
    "            \n",
    "            start_tensor = torch.zeros_like(drumroll[:, :, start:end])\n",
    "            sep = model(chunk, start_tensor)\n",
    "            s = (sep[:, 0, :] + sep[:, 1, :]) / 2   \n",
    "            start_loss = loss_fn(s.squeeze(1), drum_chunk).item()\n",
    "\n",
    "            size = 4410\n",
    "            for channel in range(5):\n",
    "                for start_ in range(0, (44100 * 4) - size, size):\n",
    "                    candidate = start_tensor.detach().clone()\n",
    "                    candidate[:, channel, start_:start_+size] = 1\n",
    "                    sep = model(chunk, candidate)\n",
    "                    s = (sep[:, 0, :] + sep[:, 1, :]) / 2\n",
    "                        \n",
    "                    loss = loss_fn(s.squeeze(1), drum_chunk, ).item()\n",
    "                    print(start_loss, loss)\n",
    "        \n",
    "                    if loss < start_loss:\n",
    "                        start_loss = loss\n",
    "                        start_tensor = candidate\n",
    "    \n",
    "            roll = start_tensor\n",
    "            out = model.forward(chunk, roll)\n",
    "        \n",
    "        final_roll[:, :, start:end] += roll\n",
    "        final[:, :, start:end] += out\n",
    "\n",
    "        \n",
    "    return final, final_roll\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8d23919-87ad-47ae-88ba-f414ecf53582",
   "metadata": {},
   "source": [
    "def separate_sources(\n",
    "    model,\n",
    "    mix,\n",
    "    drumroll,\n",
    "    drums,\n",
    "    segment=4.0,\n",
    "    device=None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "    chunk_len = int(44100 * segment)\n",
    "    final = torch.zeros(batch, channels, length, device=device)\n",
    "    final_roll = torch.zeros_like(drumroll, device=device)\n",
    "    loss_fn = nn.L1Loss(reduction='sum')\n",
    "\n",
    "    for start in tqdm(range(0, length, chunk_len)):\n",
    "        end = start + chunk_len\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            chunk = mix[:, :, start:end]\n",
    "            drum_chunk = drums[:, :, start:end]\n",
    "            \n",
    "            start_tensor = torch.zeros_like(drumroll[:, :, start:end])\n",
    "            sep = model(chunk, start_tensor)\n",
    "            s = (sep[:, 0, :] + sep[:, 1, :]) / 2   \n",
    "            start_loss = loss_fn(s.squeeze(1), drum_chunk).item()\n",
    "\n",
    "            size = 4410\n",
    "            for channel in range(5):\n",
    "                for start_ in range(0, (44100 * 4) - size, size):\n",
    "                    candidate = start_tensor.detach().clone()\n",
    "                    candidate[:, channel, start_:start_+size] = 1\n",
    "                    sep = model(chunk, candidate)\n",
    "                    s = (sep[:, 0, :] + sep[:, 1, :]) / 2\n",
    "                        \n",
    "                    loss = loss_fn(s.squeeze(1), drum_chunk, ).item()\n",
    "                    print(start_loss, loss)\n",
    "        \n",
    "                    if loss < start_loss:\n",
    "                        start_loss = loss\n",
    "                        start_tensor = candidate\n",
    "    \n",
    "            roll = start_tensor\n",
    "            out = model.forward(chunk, roll)\n",
    "        \n",
    "        final_roll[:, :, start:end] += roll\n",
    "        final[:, :, start:end] += out\n",
    "\n",
    "        \n",
    "    return final, final_roll"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b89c32e-a732-4cbb-bc23-13059f9bcce2",
   "metadata": {},
   "source": [
    "name = 'epoch_200'\n",
    "#try:\n",
    "out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_conv_{name}/\"\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = DrumConvTasnet.load_from_checkpoint(f'D:/Github/phd-drum-sep/analysis/conv_tasnet_model_analysis/checkpoint/{name}.ckpt')\n",
    "# model.to('cpu')\n",
    "model = model.eval()\n",
    "\n",
    "results = museval.EvalStore(frames_agg='median', tracks_agg='median')\n",
    "for track in tqdm(all_tracks):\n",
    "\n",
    "    mixture_tensor = load_audio(track.mix_path).unsqueeze(0).to(model.device)\n",
    "    snippet_length = (mixture_tensor.shape[2] // (44100 * 16)) * (44100 * 4)\n",
    "    mixture_tensor = mixture_tensor[:,:, :snippet_length]\n",
    "\n",
    "    drum_tensor = load_audio(track.drum_path).unsqueeze(0).to(model.device)\n",
    "    drum_tensor = drum_tensor[:,:, :snippet_length]\n",
    "\n",
    "    shape = mixture_tensor.shape[2]\n",
    "    roll_tensor = load_roll(track.midi_path, shape).unsqueeze(0).to(model.device)\n",
    "    roll_tensor = roll_tensor[:,:, :snippet_length]\n",
    "\n",
    "    seperated, final_roll = separate_sources(model, mixture_tensor, roll_tensor, drum_tensor, device='cuda')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc36a6ea-fdf1-49f4-a1b1-d5971933eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(x, out_size=44100*4, step=4410):\n",
    "    output_tensor = torch.zeros((5, out_size))\n",
    "    for i in range(x.shape[1]):  # Iterate over the second dimension\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        output_tensor[:, start_idx:end_idx] = x[:, i].unsqueeze(1)\n",
    "    return output_tensor\n",
    "\n",
    "def compress(x, original_shape=(5, 40), step=4410):\n",
    "    \"\"\"\n",
    "    Compresses a tensor from a larger size to its original smaller size by averaging blocks of values.\n",
    "    \n",
    "    Args:\n",
    "    - x (Tensor): The input tensor to be compressed, expected to have the shape (5, 44100) or similar.\n",
    "    - original_shape (tuple): The shape of the output tensor, default is (5, 40).\n",
    "    - step (int): The size of the block to average over, default is 4410.\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor: The compressed tensor with shape specified by `original_shape`.\n",
    "    \"\"\"\n",
    "    output_tensor = torch.zeros(original_shape)\n",
    "    for i in range(original_shape[1]):  # Iterate over the second dimension of the target shape\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        # Take the mean of each block and assign it to the corresponding position in the output tensor\n",
    "        output_tensor[:, i] = x[:, start_idx:end_idx].mean(dim=1)\n",
    "    return output_tensor\n",
    "\n",
    "def tournament_selection(population, losses, tournament_size=3):\n",
    "    \"\"\"\n",
    "    Selects two parents using tournament selection.\n",
    "\n",
    "    Args:\n",
    "    - population (list of Tensors): The population from which to select parents.\n",
    "    - losses (list of floats): The loss associated with each individual in the population, serving as a measure of fitness.\n",
    "    - tournament_size (int): The number of individuals to sample for each tournament.\n",
    "\n",
    "    Returns:\n",
    "    - parent1, parent2 (tuple of Tensors): Two selected parents from the population.\n",
    "    \"\"\"\n",
    "    population_size = len(population)\n",
    "\n",
    "    # Tournament 1\n",
    "    indices = np.random.choice(range(population_size), size=tournament_size, replace=False)\n",
    "    tournament_losses = [losses[i] for i in indices]\n",
    "    winner_index = indices[np.argmin(tournament_losses)]\n",
    "    parent1 = population[winner_index]\n",
    "\n",
    "    # Tournament 2\n",
    "    indices = np.random.choice(range(population_size), size=tournament_size, replace=False)\n",
    "    tournament_losses = [losses[i] for i in indices]\n",
    "    winner_index = indices[np.argmin(tournament_losses)]\n",
    "    parent2 = population[winner_index]\n",
    "\n",
    "    return parent1, parent2\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    mask = torch.randint(0, 2, size=parent1.shape, dtype=torch.bool)\n",
    "    offspring = torch.where(mask, parent1, parent2)\n",
    "    return offspring\n",
    "\n",
    "def adaptive_mutation_rate(current_iteration, max_iterations, start_rate=0.75, end_rate=0.25):\n",
    "    \"\"\"\n",
    "    Calculates an adaptive mutation rate that decreases from start_rate to end_rate over time.\n",
    "\n",
    "    Args:\n",
    "    - current_iteration (int): The current iteration number (should start from 0).\n",
    "    - max_iterations (int): The total number of iterations the algorithm will run.\n",
    "    - start_rate (float): The initial mutation rate at the start of the algorithm.\n",
    "    - end_rate (float): The final mutation rate at the end of the algorithm.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated mutation rate for the current iteration.\n",
    "    \"\"\"\n",
    "    # Linear decay\n",
    "    rate = start_rate - ((start_rate - end_rate) * (current_iteration / max_iterations))\n",
    "    \n",
    "    # Ensure the rate never falls below the end_rate\n",
    "    return max(rate, end_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b97a63a5-d58e-47db-9768-e063efd5da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioData:\n",
    "    def __init__(self, audio):\n",
    "        self.audio = audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaf6345c-e063-4fe5-9bb3-15b218c080e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(mixture_tensor_, drum_tensor_):\n",
    "    with torch.no_grad():\n",
    "        n_iters = 100\n",
    "        population_size = 32\n",
    "        batch_size = population_size\n",
    "        elite_size = 2  # Number of elites to carry over to the next generation\n",
    "        shape = (5,40)\n",
    "    \n",
    "        solution = torch.randn(shape).clamp(0, 1)\n",
    "        solution = torch.where(solution < 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    \n",
    "        population = []\n",
    "    \n",
    "        for i in range(population_size - len(population)):\n",
    "            candidates = torch.randint_like(solution, low=0, high=1)\n",
    "            population.append(candidates)\n",
    "            \n",
    "        best_loss = 10000000000\n",
    "        best_solution = []\n",
    "    \n",
    "        for iteration in tqdm(range(n_iters)):\n",
    "            \n",
    "            losses = []\n",
    "            batch = []\n",
    "            mix = []\n",
    "            drums = []\n",
    "    \n",
    "            for j in range(batch_size):\n",
    "                try:\n",
    "                    proposed = torch.where(population[j] < 0.5, torch.tensor(1), torch.tensor(0))\n",
    "                    proposed = expand(proposed).unsqueeze(0)\n",
    "                    batch.append(proposed)\n",
    "                    mix.append(mixture_tensor_)\n",
    "                    drums.append(drum_tensor_)\n",
    "                except Exception as e:\n",
    "                    print('error', e)\n",
    "            \n",
    "            batch_candidates = torch.cat(batch, axis=0).to(model.device)\n",
    "            mix = torch.cat(mix, axis=0)\n",
    "            drums = torch.cat(drums, axis=0)\n",
    "            sep = model(mix, batch_candidates)\n",
    "    \n",
    "            for j in range(batch_size):\n",
    "                # try:\n",
    "                    # print(loss)\n",
    "                loss_item = nn.L1Loss()(sep[j, : ,:].unsqueeze(0), drum_tensor_).item()\n",
    "                losses.append(loss_item)\n",
    "                # except Exception as e:\n",
    "                #     print('error', e) \n",
    "    \n",
    "            sorted_indices = np.argsort(losses)\n",
    "            sorted_population = [population[i] for i in sorted_indices]\n",
    "            sorted_losses = [losses[i] for i in sorted_indices]\n",
    "    \n",
    "            # Update best solution if found\n",
    "            if sorted_losses[0] < best_loss:\n",
    "                best_loss = sorted_losses[0]\n",
    "                best_solution = sorted_population[0]\n",
    "                print(f\"Iteration {iteration}, Loss: {best_loss}\")\n",
    "    \n",
    "    \n",
    "            # Elitism: Carry over the best solutions unchanged\n",
    "            new_population = sorted_population[:elite_size]\n",
    "            \n",
    "            # Fill the rest of the new population\n",
    "            while len(new_population) < population_size:\n",
    "                # Tournament selection for parent selection\n",
    "                parent1, parent2 = tournament_selection(sorted_population, sorted_losses)\n",
    "    \n",
    "                # Crossover to produce offspring\n",
    "                offspring1 = crossover(parent1, parent2)\n",
    "                offspring2 = crossover(parent2, parent1)\n",
    "    \n",
    "                # Adaptive mutation rate\n",
    "                mutation_rate = adaptive_mutation_rate(iteration, n_iters)\n",
    "    \n",
    "                # Mutation for offspring\n",
    "                for offspring in [offspring1, offspring2]:\n",
    "                    if len(new_population) < population_size:  # Check if there's still space in the new population\n",
    "                        if torch.rand(1) < mutation_rate:\n",
    "                            mutation = torch.randint(-1, 2, size=offspring.shape)\n",
    "                            mutated_offspring = offspring + mutation\n",
    "                            mutated_offspring = mutated_offspring.clamp(0, 1)\n",
    "                            new_population.append(mutated_offspring)\n",
    "    \n",
    "            # Update population for the next iteration\n",
    "            population = new_population\n",
    "    \n",
    "        return population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d76ada2d-30b5-4c4f-b03f-3b3c069aa549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\pytorch_lightning\\utilities\\migration\\utils.py:55: The loaded checkpoint was produced with Lightning v2.2.1, which is newer than your current Lightning version: v2.1.2\n",
      "  0%|                                                                                           | 0/23 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|â–Š                                                                                 | 1/100 [00:03<05:48,  3.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.05602195858955383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|â–ˆâ–‹                                                                                | 2/100 [00:05<03:47,  2.32s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 0.05567113682627678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|â–ˆâ–ˆâ–                                                                               | 3/100 [00:06<03:08,  1.95s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, Loss: 0.05500786751508713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  4%|â–ˆâ–ˆâ–ˆâ–Ž                                                                              | 4/100 [00:07<02:49,  1.77s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, Loss: 0.054834380745887756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  5%|â–ˆâ–ˆâ–ˆâ–ˆ                                                                              | 5/100 [00:09<02:38,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                             | 6/100 [00:10<02:31,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                            | 7/100 [00:12<02:26,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                           | 8/100 [00:13<02:22,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                          | 9/100 [00:15<02:19,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                         | 10/100 [00:16<02:16,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                        | 11/100 [00:18<02:16,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                       | 12/100 [00:20<02:14,  1.52s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, Loss: 0.05445801839232445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                      | 13/100 [00:21<02:12,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                     | 14/100 [00:23<02:10,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                    | 15/100 [00:24<02:09,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                    | 16/100 [00:26<02:08,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                   | 17/100 [00:27<02:07,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                  | 18/100 [00:29<02:05,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                 | 19/100 [00:30<02:03,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                | 20/100 [00:32<02:01,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                | 21/100 [00:33<02:00,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                               | 22/100 [00:35<01:59,  1.53s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21, Loss: 0.054188378155231476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                              | 23/100 [00:36<01:58,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                             | 24/100 [00:38<01:56,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                            | 25/100 [00:39<01:55,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 26/100 [00:41<01:53,  1.54s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25, Loss: 0.0541861429810524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                           | 27/100 [00:43<01:52,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                          | 28/100 [00:44<01:50,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 29/100 [00:46<01:50,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                        | 30/100 [00:47<01:48,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                        | 31/100 [00:49<01:46,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                       | 32/100 [00:50<01:44,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                      | 33/100 [00:52<01:42,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                     | 34/100 [00:53<01:40,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 35/100 [00:55<01:38,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                   | 36/100 [00:56<01:37,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 37/100 [00:58<01:35,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                  | 38/100 [00:59<01:34,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 39/100 [01:01<01:33,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 40/100 [01:02<01:32,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 41/100 [01:04<01:31,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                               | 42/100 [01:06<01:30,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 43/100 [01:07<01:28,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                             | 44/100 [01:09<01:27,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 45/100 [01:10<01:26,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 46/100 [01:12<01:25,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 47/100 [01:14<01:24,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 48/100 [01:15<01:22,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                         | 49/100 [01:17<01:20,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 50/100 [01:18<01:18,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                       | 51/100 [01:20<01:16,  1.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50, Loss: 0.05382584407925606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                       | 52/100 [01:21<01:15,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                      | 53/100 [01:23<01:13,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                     | 54/100 [01:24<01:11,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 55/100 [01:26<01:12,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                   | 56/100 [01:28<01:11,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 57/100 [01:29<01:09,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 58/100 [01:31<01:08,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 59/100 [01:33<01:06,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 60/100 [01:34<01:04,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 61/100 [01:36<01:02,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 62/100 [01:37<01:00,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 63/100 [01:39<00:59,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 64/100 [01:41<00:58,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 65/100 [01:42<00:56,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 66/100 [01:44<00:53,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 67/100 [01:45<00:51,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 68/100 [01:47<00:49,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 69/100 [01:48<00:47,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 70/100 [01:50<00:46,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 71/100 [01:51<00:44,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 72/100 [01:53<00:44,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/100 [01:55<00:43,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 74/100 [01:56<00:41,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 75/100 [01:58<00:39,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 76/100 [02:00<00:38,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 77/100 [02:01<00:36,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 78/100 [02:03<00:35,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 79/100 [02:04<00:33,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 80/100 [02:06<00:32,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 81/100 [02:08<00:30,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 82/100 [02:09<00:28,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 83/100 [02:11<00:26,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 84/100 [02:12<00:24,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 85/100 [02:14<00:23,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 86/100 [02:15<00:21,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 87/100 [02:17<00:20,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 88/100 [02:18<00:18,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 89/100 [02:20<00:17,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 90/100 [02:22<00:15,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 91/100 [02:23<00:14,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 92/100 [02:25<00:12,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 93/100 [02:26<00:10,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 94/100 [02:28<00:09,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 95/100 [02:29<00:07,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 96/100 [02:31<00:06,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 97/100 [02:33<00:04,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 98/100 [02:34<00:03,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 99/100 [02:36<00:01,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:38<00:00,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                         | 1/2 [02:38<02:38, 158.02s/it]\u001b[A\n",
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|â–Š                                                                                 | 1/100 [00:01<02:37,  1.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.05611332505941391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  2%|â–ˆâ–‹                                                                                | 2/100 [00:03<02:37,  1.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 0.05565597489476204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  3%|â–ˆâ–ˆâ–                                                                               | 3/100 [00:04<02:35,  1.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, Loss: 0.05488254129886627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  4%|â–ˆâ–ˆâ–ˆâ–Ž                                                                              | 4/100 [00:06<02:33,  1.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, Loss: 0.05419706553220749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  5%|â–ˆâ–ˆâ–ˆâ–ˆ                                                                              | 5/100 [00:08<02:31,  1.60s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, Loss: 0.05410761758685112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  6%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                             | 6/100 [00:09<02:32,  1.62s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, Loss: 0.05392530560493469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  7%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                            | 7/100 [00:11<02:31,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                           | 8/100 [00:13<02:31,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                          | 9/100 [00:14<02:31,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                         | 10/100 [00:16<02:29,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                        | 11/100 [00:18<02:29,  1.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                                       | 12/100 [00:19<02:26,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                      | 13/100 [00:21<02:23,  1.65s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, Loss: 0.05228666588664055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                     | 14/100 [00:22<02:21,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                    | 15/100 [00:24<02:20,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                    | 16/100 [00:26<02:19,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                   | 17/100 [00:27<02:17,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                  | 18/100 [00:29<02:16,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                 | 19/100 [00:31<02:14,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                | 20/100 [00:32<02:12,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                | 21/100 [00:34<02:11,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                               | 22/100 [00:36<02:09,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                              | 23/100 [00:37<02:06,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                             | 24/100 [00:39<02:06,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                            | 25/100 [00:41<02:03,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                            | 26/100 [00:42<02:01,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                           | 27/100 [00:44<01:59,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                          | 28/100 [00:46<01:58,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                         | 29/100 [00:47<01:57,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                        | 30/100 [00:49<01:56,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                        | 31/100 [00:51<01:54,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                       | 32/100 [00:52<01:52,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                                      | 33/100 [00:54<01:50,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                     | 34/100 [00:56<01:49,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                    | 35/100 [00:57<01:47,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                   | 36/100 [00:59<01:46,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 37/100 [01:01<01:44,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                  | 38/100 [01:02<01:42,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                 | 39/100 [01:04<01:41,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 40/100 [01:06<01:39,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                               | 41/100 [01:07<01:38,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                               | 42/100 [01:09<01:36,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                              | 43/100 [01:10<01:34,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                             | 44/100 [01:12<01:32,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                            | 45/100 [01:14<01:31,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                           | 46/100 [01:15<01:29,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                           | 47/100 [01:17<01:27,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                          | 48/100 [01:19<01:25,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                         | 49/100 [01:20<01:24,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                        | 50/100 [01:22<01:22,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                       | 51/100 [01:24<01:21,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                       | 52/100 [01:25<01:19,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                      | 53/100 [01:27<01:17,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                     | 54/100 [01:29<01:16,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 55/100 [01:30<01:14,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                   | 56/100 [01:32<01:11,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 57/100 [01:33<01:09,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                  | 58/100 [01:35<01:06,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 59/100 [01:37<01:04,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 60/100 [01:38<01:03,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 61/100 [01:40<01:01,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 62/100 [01:41<01:00,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 63/100 [01:43<00:58,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 64/100 [01:45<00:56,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 65/100 [01:46<00:54,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 66/100 [01:48<00:52,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 67/100 [01:49<00:50,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 68/100 [01:51<00:48,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 69/100 [01:52<00:47,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 70/100 [01:54<00:45,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 71/100 [01:55<00:44,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 72/100 [01:57<00:42,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/100 [01:58<00:40,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 74/100 [02:00<00:39,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 75/100 [02:01<00:37,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 76/100 [02:03<00:36,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                  | 77/100 [02:04<00:35,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 78/100 [02:06<00:34,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 79/100 [02:07<00:32,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 80/100 [02:09<00:30,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 81/100 [02:10<00:29,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 82/100 [02:12<00:27,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 83/100 [02:13<00:26,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 84/100 [02:15<00:24,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 85/100 [02:17<00:23,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 86/100 [02:18<00:21,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 87/100 [02:20<00:20,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 88/100 [02:21<00:18,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 89/100 [02:23<00:17,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 90/100 [02:24<00:15,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 91/100 [02:26<00:14,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 92/100 [02:28<00:12,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 93/100 [02:29<00:10,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 94/100 [02:31<00:09,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 95/100 [02:32<00:07,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 96/100 [02:34<00:06,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 97/100 [02:35<00:04,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 98/100 [02:37<00:03,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 99/100 [02:38<00:01,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:40<00:00,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [05:18<00:00, 159.29s/it]\u001b[A\n",
      "  0%|                                                                                           | 0/23 [05:18<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "name = 'epoch_200'\n",
    "#try:\n",
    "out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_conv_{name}/\"\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = DrumConvTasnet.load_from_checkpoint(f'D:/Github/phd-drum-sep/analysis/conv_tasnet_model_analysis/checkpoint/{name}.ckpt')\n",
    "model = model.eval()\n",
    "\n",
    "results = museval.EvalStore(frames_agg='median', tracks_agg='median')\n",
    "for track in tqdm(all_tracks):\n",
    "\n",
    "    mixture_tensor = load_audio(track.mix_path).unsqueeze(0).to(model.device)\n",
    "    snippet_length = (mixture_tensor.shape[2] // (44100 * 16)) * (44100 * 4)\n",
    "    mixture_tensor = mixture_tensor[:,:, :snippet_length]\n",
    "\n",
    "    drum_tensor = load_audio(track.drum_path).unsqueeze(0)\n",
    "    drum_tensor = torch.cat([drum_tensor, drum_tensor], dim=1).to(model.device)\n",
    "    drum_tensor = drum_tensor[:,:, :snippet_length]\n",
    "\n",
    "    shape = mixture_tensor.shape[2]\n",
    "    roll_tensor = load_roll(track.midi_path, shape).unsqueeze(0).to(model.device)\n",
    "    roll_tensor = roll_tensor[:,:, :snippet_length]\n",
    "\n",
    "    proposed_answers = []\n",
    "    \n",
    "    device = mixture_tensor.device\n",
    "    batch, channels, length = mixture_tensor.shape\n",
    "    chunk_len = int(44100 * 4)\n",
    "\n",
    "    for start in tqdm(range(0, length, chunk_len)):\n",
    "        end = start + chunk_len\n",
    "        answer = find_best(mixture_tensor[:,:,start:end], drum_tensor[:,:, start:end])\n",
    "        proposed_answers.append(answer)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74b2165f-3a25-4ffb-90f2-d8c1ad6879ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_torch(transcription, prediction):\n",
    "    TPs = torch.sum((transcription == 1) & (prediction == 1), dim=1)\n",
    "    FPs = torch.sum((transcription == 0) & (prediction == 1), dim=1)\n",
    "    FNs = torch.sum((transcription == 1) & (prediction == 0), dim=1)\n",
    "\n",
    "    precision = TPs.float() / (TPs + FPs).float()\n",
    "    recall = TPs.float() / (TPs + FNs).float()\n",
    "\n",
    "    # Handle potential division by zero for precision and recall\n",
    "    precision[torch.isnan(precision)] = 0\n",
    "    recall[torch.isnan(recall)] = 0\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f9974d7-61e7-483f-83d0-733710827e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f_measure(precision, recall, beta=1):\n",
    "    \"\"\"\n",
    "    Calculate the F-measure for each class and the average F-measure.\n",
    "\n",
    "    Parameters:\n",
    "    - precision: Tensor of precision values per class.\n",
    "    - recall: Tensor of recall values per class.\n",
    "    - beta: Weight of recall in the harmonic mean.\n",
    "\n",
    "    Returns:\n",
    "    - f_measure: Tensor of F-measure for each class.\n",
    "    - average_f_measure: Scalar, average F-measure across all classes.\n",
    "    \"\"\"\n",
    "    numerator = (1 + beta**2) * precision * recall\n",
    "    denominator = (beta**2 * precision) + recall\n",
    "\n",
    "    # Avoid division by zero\n",
    "    denominator[denominator == 0] = 1\n",
    "\n",
    "    f_measure = numerator / denominator\n",
    "\n",
    "    # Handle potential NaN values\n",
    "    f_measure[torch.isnan(f_measure)] = 0\n",
    "\n",
    "    average_f_measure = torch.mean(f_measure)\n",
    "\n",
    "    return f_measure, average_f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10dcacba-b189-4c22-b5f7-7af87c55b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded = [expand(p).unsqueeze(0) for p in proposed_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3286af2c-ad08-4bfd-be6d-c62e149073c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([0.2344, 0.0909, 0.0000, 0.0000, 0.0000], device='cuda:0') tensor([0.6591, 0.3976, 0.0000, 0.0000, 0.0000], device='cuda:0')\n",
      "1 tensor([0.1297, 0.0588, 0.0000, 0.0000, 0.0217], device='cuda:0') tensor([0.5000, 0.3333, 0.0000, 0.0000, 0.4999], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pres = [[],[],[],[],[]]\n",
    "recs = [[],[],[],[],[]]\n",
    "for idx, val in enumerate(expanded):\n",
    "    segment = 44100 * 4\n",
    "    start = idx * segment\n",
    "    end = start + segment\n",
    "    slice = roll_tensor[:, :, start:end].to(model.device).squeeze(0)\n",
    "    pred = val.to(model.device).squeeze(0)\n",
    "    pre, rec = calculate_precision_recall_torch(slice, pred)\n",
    "\n",
    "    for drum in range(5):\n",
    "        pres[drum].append(pre[drum].unsqueeze(0))\n",
    "        recs[drum].append(rec[drum].unsqueeze(0))\n",
    "    print(idx, pre, rec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e43f19-0727-4a7b-b131-f97c4530bcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(torch.cat(pres[0])), torch.mean(torch.cat(recs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a78b5-9038-4cd5-8900-f900295ca669",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_f_measure(torch.mean(torch.cat(pres[0])), torch.mean(torch.cat(recs[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e8996a-9236-4bcb-97fc-14ba8dabaed8",
   "metadata": {},
   "source": [
    "# mir eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f31df74a-bf73-4cf2-9039-7301456f8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cd106f2a-4e3a-4091-883a-de35ac8e0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_tensor_to_times(binary_tensor, block_size=4410):\n",
    "    drum_hit_times = [[], [], [], [], []]\n",
    "    time_step = 1 / 44100\n",
    "    \n",
    "    for start in range(0, binary_tensor.shape[-1], block_size):\n",
    "        end = start + block_size\n",
    "\n",
    "        for channel in range(5):\n",
    "            # print(start, end, channel)\n",
    "            value = binary_tensor[:, start:end]\n",
    "            value = torch.mean(value).item()\n",
    "\n",
    "            if value > 0.5:\n",
    "                in_seconds = ((start * time_step) // 0.1)  * 0.1\n",
    "                drum_hit_times[channel].append(in_seconds)\n",
    "            \n",
    "    return drum_hit_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "74caac81-639c-4a3f-99fc-1deee48a4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = torch.cat(expanded, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "56b6b0c3-7ec8-47c6-8176-f794a9148234",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = exp.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2333a9b3-7941-4caf-9fbe-4c50dbb3c0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_hit_times = binary_tensor_to_times(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40da68f7-790e-4a0d-9c82-9b8836041c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt =  roll_tensor.squeeze(0)\n",
    "gt_hit_times = binary_tensor_to_times(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c9781bc9-a949-42d0-bf3b-5165b70f71df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: F-measure\n",
      "Recall: Precision\n",
      "F-measure: Recall\n"
     ]
    }
   ],
   "source": [
    "score = mir_eval.onset.evaluate(np.asarray(gt_hit_times[0]), np.asarray(drum_hit_times[0]), window=1)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F-measure: {f_measure}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad882d15-8e26-4dfd-9be4-46b87ce830df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('F-measure', 0.0), ('Precision', 0.0), ('Recall', 0.0)])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf83dac-f66d-4f70-9354-43978eaeac77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fb3e7-33e0-4a4a-bfcf-94bcfe153a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
