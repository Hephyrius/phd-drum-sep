{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b2a3bb-3d01-4f42-bc37-dff46522af7f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d686e6-8847-4861-88f0-58141c363862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "import plotly.graph_objects as go\n",
    "from torch.optim import lr_scheduler\n",
    "from IPython.display import Audio\n",
    "from torchaudio.transforms import Fade\n",
    "import musdb\n",
    "import museval\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "275ddd3f-aa3d-4444-8ddd-c8874bac940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import subprocess\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "from torch.optim import lr_scheduler\n",
    "import pretty_midi\n",
    "from typing import Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091eea44-52d4-44e1-ab82-863b9954851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, name, midi_path, drum_path, mix_path):\n",
    "        self.name = name\n",
    "        self.midi_path = midi_path\n",
    "        self.drum_path = drum_path\n",
    "        self.mix_path = mix_path\n",
    "        self.targets = {'drums': '', 'bass': ''}\n",
    "        self.rate = 44100\n",
    "        self.subset = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c82506-04c8-4200-9e63-26857aa176bd",
   "metadata": {},
   "source": [
    "# Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3acd3b-d177-44d3-a3f4-35140f45e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 3407\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf49cdd4-f6c9-4df4-adc8-f4e98f24e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/full_mix/'\n",
    "mixes = os.listdir(mix_folder)\n",
    "mixes = [mix_folder + m for m in mixes]\n",
    "\n",
    "drum_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/drum_only/'\n",
    "drum = os.listdir(drum_folder)\n",
    "drum = [drum_folder + d for d in drum]\n",
    "\n",
    "beats_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/beats/'\n",
    "beats = os.listdir(beats_folder)\n",
    "beats = [beats_folder + b for b in beats]#\n",
    "\n",
    "class_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/'\n",
    "classes = os.listdir(class_folder)\n",
    "classes = [class_folder + c for c in classes]\n",
    "\n",
    "midi_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/midi/'\n",
    "midis = os.listdir(midi_folder)\n",
    "midis = [midi_folder + m for m in midis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac376c2d-6c64-4901-a1f6-687c59d7a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tracks = []\n",
    "for idx, val in tqdm(enumerate(classes)):\n",
    "\n",
    "    name = val.replace('D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/', '')\n",
    "    name = name.replace('_subclass.txt', '')\n",
    "\n",
    "    t = Track(name, midis[idx], drum[idx], mixes[idx])\n",
    "    all_tracks.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182916a-aec1-4c05-8a08-4d2e44dfd979",
   "metadata": {},
   "source": [
    "# Construct Teh Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d4eb9a-f08d-4452-a4ad-b9769a218278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_transcription_into_roll(transcription, frames):\n",
    "    # Determine your sampling frequency (frames per second)\n",
    "    fs = 44100\n",
    "    \n",
    "    piano_roll_length = int(frames)\n",
    "    \n",
    "    # Initialize the piano roll array\n",
    "    piano_roll = np.zeros((64, piano_roll_length))\n",
    "    \n",
    "    # Fill in the piano roll array\n",
    "    for note in transcription.instruments[0].notes:\n",
    "        # Convert start and end times to frame indices\n",
    "        start_frame = int(np.floor(note.start * fs))\n",
    "        end_frame = int(np.ceil(note.end * fs))\n",
    "        \n",
    "        # Set the corresponding frames to 1 (or note.velocity for a velocity-sensitive representation)\n",
    "        piano_roll[note.pitch, start_frame:end_frame] = 1  # Or use note.velocity\n",
    "        \n",
    "    roll = np.vstack([piano_roll[35:36, :], piano_roll[38:39, :], piano_roll[42:43, :], piano_roll[47:48, :], piano_roll[49:50, :]])\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988d30-a95e-4dca-ac19-a6f7cea95717",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72a57b5-7115-421d-b26e-3ca3d21fef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvBlock(torch.nn.Module):\n",
    "    \"\"\"1D Convolutional block.\n",
    "\n",
    "    Args:\n",
    "        io_channels (int): The number of input/output channels, <B, Sc>\n",
    "        hidden_channels (int): The number of channels in the internal layers, <H>.\n",
    "        kernel_size (int): The convolution kernel size of the middle layer, <P>.\n",
    "        padding (int): Padding value of the convolution in the middle layer.\n",
    "        dilation (int, optional): Dilation value of the convolution in the middle layer.\n",
    "        no_redisual (bool, optional): Disable residual block/output.\n",
    "\n",
    "    Note:\n",
    "        This implementation corresponds to the \"non-causal\" setting in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        io_channels: int,\n",
    "        hidden_channels: int,\n",
    "        kernel_size: int,\n",
    "        padding: int,\n",
    "        dilation: int = 1,\n",
    "        no_residual: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=io_channels, out_channels=hidden_channels, kernel_size=1),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.GroupNorm(num_groups=1, num_channels=hidden_channels, eps=1e-08),\n",
    "            torch.nn.Conv1d(\n",
    "                in_channels=hidden_channels,\n",
    "                out_channels=hidden_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                dilation=dilation,\n",
    "                groups=hidden_channels,\n",
    "            ),\n",
    "            torch.nn.PReLU(),\n",
    "            torch.nn.GroupNorm(num_groups=1, num_channels=hidden_channels, eps=1e-08),\n",
    "        )\n",
    "\n",
    "        self.res_out = (\n",
    "            None\n",
    "            if no_residual\n",
    "            else torch.nn.Conv1d(in_channels=hidden_channels, out_channels=io_channels, kernel_size=1)\n",
    "        )\n",
    "        self.skip_out = torch.nn.Conv1d(in_channels=hidden_channels, out_channels=io_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> Tuple[Optional[torch.Tensor], torch.Tensor]:\n",
    "        feature = self.conv_layers(input)\n",
    "        if self.res_out is None:\n",
    "            residual = None\n",
    "        else:\n",
    "            residual = self.res_out(feature)\n",
    "        skip_out = self.skip_out(feature)\n",
    "        return residual, skip_out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "class MaskGenerator(torch.nn.Module):\n",
    "    \"\"\"TCN (Temporal Convolution Network) Separation Module\n",
    "\n",
    "    Generates masks for separation.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Input feature dimension, <N>.\n",
    "        num_sources (int): The number of sources to separate.\n",
    "        kernel_size (int): The convolution kernel size of conv blocks, <P>.\n",
    "        num_featrs (int): Input/output feature dimenstion of conv blocks, <B, Sc>.\n",
    "        num_hidden (int): Intermediate feature dimention of conv blocks, <H>\n",
    "        num_layers (int): The number of conv blocks in one stack, <X>.\n",
    "        num_stacks (int): The number of conv block stacks, <R>.\n",
    "        msk_activate (str): The activation function of the mask output.\n",
    "\n",
    "    Note:\n",
    "        This implementation corresponds to the \"non-causal\" setting in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_sources: int,\n",
    "        kernel_size: int,\n",
    "        num_feats: int,\n",
    "        num_hidden: int,\n",
    "        num_layers: int,\n",
    "        num_stacks: int,\n",
    "        msk_activate: str,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_sources = num_sources\n",
    "\n",
    "        self.input_norm = torch.nn.GroupNorm(num_groups=1, num_channels=input_dim, eps=1e-8)\n",
    "        self.input_conv = torch.nn.Conv1d(in_channels=input_dim, out_channels=num_feats, kernel_size=1)\n",
    "\n",
    "        self.receptive_field = 0\n",
    "        self.conv_layers = torch.nn.ModuleList([])\n",
    "        for s in range(num_stacks):\n",
    "            for l in range(num_layers):\n",
    "                multi = 2**l\n",
    "                self.conv_layers.append(\n",
    "                    ConvBlock(\n",
    "                        io_channels=num_feats,\n",
    "                        hidden_channels=num_hidden,\n",
    "                        kernel_size=kernel_size,\n",
    "                        dilation=multi,\n",
    "                        padding=multi,\n",
    "                        # The last ConvBlock does not need residual\n",
    "                        no_residual=(l == (num_layers - 1) and s == (num_stacks - 1)),\n",
    "                    )\n",
    "                )\n",
    "                self.receptive_field += kernel_size if s == 0 and l == 0 else (kernel_size - 1) * multi\n",
    "        self.output_prelu = torch.nn.PReLU()\n",
    "        self.output_conv = torch.nn.Conv1d(\n",
    "            in_channels=num_feats,\n",
    "            out_channels=input_dim * num_sources,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        if msk_activate == \"sigmoid\":\n",
    "            self.mask_activate = torch.nn.Sigmoid()\n",
    "        elif msk_activate == \"relu\":\n",
    "            self.mask_activate = torch.nn.ReLU()\n",
    "        elif msk_activate == \"prelu\":\n",
    "            self.mask_activate = torch.nn.PReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation {msk_activate}\")\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Generate separation mask.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): 3D Tensor with shape [batch, features, frames]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: shape [batch, num_sources, features, frames]\n",
    "        \"\"\"\n",
    "        batch_size = input.shape[0]\n",
    "        feats = self.input_norm(input)\n",
    "        feats = self.input_conv(feats)\n",
    "        output = 0.0\n",
    "        for layer in self.conv_layers:\n",
    "            residual, skip = layer(feats)\n",
    "            if residual is not None:  # the last conv layer does not produce residual\n",
    "                feats = feats + residual\n",
    "            output = output + skip\n",
    "        output = self.output_prelu(output)\n",
    "        output = self.output_conv(output)\n",
    "        output = self.mask_activate(output)\n",
    "        return output.view(batch_size, self.num_sources, self.input_dim, -1)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "class ConvTasNet(torch.nn.Module):\n",
    "    \"\"\"Conv-TasNet architecture introduced in\n",
    "    *Conv-TasNet: Surpassing Ideal Time–Frequency Magnitude Masking for Speech Separation*\n",
    "    :cite:`Luo_2019`.\n",
    "\n",
    "    Note:\n",
    "        This implementation corresponds to the \"non-causal\" setting in the paper.\n",
    "\n",
    "    See Also:\n",
    "        * :class:`torchaudio.pipelines.SourceSeparationBundle`: Source separation pipeline with pre-trained models.\n",
    "\n",
    "    Args:\n",
    "        num_sources (int, optional): The number of sources to split.\n",
    "        enc_kernel_size (int, optional): The convolution kernel size of the encoder/decoder, <L>.\n",
    "        enc_num_feats (int, optional): The feature dimensions passed to mask generator, <N>.\n",
    "        msk_kernel_size (int, optional): The convolution kernel size of the mask generator, <P>.\n",
    "        msk_num_feats (int, optional): The input/output feature dimension of conv block in the mask generator, <B, Sc>.\n",
    "        msk_num_hidden_feats (int, optional): The internal feature dimension of conv block of the mask generator, <H>.\n",
    "        msk_num_layers (int, optional): The number of layers in one conv block of the mask generator, <X>.\n",
    "        msk_num_stacks (int, optional): The numbr of conv blocks of the mask generator, <R>.\n",
    "        msk_activate (str, optional): The activation function of the mask output (Default: ``sigmoid``).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_sources: int = 2,\n",
    "        # encoder/decoder parameters\n",
    "        enc_kernel_size: int = 16,\n",
    "        enc_num_feats: int = 512,\n",
    "        # mask generator parameters\n",
    "        msk_kernel_size: int = 3,\n",
    "        msk_num_feats: int = 128,\n",
    "        msk_num_hidden_feats: int = 512,\n",
    "        msk_num_layers: int = 8,\n",
    "        msk_num_stacks: int = 3,\n",
    "        msk_activate: str = \"sigmoid\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_sources = num_sources\n",
    "        self.enc_num_feats = enc_num_feats\n",
    "        self.enc_kernel_size = enc_kernel_size\n",
    "        self.enc_stride = enc_kernel_size // 2\n",
    "\n",
    "        self.encoder = torch.nn.Conv1d(\n",
    "            in_channels=7,\n",
    "            out_channels=enc_num_feats,\n",
    "            kernel_size=enc_kernel_size,\n",
    "            stride=self.enc_stride,\n",
    "            padding=self.enc_stride,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.mask_generator = MaskGenerator(\n",
    "            input_dim=enc_num_feats,\n",
    "            num_sources=num_sources,\n",
    "            kernel_size=msk_kernel_size,\n",
    "            num_feats=msk_num_feats,\n",
    "            num_hidden=msk_num_hidden_feats,\n",
    "            num_layers=msk_num_layers,\n",
    "            num_stacks=msk_num_stacks,\n",
    "            msk_activate=msk_activate,\n",
    "        )\n",
    "        self.decoder = torch.nn.ConvTranspose1d(\n",
    "            in_channels=enc_num_feats,\n",
    "            out_channels=2,\n",
    "            kernel_size=enc_kernel_size,\n",
    "            stride=self.enc_stride,\n",
    "            padding=self.enc_stride,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def _align_num_frames_with_strides(self, input: torch.Tensor) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"Pad input Tensor so that the end of the input tensor corresponds with\n",
    "\n",
    "        1. (if kernel size is odd) the center of the last convolution kernel\n",
    "        or 2. (if kernel size is even) the end of the first half of the last convolution kernel\n",
    "\n",
    "        Assumption:\n",
    "            The resulting Tensor will be padded with the size of stride (== kernel_width // 2)\n",
    "            on the both ends in Conv1D\n",
    "\n",
    "        |<--- k_1 --->|\n",
    "        |      |            |<-- k_n-1 -->|\n",
    "        |      |                  |  |<--- k_n --->|\n",
    "        |      |                  |         |      |\n",
    "        |      |                  |         |      |\n",
    "        |      v                  v         v      |\n",
    "        |<---->|<--- input signal --->|<--->|<---->|\n",
    "         stride                         PAD  stride\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): 3D Tensor with shape (batch_size, channels==1, frames)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Padded Tensor\n",
    "            int: Number of paddings performed\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, num_frames = input.shape\n",
    "        is_odd = self.enc_kernel_size % 2\n",
    "        num_strides = (num_frames - is_odd) // self.enc_stride\n",
    "        num_remainings = num_frames - (is_odd + num_strides * self.enc_stride)\n",
    "        if num_remainings == 0:\n",
    "            return input, 0\n",
    "\n",
    "        num_paddings = self.enc_stride - num_remainings\n",
    "        pad = torch.zeros(\n",
    "            batch_size,\n",
    "            num_channels,\n",
    "            num_paddings,\n",
    "            dtype=input.dtype,\n",
    "            device=input.device,\n",
    "        )\n",
    "        return torch.cat([input, pad], 2), num_paddings\n",
    "    \n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Perform source separation. Generate audio source waveforms.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): 3D Tensor with shape [batch, channel==1, frames]\n",
    "\n",
    "        Returns:\n",
    "            Tensor: 3D Tensor with shape [batch, channel==num_sources, frames]\n",
    "        \"\"\"\n",
    "\n",
    "        # B: batch size\n",
    "        # L: input frame length\n",
    "        # L': padded input frame length\n",
    "        # F: feature dimension\n",
    "        # M: feature frame length\n",
    "        # S: number of sources\n",
    "\n",
    "        padded, num_pads = self._align_num_frames_with_strides(input)  # B, 1, L'\n",
    "        batch_size, num_padded_frames = padded.shape[0], padded.shape[2]\n",
    "        feats = self.encoder(padded)  # B, F, M\n",
    "        masked = self.mask_generator(feats) * feats.unsqueeze(1)  # B, S, F, M\n",
    "        masked = masked.view(batch_size * self.num_sources, self.enc_num_feats, -1)  # B*S, F, M\n",
    "        decoded = self.decoder(masked)  # B*S, 1, L'\n",
    "        out = decoded.reshape(batch_size, 4, -1)\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "class DrumConvTasnet(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(DrumConvTasnet, self).__init__()\n",
    "\n",
    "        self.loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
    "                    fft_sizes=[1024, 2048, 4096],\n",
    "                    hop_sizes=[256, 512, 1024],\n",
    "                    win_lengths=[1024, 2048, 4096],\n",
    "                    scale=\"mel\", \n",
    "                    n_bins=150,\n",
    "                    sample_rate=44100,\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "\n",
    "        self.loss_fn_2 = auraloss.time.SISDRLoss()\n",
    "\n",
    "        self.loss_fn_3 = torch.nn.L1Loss()\n",
    "\n",
    "        self.loss_used = 0\n",
    "        \n",
    "        self.conv_tasnet =  ConvTasNet(\n",
    "            num_sources=2,\n",
    "            enc_kernel_size=16,\n",
    "            enc_num_feats=512,\n",
    "            msk_kernel_size=3,\n",
    "            msk_num_feats=128,\n",
    "            msk_num_hidden_feats=512,\n",
    "            msk_num_layers=8,\n",
    "            msk_num_stacks=3,\n",
    "            msk_activate=\"prelu\",\n",
    "        )\n",
    "\n",
    "        self.out = nn.Conv1d(4, 2, kernel_size=1)\n",
    "\n",
    "    def compute_loss(self, outputs, ref_signals):\n",
    "        loss = self.loss_fn(outputs, ref_signals) + self.loss_fn_2(outputs, ref_signals) +  self.loss_fn_3(outputs, ref_signals)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, audio, drumroll):\n",
    "        to_mix = torch.cat([audio, drumroll], axis=1)\n",
    "        out = self.conv_tasnet(to_mix)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        audio, drum, drumroll = batch\n",
    "        \n",
    "        outputs = self.forward(audio, drumroll)\n",
    "        # print(outputs.size())\n",
    "\n",
    "        if batch_idx % 64 == 0:\n",
    "            input_signal = audio[0].cpu().detach().numpy().T\n",
    "            generated_signal = outputs[0].cpu().detach().numpy().T\n",
    "            drum_signal = drum[0].cpu().detach().numpy().T \n",
    "            wandb.log({'audio_input': [wandb.Audio(input_signal, caption=\"Input\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_reference': [wandb.Audio(drum_signal, caption=\"Reference\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_output': [wandb.Audio(generated_signal, caption=\"Output\", sample_rate=44100)]})\n",
    "             \n",
    "            for i in range(5):\n",
    "                wandb.log({f'drum_{i + 1}': [wandb.Audio(drumroll[0].cpu().detach().numpy()[i, :], caption=\"Output\", sample_rate=44100)]})\n",
    "\n",
    "\n",
    "        loss = self.compute_loss(outputs, drum)         \n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer and optionally learning rate scheduler here\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d756ac7-1290-4063-b7fc-aa4c6fe99249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999987\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.09999999999999964\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000053\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000853\n",
      "0.10000000000000853\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000002274\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n"
     ]
    }
   ],
   "source": [
    "path='D:/Github/phd-drum-sep/data/adtof/test/Al James - Schoolboy Facination/adtof.mid'\n",
    "transcription = pretty_midi.PrettyMIDI(path)\n",
    "for note in transcription.instruments[0].notes:\n",
    "    print(note.end-note.start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6761dda3-88da-4175-ad3a-f67ff9ac431c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_PrettyMIDI__tick_to_time',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_load_instruments',\n",
       " '_load_metadata',\n",
       " '_load_tempo_changes',\n",
       " '_tick_scales',\n",
       " '_update_tick_to_time',\n",
       " 'adjust_times',\n",
       " 'estimate_beat_start',\n",
       " 'estimate_tempi',\n",
       " 'estimate_tempo',\n",
       " 'fluidsynth',\n",
       " 'get_beats',\n",
       " 'get_chroma',\n",
       " 'get_downbeats',\n",
       " 'get_end_time',\n",
       " 'get_onsets',\n",
       " 'get_piano_roll',\n",
       " 'get_pitch_class_histogram',\n",
       " 'get_pitch_class_transition_matrix',\n",
       " 'get_tempo_changes',\n",
       " 'instruments',\n",
       " 'key_signature_changes',\n",
       " 'lyrics',\n",
       " 'remove_invalid_notes',\n",
       " 'resolution',\n",
       " 'synthesize',\n",
       " 'text_events',\n",
       " 'tick_to_time',\n",
       " 'time_signature_changes',\n",
       " 'time_to_tick',\n",
       " 'write']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45654e9b-e0ba-4177-bfd1-9860bfc2f3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999999\n",
      "0.09999999999999998\n",
      "0.09999999999999998\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000009\n",
      "0.10000000000000009\n",
      "0.10000000000000053\n",
      "0.10000000000000053\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000053\n",
      "0.10000000000000053\n",
      "0.10000000000000053\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.10000000000000142\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999964\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999787\n",
      "0.09999999999999787\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.10000000000000142\n",
      "0.09999999999999432\n",
      "0.09999999999999432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19333dafd10>,\n",
       " <matplotlib.lines.Line2D at 0x19333ccd490>,\n",
       " <matplotlib.lines.Line2D at 0x19333db8610>,\n",
       " <matplotlib.lines.Line2D at 0x19333db8950>,\n",
       " <matplotlib.lines.Line2D at 0x19333db8e50>,\n",
       " <matplotlib.lines.Line2D at 0x19333db9350>,\n",
       " <matplotlib.lines.Line2D at 0x19333db93d0>,\n",
       " <matplotlib.lines.Line2D at 0x193336a55d0>,\n",
       " <matplotlib.lines.Line2D at 0x19333dba010>,\n",
       " <matplotlib.lines.Line2D at 0x19333dba550>,\n",
       " <matplotlib.lines.Line2D at 0x19333d929d0>,\n",
       " <matplotlib.lines.Line2D at 0x19333dbad10>,\n",
       " <matplotlib.lines.Line2D at 0x19333dbb0d0>,\n",
       " <matplotlib.lines.Line2D at 0x19333dbb650>,\n",
       " <matplotlib.lines.Line2D at 0x19333dbbb50>,\n",
       " <matplotlib.lines.Line2D at 0x19333dbbf90>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc2e50>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc0390>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc0850>,\n",
       " <matplotlib.lines.Line2D at 0x19333cd1fd0>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc1350>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc1610>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc1dd0>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc2410>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc2650>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc2d50>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc30d0>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc3650>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc3b10>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc3f90>,\n",
       " <matplotlib.lines.Line2D at 0x19333dc9e90>,\n",
       " <matplotlib.lines.Line2D at 0x19333b99a50>,\n",
       " <matplotlib.lines.Line2D at 0x19333a91590>,\n",
       " <matplotlib.lines.Line2D at 0x19333d59190>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transcription = pretty_midi.PrettyMIDI(midis[0])\n",
    "for note in transcription.instruments[0].notes:\n",
    "    print(note.end-note.start)\n",
    "plt.plot(transcription.get_piano_roll(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d11190-4e68-41f8-ae19-36d8199c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    audio_tensors = []\n",
    "    waveform, _ = torchaudio.load(path)\n",
    "    return waveform\n",
    "\n",
    "def load_roll(path, frames):\n",
    "    transcription = pretty_midi.PrettyMIDI(path)\n",
    "    roll = turn_transcription_into_roll(transcription, frames)\n",
    "\n",
    "    return torch.from_numpy(roll).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a00457-307d-4743-9784-9f4cdae73730",
   "metadata": {},
   "source": [
    "# SISNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3873fcee-a3cb-4975-98f5-fc50e77883fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sources(\n",
    "    model,\n",
    "    mix,\n",
    "    drumroll,\n",
    "    segment=4.0,\n",
    "    overlap=0,\n",
    "    device=None,\n",
    "    sample_rate=44100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply model to a given mixture. Use fade, and add segments together in order to add model segment by segment.\n",
    "\n",
    "    Args:\n",
    "        segment (int): segment length in seconds\n",
    "        device (torch.device, str, or None): if provided, device on which to\n",
    "            execute the computation, otherwise `mix.device` is assumed.\n",
    "            When `device` is different from `mix.device`, only local computations will\n",
    "            be on `device`, while the entire tracks will be stored on `mix.device`.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "\n",
    "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * sample_rate\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape=\"linear\")\n",
    "\n",
    "    final = torch.zeros(batch, channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        chunk = mix[:, :, start:end]\n",
    "        roll = drumroll[:, :, start:end]\n",
    "        # print(torch.max(roll))\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            out = model.forward(chunk, roll)\n",
    "        # print(out.shape)\n",
    "        out = fade(out)\n",
    "        # print(out.shape)\n",
    "        try:\n",
    "            final[:, :, start:end] += out\n",
    "        except:\n",
    "            pass\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "        end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b97a63a5-d58e-47db-9768-e063efd5da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioData:\n",
    "    def __init__(self, audio):\n",
    "        self.audio = audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf06e62-3267-4faf-838f-70f14be50e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\pytorch_lightning\\utilities\\migration\\utils.py:55: The loaded checkpoint was produced with Lightning v2.2.1, which is newer than your current Lightning version: v2.1.2\n",
      "  0%|                                                                                           | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 1628160])\n",
      "torch.Size([1, 1, 1628160])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▌                                                                               | 1/23 [00:10<03:41, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -1.477  SIR: 215.466  ISR:  -0.627  SAR:  -2.790  \n",
      "bass            ==> SDR:  -1.694  SIR: 219.542  ISR:  -0.763  SAR:  -2.676  \n",
      "mean            ==> SDR:  -1.584  SIR: 227.633  ISR:  -0.694  SAR:  -2.732  \n",
      "\n",
      "torch.Size([1, 2, 1604072])\n",
      "torch.Size([1, 1, 1604072])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▏                                                                           | 2/23 [00:16<02:51,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -4.518  SIR: 153.382  ISR:  -2.870  SAR:   4.104  \n",
      "bass            ==> SDR:  -5.166  SIR: 152.133  ISR:  -3.553  SAR:   4.068  \n",
      "mean            ==> SDR:  -4.830  SIR: 152.725  ISR:  -3.214  SAR:   4.110  \n",
      "\n",
      "torch.Size([1, 2, 4540496])\n",
      "torch.Size([1, 1, 4540496])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▊                                                                        | 3/23 [00:35<04:18, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -1.508  SIR: 209.640  ISR:   0.873  SAR: -15.324  \n",
      "bass            ==> SDR:  -1.316  SIR: 211.547  ISR:   0.935  SAR: -14.302  \n",
      "mean            ==> SDR:  -1.284  SIR: 209.374  ISR:   0.904  SAR: -14.677  \n",
      "\n",
      "torch.Size([1, 2, 1622389])\n",
      "torch.Size([1, 1, 1622389])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|██████████████▍                                                                    | 4/23 [00:42<03:19, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -1.412  SIR: 246.024  ISR:  -0.736  SAR:   0.469  \n",
      "bass            ==> SDR:  -1.530  SIR: 244.598  ISR:  -0.797  SAR:   0.478  \n",
      "mean            ==> SDR:  -1.469  SIR: 255.867  ISR:  -0.766  SAR:   0.504  \n",
      "\n",
      "torch.Size([1, 2, 4478906])\n",
      "torch.Size([1, 1, 4478906])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████                                                                 | 5/23 [01:00<04:01, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -4.425  SIR: 249.316  ISR:   0.331  SAR: -10.817  \n",
      "bass            ==> SDR:  -4.072  SIR: 249.187  ISR:   0.330  SAR:  -8.936  \n",
      "mean            ==> SDR:  -4.093  SIR: 247.789  ISR:   0.342  SAR:  -9.657  \n",
      "\n",
      "torch.Size([1, 2, 1532160])\n",
      "torch.Size([1, 1, 1532160])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████████████████▋                                                             | 6/23 [01:06<03:05, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR: -15.000  SIR: 246.092  ISR: -13.269  SAR:   3.526  \n",
      "bass            ==> SDR: -15.692  SIR: 249.640  ISR: -13.997  SAR:   3.487  \n",
      "mean            ==> SDR: -15.349  SIR: 244.375  ISR: -13.640  SAR:   3.516  \n",
      "\n",
      "torch.Size([1, 2, 5502796])\n",
      "torch.Size([1, 1, 5502796])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████▎                                                         | 7/23 [01:27<03:47, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -7.554  SIR: 250.168  ISR:  -5.065  SAR:   1.400  \n",
      "bass            ==> SDR:  -8.159  SIR: 242.961  ISR:  -5.631  SAR:   1.418  \n",
      "mean            ==> SDR:  -7.855  SIR: 239.727  ISR:  -5.350  SAR:   1.421  \n",
      "\n",
      "torch.Size([1, 2, 4600605])\n",
      "torch.Size([1, 1, 4600605])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|████████████████████████████▊                                                      | 8/23 [01:46<03:53, 15.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -7.641  SIR: 229.892  ISR:  -0.159  SAR: -17.238  \n",
      "bass            ==> SDR:  -4.801  SIR: 225.976  ISR:  -0.308  SAR: -12.289  \n",
      "mean            ==> SDR:  -5.828  SIR: 229.286  ISR:  -0.226  SAR: -14.344  \n",
      "\n",
      "torch.Size([1, 2, 2177937])\n",
      "torch.Size([1, 1, 2177937])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|████████████████████████████████▍                                                  | 9/23 [01:56<03:12, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:   3.561  SIR: 261.595  ISR:   4.931  SAR:   2.650  \n",
      "bass            ==> SDR:   3.771  SIR: 254.782  ISR:   5.455  SAR:   2.980  \n",
      "mean            ==> SDR:   3.696  SIR: 255.539  ISR:   5.195  SAR:   2.921  \n",
      "\n",
      "torch.Size([1, 2, 4894465])\n",
      "torch.Size([1, 1, 4894465])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████▋                                              | 10/23 [02:16<03:25, 15.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:   1.853  SIR: 235.976  ISR:   4.559  SAR:   2.927  \n",
      "bass            ==> SDR:   1.829  SIR: 242.706  ISR:   4.558  SAR:   3.155  \n",
      "mean            ==> SDR:   1.956  SIR: 252.177  ISR:   4.568  SAR:   3.196  \n",
      "\n",
      "torch.Size([1, 2, 3340015])\n",
      "torch.Size([1, 1, 3340015])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████▏                                          | 11/23 [02:28<02:55, 14.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -4.784  SIR: 257.886  ISR:  -0.391  SAR:  -1.168  \n",
      "bass            ==> SDR:  -5.452  SIR: 260.409  ISR:  -0.886  SAR:  -1.137  \n",
      "mean            ==> SDR:  -5.128  SIR: 249.539  ISR:  -0.637  SAR:  -1.151  \n",
      "\n",
      "torch.Size([1, 2, 1845768])\n",
      "torch.Size([1, 1, 1845768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████████████████████████▊                                       | 12/23 [02:35<02:15, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -9.735  SIR: 249.545  ISR:  -7.734  SAR:   0.820  \n",
      "bass            ==> SDR: -10.299  SIR: 248.816  ISR:  -8.218  SAR:   0.836  \n",
      "mean            ==> SDR: -10.018  SIR: 234.330  ISR:  -7.970  SAR:   0.834  \n",
      "\n",
      "torch.Size([1, 2, 875118])\n",
      "torch.Size([1, 1, 875118])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████████████████████████████████▎                                   | 13/23 [02:39<01:36,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -1.275  SIR: 110.349  ISR:  -0.749  SAR:   0.663  \n",
      "bass            ==> SDR:  -1.432  SIR: 110.918  ISR:  -0.851  SAR:   0.730  \n",
      "mean            ==> SDR:  -1.352  SIR: 110.505  ISR:  -0.799  SAR:   0.700  \n",
      "\n",
      "torch.Size([1, 2, 2953192])\n",
      "torch.Size([1, 1, 2953192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████████████████████████████▉                                | 14/23 [02:50<01:30, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:   0.746  SIR: 250.884  ISR:   0.861  SAR:  -0.888  \n",
      "bass            ==> SDR:   0.805  SIR: 248.561  ISR:   0.941  SAR:  -0.981  \n",
      "mean            ==> SDR:   0.782  SIR: 252.408  ISR:   0.901  SAR:  -0.856  \n",
      "\n",
      "torch.Size([1, 2, 4576639])\n",
      "torch.Size([1, 1, 4576639])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████████████████▍                            | 15/23 [03:09<01:42, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:   1.784  SIR: 245.921  ISR:   4.052  SAR:  -0.780  \n",
      "bass            ==> SDR:   1.706  SIR: 241.163  ISR:   4.342  SAR:  -0.865  \n",
      "mean            ==> SDR:   1.865  SIR: 246.812  ISR:   4.197  SAR:  -0.660  \n",
      "\n",
      "torch.Size([1, 2, 1268428])\n",
      "torch.Size([1, 1, 1268428])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████                         | 16/23 [03:15<01:16, 10.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -6.063  SIR: 246.764  ISR:  -2.785  SAR:  -1.080  \n",
      "bass            ==> SDR:  -6.658  SIR: 246.621  ISR:  -3.123  SAR:  -1.173  \n",
      "mean            ==> SDR:  -6.348  SIR: 236.594  ISR:  -2.953  SAR:  -1.092  \n",
      "\n",
      "torch.Size([1, 2, 770104])\n",
      "torch.Size([1, 1, 770104])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████████████████████████████████████▌                     | 17/23 [03:19<00:52,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -4.405  SIR: 155.838  ISR:  -3.735  SAR:   5.603  \n",
      "bass            ==> SDR:  -5.120  SIR: 159.218  ISR:  -4.421  SAR:   5.765  \n",
      "mean            ==> SDR:  -4.754  SIR: 158.714  ISR:  -4.080  SAR:   5.811  \n",
      "\n",
      "torch.Size([1, 2, 1144358])\n",
      "torch.Size([1, 1, 1144358])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████████████████████████████████████████▏                 | 18/23 [03:24<00:38,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR: -11.226  SIR: 180.633  ISR:  -7.368  SAR:  -0.298  \n",
      "bass            ==> SDR: -11.928  SIR: 183.383  ISR:  -8.010  SAR:  -0.359  \n",
      "mean            ==> SDR: -11.576  SIR: 181.177  ISR:  -7.693  SAR:  -0.320  \n",
      "\n",
      "torch.Size([1, 2, 577320])\n",
      "torch.Size([1, 1, 577320])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|███████████████████████████████████████████████████████████████████▋              | 19/23 [03:27<00:24,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -5.384  SIR: 164.601  ISR:  -1.879  SAR:   0.653  \n",
      "bass            ==> SDR:  -6.026  SIR: 164.078  ISR:  -2.479  SAR:   0.627  \n",
      "mean            ==> SDR:  -5.701  SIR: 163.442  ISR:  -2.180  SAR:   0.659  \n",
      "\n",
      "torch.Size([1, 2, 1476953])\n",
      "torch.Size([1, 1, 1476953])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████▎          | 20/23 [03:33<00:18,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:   3.232  SIR: 178.752  ISR:   5.710  SAR:   4.035  \n",
      "bass            ==> SDR:   3.377  SIR: 183.709  ISR:   6.045  SAR:   4.357  \n",
      "mean            ==> SDR:   3.376  SIR: 180.542  ISR:   5.887  SAR:   4.299  \n",
      "\n",
      "torch.Size([1, 2, 1587968])\n",
      "torch.Size([1, 1, 1587968])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|██████████████████████████████████████████████████████████████████████████▊       | 21/23 [03:40<00:12,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:  -1.590  SIR: 242.157  ISR:   0.136  SAR:   2.242  \n",
      "bass            ==> SDR:  -2.033  SIR: 256.215  ISR:  -0.117  SAR:   2.379  \n",
      "mean            ==> SDR:  -1.797  SIR: 257.720  ISR:   0.016  SAR:   2.370  \n",
      "\n",
      "torch.Size([1, 2, 3947520])\n",
      "torch.Size([1, 1, 3947520])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|██████████████████████████████████████████████████████████████████████████████▍   | 22/23 [03:55<00:09,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:   1.299  SIR: 247.855  ISR:   6.876  SAR:   0.293  \n",
      "bass            ==> SDR:   1.561  SIR: 248.445  ISR:   7.241  SAR:   0.666  \n",
      "mean            ==> SDR:   1.526  SIR: 250.844  ISR:   7.047  SAR:   0.468  \n",
      "\n",
      "torch.Size([1, 2, 770649])\n",
      "torch.Size([1, 1, 770649])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 23/23 [04:01<00:00, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drums           ==> SDR:   2.470  SIR: 152.249  ISR:   2.465  SAR:   4.599  \n",
      "bass            ==> SDR:   2.706  SIR: 152.726  ISR:   2.727  SAR:   4.603  \n",
      "mean            ==> SDR:   2.596  SIR: 152.184  ISR:   2.595  SAR:   4.697  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'epoch_200'\n",
    "#try:\n",
    "out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_conv_{name}/\"\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = DrumConvTasnet.load_from_checkpoint(f'D:/Github/phd-drum-sep/analysis/conv_tasnet_model_analysis/checkpoint/{name}.ckpt')\n",
    "# model.to('cpu')\n",
    "model = model.eval()\n",
    "\n",
    "results = museval.EvalStore(frames_agg='median', tracks_agg='median')\n",
    "for track in tqdm(all_tracks):\n",
    "\n",
    "    mixture_tensor = load_audio(track.mix_path).unsqueeze(0).to(model.device)\n",
    "    print(mixture_tensor.shape)\n",
    "    drum_tensor = load_audio(track.drum_path).unsqueeze(0).to(model.device)\n",
    "    print(drum_tensor.shape)\n",
    "    \n",
    "    shape = mixture_tensor.shape[2]\n",
    "    roll_tensor = load_roll(track.midi_path, shape).unsqueeze(0).to(model.device)\n",
    "\n",
    "    seperated = separate_sources(model, mixture_tensor, roll_tensor, device='cuda')\n",
    "\n",
    "    output_path = out_dir\n",
    "    \n",
    "    audio = seperated[:,0,:].cpu().numpy()\n",
    "    audio = np.swapaxes(audio,0,1)\n",
    "\n",
    "    audio2 = seperated[:,1,:].cpu().numpy()\n",
    "    audio2 = np.swapaxes(audio2,0,1)\n",
    "\n",
    "    audio3 = (seperated[:,1,:] + seperated[:,0,:]) / 2\n",
    "    audio3 = audio3.cpu().numpy()\n",
    "    audio3 = np.swapaxes(audio3, 0, 1)\n",
    "\n",
    "    estimates = {'drums': audio, 'bass': audio2, 'mean': audio3}\n",
    "\n",
    "    d = drum_tensor.squeeze(0).cpu().numpy()\n",
    "    d = np.swapaxes(d,0,1)\n",
    "    \n",
    "    track.targets['drums'] = AudioData(d)\n",
    "    track.targets['bass'] = AudioData(d)\n",
    "    track.targets['mean'] = AudioData(d)\n",
    "    \n",
    "    scores = museval.eval_mus_track(\n",
    "        track, estimates, output_dir=f\"{output_path}\"\n",
    "    )\n",
    "\n",
    "    print(scores)\n",
    "    results.add_track(scores)\n",
    "\n",
    "results.df.to_csv(f\"{output_path}results.csv\")\n",
    "\n",
    "del model\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cd0a0c5-a825-44cb-94e7-9fef9dcc91fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(target  metric\n",
       " bass    ISR        -0.763295\n",
       "         SAR         0.665770\n",
       "         SDR        -2.033110\n",
       "         SIR       242.706060\n",
       " drums   ISR        -0.391020\n",
       "         SAR         0.653390\n",
       "         SDR        -1.590200\n",
       "         SIR       242.157360\n",
       " mean    ISR        -0.636810\n",
       "         SAR         0.658920\n",
       "         SDR        -1.797345\n",
       "         SIR       236.594285\n",
       " Name: score, dtype: float64,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.agg_frames_tracks_scores(), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "246b6f4f-d272-4105-aef9-26757a4c3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results.agg_frames_scores().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d600d210-0015-462f-8dc5-4c5888589fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.797345\n"
     ]
    }
   ],
   "source": [
    "sdrs = []\n",
    "for key in list(r.keys()):\n",
    "    if 'SDR' in key:\n",
    "        sdrs.append(r[key])\n",
    "\n",
    "print(np.median(sdrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db3f19-21c2-4daf-bdf1-eb25814ec5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
