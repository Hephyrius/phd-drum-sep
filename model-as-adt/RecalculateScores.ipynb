{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589ddb8b-3c37-4589-9345-beb7517dfc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "import plotly.graph_objects as go\n",
    "from torch.optim import lr_scheduler\n",
    "from IPython.display import Audio\n",
    "from torchaudio.transforms import Fade\n",
    "import musdb\n",
    "import museval\n",
    "import gc\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, jaccard_score, accuracy_score,zero_one_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7aa787c-e510-415b-8a0c-5ee821ed5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, name, midi_path, drum_path, mix_path):\n",
    "        self.name = name\n",
    "        self.midi_path = midi_path\n",
    "        self.drum_path = drum_path\n",
    "        self.mix_path = mix_path\n",
    "        self.targets = {'drums': '', 'bass': ''}\n",
    "        self.rate = 44100\n",
    "        self.subset = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea86f521-164c-42cf-93f2-5cb214b1e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioData:\n",
    "    def __init__(self, audio):\n",
    "        self.audio = audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e8e37bc-52f9-422f-939b-81b771224a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 3407\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8557dee-c012-44ba-bd41-52ff04c55d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/full_mix/'\n",
    "mixes = os.listdir(mix_folder)\n",
    "mixes = [mix_folder + m for m in mixes]\n",
    "\n",
    "drum_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/drum_only/'\n",
    "drum = os.listdir(drum_folder)\n",
    "drum = [drum_folder + d for d in drum]\n",
    "\n",
    "beats_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/beats/'\n",
    "beats = os.listdir(beats_folder)\n",
    "beats = [beats_folder + b for b in beats]#\n",
    "\n",
    "class_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/'\n",
    "classes = os.listdir(class_folder)\n",
    "classes = [class_folder + c for c in classes]\n",
    "\n",
    "midi_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/midi/'\n",
    "midis = os.listdir(midi_folder)\n",
    "midis = [midi_folder + m for m in midis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e2b358f-3d0d-4293-ac58-a506fe7dcdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tracks = []\n",
    "for idx, val in tqdm(enumerate(classes)):\n",
    "\n",
    "    name = val.replace('D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/', '')\n",
    "    name = name.replace('_subclass.txt', '')\n",
    "\n",
    "    t = Track(name, midis[idx], drum[idx], mixes[idx])\n",
    "    all_tracks.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a5275c-158f-4d24-bf45-313ab7604f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_transcription_into_roll(transcription, frames):\n",
    "    # Determine your sampling frequency (frames per second)\n",
    "    fs = 44100\n",
    "    \n",
    "    piano_roll_length = int(frames)\n",
    "    \n",
    "    # Initialize the piano roll array\n",
    "    piano_roll = np.zeros((64, piano_roll_length))\n",
    "    \n",
    "    # Fill in the piano roll array\n",
    "    for note in transcription.instruments[0].notes:\n",
    "        # Convert start and end times to frame indices\n",
    "        start_frame = int(np.floor(note.start * fs))\n",
    "        end_frame = int(np.ceil(note.end * fs))\n",
    "        \n",
    "        # Set the corresponding frames to 1 (or note.velocity for a velocity-sensitive representation)\n",
    "        piano_roll[note.pitch, start_frame:end_frame] = 1  # Or use note.velocity\n",
    "        \n",
    "    roll = np.vstack([piano_roll[35:36, :], piano_roll[38:39, :], piano_roll[42:43, :], piano_roll[47:48, :], piano_roll[49:50, :]])\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df84404c-7536-4f0e-87c5-7c84197b544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(x, out_size=44100*4, step=4410):\n",
    "    output_tensor = torch.zeros((5, out_size))\n",
    "    for i in range(x.shape[1]):  # Iterate over the second dimension\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        output_tensor[:, start_idx:end_idx] = x[:, i].unsqueeze(1)\n",
    "    return output_tensor\n",
    "\n",
    "def compress(x, original_shape=(5, 40), step=4410):\n",
    "    \"\"\"\n",
    "    Compresses a tensor from a larger size to its original smaller size by averaging blocks of values.\n",
    "    \n",
    "    Args:\n",
    "    - x (Tensor): The input tensor to be compressed, expected to have the shape (5, 44100) or similar.\n",
    "    - original_shape (tuple): The shape of the output tensor, default is (5, 40).\n",
    "    - step (int): The size of the block to average over, default is 4410.\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor: The compressed tensor with shape specified by `original_shape`.\n",
    "    \"\"\"\n",
    "    output_tensor = torch.zeros(original_shape)\n",
    "    for i in range(original_shape[1]):  # Iterate over the second dimension of the target shape\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        # Take the mean of each block and assign it to the corresponding position in the output tensor\n",
    "        output_tensor[:, i] = x[:, start_idx:end_idx].mean(dim=1)\n",
    "    return output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d08a9e-a763-4ea1-b5f7-80e908f1805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    audio_tensors = []\n",
    "    waveform, _ = torchaudio.load(path)\n",
    "    return waveform\n",
    "\n",
    "def load_roll(path, frames):\n",
    "    transcription = pretty_midi.PrettyMIDI(path)\n",
    "    roll = turn_transcription_into_roll(transcription, frames)\n",
    "\n",
    "    return torch.from_numpy(roll).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ee60c0-785e-4829-833c-d2c7fc1eaab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [08:43<00:00, 22.78s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [08:25<00:00, 21.98s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [08:21<00:00, 21.80s/it]\n"
     ]
    }
   ],
   "source": [
    "models = ['ht_epoch_280', 'trans_epoch_280', 'conv_epoch_200']\n",
    "\n",
    "for m in models:\n",
    "    out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_{m}/adt/\"\n",
    "    rows = []\n",
    "    for track in tqdm(all_tracks):\n",
    "    \n",
    "        mixture_tensor = load_audio(track.mix_path)\n",
    "        shape = mixture_tensor.shape[1]\n",
    "        \n",
    "        snippet_length = (mixture_tensor.shape[1] // (44100 * 4)) * (44100 * 4)\n",
    "        mixture_tensor = mixture_tensor[:, :snippet_length]\n",
    "    \n",
    "        roll_tensor = load_roll(track.midi_path, shape)\n",
    "        roll_tensor = roll_tensor[:, :snippet_length]\n",
    "    \n",
    "        proposed_answers = []\n",
    "    \n",
    "        track_dir = f'{out_dir}{track.name}'\n",
    "        track_folder = os.listdir(track_dir)\n",
    "        segments = len(track_folder) - 2\n",
    "    \n",
    "        chunk_len = int(44100 * 4)\n",
    "    \n",
    "        for i in range(segments):\n",
    "            proposed_transcription = pd.read_csv(f'{track_dir}/{i}.csv')\n",
    "            proposed_transcription = np.asarray(proposed_transcription)\n",
    "            proposed_transcription = np.asarray(proposed_transcription.T)[1:].T\n",
    "            proposed_transcription = expand(torch.from_numpy(proposed_transcription))\n",
    "    \n",
    "            start = i * chunk_len\n",
    "            end = start + chunk_len\n",
    "    \n",
    "            drum_chunk_ = roll_tensor[:, start:end].numpy()\n",
    "            proposed_transcription_ = proposed_transcription.numpy()\n",
    "    \n",
    "            for drum in range(5):\n",
    "                drum_chunk = drum_chunk_[drum, :]\n",
    "                proposed_transcription = proposed_transcription_[drum, :]\n",
    "                recall = recall_score(drum_chunk, proposed_transcription, average='weighted', zero_division=0)\n",
    "                precision = precision_score(drum_chunk, proposed_transcription, average='weighted', zero_division=0)\n",
    "                f1 = f1_score(drum_chunk, proposed_transcription, average='weighted')\n",
    "                jaccard = jaccard_score(drum_chunk, proposed_transcription, average='weighted')\n",
    "                acc = accuracy_score(drum_chunk, proposed_transcription)\n",
    "                rows.append([track.name, i, drum, recall, precision, f1, jaccard, acc])\n",
    "\n",
    "            drum_chunk = drum_chunk_.flatten()\n",
    "            proposed_transcription = proposed_transcription_.flatten()\n",
    "            recall = recall_score(drum_chunk, proposed_transcription, average='weighted', zero_division=0)\n",
    "            precision = precision_score(drum_chunk, proposed_transcription, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(drum_chunk, proposed_transcription, average='weighted')\n",
    "            jaccard = jaccard_score(drum_chunk, proposed_transcription, average='weighted')\n",
    "            acc = accuracy_score(drum_chunk, proposed_transcription)\n",
    "            rows.append([track.name, i, 'all', recall, precision, f1, jaccard, acc])\n",
    "                \n",
    "    df_results = pd.DataFrame(rows, columns=['track_name', 'slice', 'drum', 'recall', 'precision', 'f1', 'jaccard', 'accuracy'])\n",
    "    df_results.to_csv(f\"D:/Github/phd-drum-sep/model-as-adt/results_final/{m}.csv\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6458e-7f16-40fd-b92d-e1ac750fd088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
