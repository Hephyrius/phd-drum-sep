{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b2a3bb-3d01-4f42-bc37-dff46522af7f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d686e6-8847-4861-88f0-58141c363862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "import plotly.graph_objects as go\n",
    "from torch.optim import lr_scheduler\n",
    "from IPython.display import Audio\n",
    "from torchaudio.transforms import Fade\n",
    "import musdb\n",
    "import museval\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091eea44-52d4-44e1-ab82-863b9954851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, name, midi_path, drum_path, mix_path):\n",
    "        self.name = name\n",
    "        self.midi_path = midi_path\n",
    "        self.drum_path = drum_path\n",
    "        self.mix_path = mix_path\n",
    "        self.targets = {'drums': '', 'bass': ''}\n",
    "        self.rate = 44100\n",
    "        self.subset = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c82506-04c8-4200-9e63-26857aa176bd",
   "metadata": {},
   "source": [
    "# Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3acd3b-d177-44d3-a3f4-35140f45e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 3407\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf49cdd4-f6c9-4df4-adc8-f4e98f24e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/full_mix/'\n",
    "mixes = os.listdir(mix_folder)\n",
    "mixes = [mix_folder + m for m in mixes]\n",
    "\n",
    "drum_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/drum_only/'\n",
    "drum = os.listdir(drum_folder)\n",
    "drum = [drum_folder + d for d in drum]\n",
    "\n",
    "beats_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/beats/'\n",
    "beats = os.listdir(beats_folder)\n",
    "beats = [beats_folder + b for b in beats]#\n",
    "\n",
    "class_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/'\n",
    "classes = os.listdir(class_folder)\n",
    "classes = [class_folder + c for c in classes]\n",
    "\n",
    "midi_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/midi/'\n",
    "midis = os.listdir(midi_folder)\n",
    "midis = [midi_folder + m for m in midis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac376c2d-6c64-4901-a1f6-687c59d7a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tracks = []\n",
    "for idx, val in tqdm(enumerate(classes)):\n",
    "\n",
    "    name = val.replace('D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/', '')\n",
    "    name = name.replace('_subclass.txt', '')\n",
    "\n",
    "    t = Track(name, midis[idx], drum[idx], mixes[idx])\n",
    "    all_tracks.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182916a-aec1-4c05-8a08-4d2e44dfd979",
   "metadata": {},
   "source": [
    "# Construct Teh Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d4eb9a-f08d-4452-a4ad-b9769a218278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_transcription_into_roll(transcription, frames):\n",
    "    # Determine your sampling frequency (frames per second)\n",
    "    fs = 44100\n",
    "    \n",
    "    piano_roll_length = int(frames)\n",
    "    \n",
    "    # Initialize the piano roll array\n",
    "    piano_roll = np.zeros((64, piano_roll_length))\n",
    "    \n",
    "    # Fill in the piano roll array\n",
    "    for note in transcription.instruments[0].notes:\n",
    "        # Convert start and end times to frame indices\n",
    "        start_frame = int(np.floor(note.start * fs))\n",
    "        end_frame = int(np.ceil(note.end * fs))\n",
    "        \n",
    "        # Set the corresponding frames to 1 (or note.velocity for a velocity-sensitive representation)\n",
    "        piano_roll[note.pitch, start_frame:end_frame] = 1  # Or use note.velocity\n",
    "        \n",
    "    roll = np.vstack([piano_roll[35:36, :], piano_roll[38:39, :], piano_roll[42:43, :], piano_roll[47:48, :], piano_roll[49:50, :]])\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988d30-a95e-4dca-ac19-a6f7cea95717",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72a57b5-7115-421d-b26e-3ca3d21fef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumDemucs(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(DrumDemucs, self).__init__()\n",
    "\n",
    "        self.loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
    "                    fft_sizes=[1024, 2048, 4096],\n",
    "                    hop_sizes=[256, 512, 1024],\n",
    "                    win_lengths=[1024, 2048, 4096],\n",
    "                    scale=\"mel\", \n",
    "                    n_bins=150,\n",
    "                    sample_rate=44100,\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "\n",
    "        self.loss_fn_2 = auraloss.time.SISDRLoss()\n",
    "\n",
    "        self.loss_fn_3 = torch.nn.L1Loss()\n",
    "\n",
    "        self.loss_used = 0\n",
    "\n",
    "        sources = ['drum',\n",
    "                   'noise',\n",
    "                   ]\n",
    "        \n",
    "        self.demucs_mixer =  torchaudio.models.HDemucs(\n",
    "            sources=sources,\n",
    "            audio_channels=7,\n",
    "            depth=6,\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Conv1d(in_channels=7, out_channels=2, kernel_size=1)\n",
    "        self.out = nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1)      \n",
    "\n",
    "\n",
    "    def compute_loss(self, outputs, ref_signals):\n",
    "        loss = self.loss_fn(outputs, ref_signals) + self.loss_fn_2(outputs, ref_signals) +  self.loss_fn_3(outputs, ref_signals)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, audio, drumroll):\n",
    "        to_mix = torch.cat([audio, drumroll], axis=1)\n",
    "        out = self.demucs_mixer(to_mix)\n",
    "        out_2 = self.out_conv(out[:, 0, :, :])\n",
    "        out_2 = self.out(out_2)\n",
    "        # out_2 = torch.tanh(out_2)\n",
    "\n",
    "        return out_2\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        audio, drum, drumroll = batch\n",
    "        \n",
    "        outputs = self.forward(audio, drumroll)\n",
    "        # print(outputs.size())\n",
    "\n",
    "        if batch_idx % 64 == 0:\n",
    "            input_signal = audio[0].cpu().detach().numpy().T\n",
    "            generated_signal = outputs[0].cpu().detach().numpy().T\n",
    "            drum_signal = drum[0].cpu().detach().numpy().T \n",
    "            wandb.log({'audio_input': [wandb.Audio(input_signal, caption=\"Input\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_reference': [wandb.Audio(drum_signal, caption=\"Reference\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_output': [wandb.Audio(generated_signal, caption=\"Output\", sample_rate=44100)]})\n",
    "             \n",
    "            for i in range(5):\n",
    "                wandb.log({f'drum_{i + 1}': [wandb.Audio(drumroll[0].cpu().detach().numpy()[i, :], caption=\"Output\", sample_rate=44100)]})\n",
    "\n",
    "\n",
    "        loss = self.compute_loss(outputs, drum)         \n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer and optionally learning rate scheduler here\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d11190-4e68-41f8-ae19-36d8199c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    audio_tensors = []\n",
    "    waveform, _ = torchaudio.load(path)\n",
    "    return waveform\n",
    "\n",
    "def load_roll(path, frames):\n",
    "    transcription = pretty_midi.PrettyMIDI(path)\n",
    "    roll = turn_transcription_into_roll(transcription, frames)\n",
    "\n",
    "    return torch.from_numpy(roll).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a00457-307d-4743-9784-9f4cdae73730",
   "metadata": {},
   "source": [
    "# SISNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc36a6ea-fdf1-49f4-a1b1-d5971933eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(x, out_size=44100*4, step=4410):\n",
    "    output_tensor = torch.zeros((5, out_size))\n",
    "    for i in range(x.shape[1]):  # Iterate over the second dimension\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        output_tensor[:, start_idx:end_idx] = x[:, i].unsqueeze(1)\n",
    "    return output_tensor\n",
    "\n",
    "def compress(x, original_shape=(5, 40), step=4410):\n",
    "    \"\"\"\n",
    "    Compresses a tensor from a larger size to its original smaller size by averaging blocks of values.\n",
    "    \n",
    "    Args:\n",
    "    - x (Tensor): The input tensor to be compressed, expected to have the shape (5, 44100) or similar.\n",
    "    - original_shape (tuple): The shape of the output tensor, default is (5, 40).\n",
    "    - step (int): The size of the block to average over, default is 4410.\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor: The compressed tensor with shape specified by `original_shape`.\n",
    "    \"\"\"\n",
    "    output_tensor = torch.zeros(original_shape)\n",
    "    for i in range(original_shape[1]):  # Iterate over the second dimension of the target shape\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        # Take the mean of each block and assign it to the corresponding position in the output tensor\n",
    "        output_tensor[:, i] = x[:, start_idx:end_idx].mean(dim=1)\n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b97a63a5-d58e-47db-9768-e063efd5da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioData:\n",
    "    def __init__(self, audio):\n",
    "        self.audio = audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaf6345c-e063-4fe5-9bb3-15b218c080e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(mixture_tensor_, drum_tensor_):\n",
    "    with torch.no_grad():\n",
    "        shape = (5,40)\n",
    "        solution = torch.zeros(shape)\n",
    "        proposed = expand(solution).unsqueeze(0)\n",
    "        \n",
    "        sep = model(mixture_tensor_.to(model.device), proposed.to(model.device))\n",
    "        loss = model.compute_loss(sep, drum_tensor_).item()\n",
    "\n",
    "        for i in range(40):\n",
    "            for c in range(5):\n",
    "                candidate = solution.detach().clone()\n",
    "                candidate[c, i] = 1\n",
    "\n",
    "                candidate_expand = expand(candidate).unsqueeze(0) \n",
    "\n",
    "                sep = model(mixture_tensor_.to(model.device), candidate_expand.to(model.device))\n",
    "                new_loss = model.compute_loss(sep.to(model.device), drum_tensor_.to(model.device)).item()\n",
    "\n",
    "                if new_loss < loss:\n",
    "                    loss = new_loss\n",
    "                    solution = candidate\n",
    "\n",
    "        return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b2165f-3a25-4ffb-90f2-d8c1ad6879ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_torch(transcription, prediction):\n",
    "    TPs = torch.sum((transcription == 1) & (prediction == 1), dim=1)\n",
    "    FPs = torch.sum((transcription == 0) & (prediction == 1), dim=1)\n",
    "    FNs = torch.sum((transcription == 1) & (prediction == 0), dim=1)\n",
    "\n",
    "    precision = TPs.float() / (TPs + FPs).float()\n",
    "    recall = TPs.float() / (TPs + FNs).float()\n",
    "\n",
    "    # Handle potential division by zero for precision and recall\n",
    "    precision[torch.isnan(precision)] = 0\n",
    "    recall[torch.isnan(recall)] = 0\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f9974d7-61e7-483f-83d0-733710827e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f_measure(precision, recall, beta=1):\n",
    "    \"\"\"\n",
    "    Calculate the F-measure for each class and the average F-measure.\n",
    "\n",
    "    Parameters:\n",
    "    - precision: Tensor of precision values per class.\n",
    "    - recall: Tensor of recall values per class.\n",
    "    - beta: Weight of recall in the harmonic mean.\n",
    "\n",
    "    Returns:\n",
    "    - f_measure: Tensor of F-measure for each class.\n",
    "    - average_f_measure: Scalar, average F-measure across all classes.\n",
    "    \"\"\"\n",
    "    numerator = (1 + beta**2) * precision * recall\n",
    "    denominator = (beta**2 * precision) + recall\n",
    "\n",
    "    # Avoid division by zero\n",
    "    denominator[denominator == 0] = 1\n",
    "\n",
    "    f_measure = numerator / denominator\n",
    "\n",
    "    # Handle potential NaN values\n",
    "    f_measure[torch.isnan(f_measure)] = 0\n",
    "\n",
    "    average_f_measure = torch.mean(f_measure)\n",
    "\n",
    "    return f_measure, average_f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ada2d-30b5-4c4f-b03f-3b3c069aa549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                | 0/23 [00:00<?, ?it/s]\n",
      "  0%|                                                                                                                                                                                                                 | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|██████████████████████▎                                                                                                                                                                                  | 1/9 [00:11<01:29, 11.19s/it]\u001b[A\n",
      " 22%|████████████████████████████████████████████▋                                                                                                                                                            | 2/9 [00:21<01:14, 10.59s/it]\u001b[A\n",
      " 33%|███████████████████████████████████████████████████████████████████                                                                                                                                      | 3/9 [00:31<01:02, 10.41s/it]\u001b[A\n",
      " 44%|█████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                               | 4/9 [00:41<00:51, 10.34s/it]\u001b[A\n",
      " 56%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                         | 5/9 [00:52<00:41, 10.36s/it]\u001b[A\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 6/9 [01:02<00:31, 10.38s/it]\u001b[A\n",
      " 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                            | 7/9 [01:13<00:20, 10.44s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 8/9 [01:23<00:10, 10.43s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:33<00:00, 10.43s/it]\u001b[A\n",
      "  4%|████████▋                                                                                                                                                                                               | 1/23 [01:33<34:27, 93.97s/it]\n",
      "  0%|                                                                                                                                                                                                                 | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|██████████████████████▎                                                                                                                                                                                  | 1/9 [00:10<01:24, 10.57s/it]\u001b[A\n",
      " 22%|████████████████████████████████████████████▋                                                                                                                                                            | 2/9 [00:20<01:13, 10.45s/it]\u001b[A\n",
      " 33%|███████████████████████████████████████████████████████████████████                                                                                                                                      | 3/9 [00:31<01:02, 10.37s/it]\u001b[A\n",
      " 44%|█████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                               | 4/9 [00:41<00:51, 10.35s/it]\u001b[A\n",
      " 56%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                         | 5/9 [00:51<00:41, 10.34s/it]\u001b[A\n",
      " 67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 6/9 [01:02<00:31, 10.36s/it]\u001b[A\n",
      " 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                            | 7/9 [01:12<00:20, 10.36s/it]\u001b[A\n",
      " 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 8/9 [01:22<00:10, 10.35s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [01:33<00:00, 10.35s/it]\u001b[A\n",
      "  9%|█████████████████▍                                                                                                                                                                                      | 2/23 [03:07<32:44, 93.55s/it]\n",
      "  0%|                                                                                                                                                                                                                | 0/25 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "name = 'epoch_285'\n",
    "#try:\n",
    "\n",
    "out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_ht_{name}/\"\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_ht_{name}/adt/\"\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = DrumDemucs.load_from_checkpoint(f'D:/Github/phd-drum-sep/analysis/demucs_small_model_analysis/checkpoint/{name}.ckpt')\n",
    "model = model.eval()\n",
    "\n",
    "results = museval.EvalStore(frames_agg='median', tracks_agg='median')\n",
    "for track in tqdm(all_tracks):\n",
    "\n",
    "    mixture_tensor = load_audio(track.mix_path).unsqueeze(0).to(model.device)\n",
    "    snippet_length = (mixture_tensor.shape[2] // (44100 * 4)) * (44100 * 4)\n",
    "    mixture_tensor = mixture_tensor[:,:, :snippet_length]\n",
    "\n",
    "    drum_tensor = load_audio(track.drum_path).unsqueeze(0)\n",
    "    drum_tensor = torch.cat([drum_tensor, drum_tensor], dim=1).to(model.device)\n",
    "    drum_tensor = drum_tensor[:,:, :snippet_length]\n",
    "\n",
    "    shape = mixture_tensor.shape[2]\n",
    "    roll_tensor = load_roll(track.midi_path, shape).unsqueeze(0).to(model.device)\n",
    "    roll_tensor = roll_tensor[:,:, :snippet_length]\n",
    "\n",
    "    proposed_answers = []\n",
    "    \n",
    "    device = mixture_tensor.device\n",
    "    batch, channels, length = mixture_tensor.shape\n",
    "    chunk_len = int(44100 * 4)\n",
    "\n",
    "    for start in tqdm(range(0, length, chunk_len)):\n",
    "        end = start + chunk_len\n",
    "        answer = find_best(mixture_tensor[:,:,start:end], drum_tensor[:,:, start:end])\n",
    "        proposed_answers.append(answer)\n",
    "\n",
    "    expanded = [expand(p).unsqueeze(0) for p in proposed_answers]\n",
    "    # expanded = torch.cat(expanded, dim=2)\n",
    "    pres = [[],[],[],[],[]]\n",
    "    recs = [[],[],[],[],[]]\n",
    "    for idx, val in enumerate(expanded):\n",
    "        segment = 44100 * 4\n",
    "        start = idx * segment\n",
    "        end = start + segment\n",
    "        slice = roll_tensor[:, :, start:end].to(model.device).squeeze(0)\n",
    "        pred = val.to(model.device).squeeze(0)\n",
    "        pre, rec = calculate_precision_recall_torch(slice, pred)\n",
    "    \n",
    "        for drum in range(5):\n",
    "            pres[drum].append(pre[drum].unsqueeze(0))\n",
    "            recs[drum].append(rec[drum].unsqueeze(0))\n",
    "            \n",
    "    for p in range(len(pres)):\n",
    "        for q in range(len(pres[p])):\n",
    "            try:\n",
    "                pres[p][q] = pres[p][q].item()\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    for p in range(len(recs)):\n",
    "        for q in range(len(recs[p])):\n",
    "            try:\n",
    "                recs[p][q] = recs[p][q].item()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(f'{out_dir}{track.name}')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(pres)\n",
    "    df = df.T\n",
    "    df.to_csv(f'{out_dir}{track.name}/precision.csv')\n",
    "\n",
    "    df = pd.DataFrame(recs)\n",
    "    df = df.T\n",
    "    df.to_csv(f'{out_dir}{track.name}/recall.csv')\n",
    "\n",
    "    for idx, val in enumerate(proposed_answers):\n",
    "        adf = pd.DataFrame(val.numpy())\n",
    "        adf.to_csv(f'{out_dir}{track.name}/{idx}.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
