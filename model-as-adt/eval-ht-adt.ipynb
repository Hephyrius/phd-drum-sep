{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20b2a3bb-3d01-4f42-bc37-dff46522af7f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d686e6-8847-4861-88f0-58141c363862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import auraloss\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB\n",
    "print(torch.cuda.is_available())\n",
    "import plotly.graph_objects as go\n",
    "from torch.optim import lr_scheduler\n",
    "from IPython.display import Audio\n",
    "from torchaudio.transforms import Fade\n",
    "import musdb\n",
    "import museval\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091eea44-52d4-44e1-ab82-863b9954851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Track:\n",
    "    def __init__(self, name, midi_path, drum_path, mix_path):\n",
    "        self.name = name\n",
    "        self.midi_path = midi_path\n",
    "        self.drum_path = drum_path\n",
    "        self.mix_path = mix_path\n",
    "        self.targets = {'drums': '', 'bass': ''}\n",
    "        self.rate = 44100\n",
    "        self.subset = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c82506-04c8-4200-9e63-26857aa176bd",
   "metadata": {},
   "source": [
    "# Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3acd3b-d177-44d3-a3f4-35140f45e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 3407\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf49cdd4-f6c9-4df4-adc8-f4e98f24e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/full_mix/'\n",
    "mixes = os.listdir(mix_folder)\n",
    "mixes = [mix_folder + m for m in mixes]\n",
    "\n",
    "drum_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/audio/drum_only/'\n",
    "drum = os.listdir(drum_folder)\n",
    "drum = [drum_folder + d for d in drum]\n",
    "\n",
    "beats_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/beats/'\n",
    "beats = os.listdir(beats_folder)\n",
    "beats = [beats_folder + b for b in beats]#\n",
    "\n",
    "class_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/'\n",
    "classes = os.listdir(class_folder)\n",
    "classes = [class_folder + c for c in classes]\n",
    "\n",
    "midi_folder = 'D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/midi/'\n",
    "midis = os.listdir(midi_folder)\n",
    "midis = [midi_folder + m for m in midis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac376c2d-6c64-4901-a1f6-687c59d7a5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tracks = []\n",
    "for idx, val in tqdm(enumerate(classes)):\n",
    "\n",
    "    name = val.replace('D:/Github/phd-drum-sep/data/MDBDrums-master/MDB Drums/annotations/subclass/', '')\n",
    "    name = name.replace('_subclass.txt', '')\n",
    "\n",
    "    t = Track(name, midis[idx], drum[idx], mixes[idx])\n",
    "    all_tracks.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6182916a-aec1-4c05-8a08-4d2e44dfd979",
   "metadata": {},
   "source": [
    "# Construct Teh Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d4eb9a-f08d-4452-a4ad-b9769a218278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_transcription_into_roll(transcription, frames):\n",
    "    # Determine your sampling frequency (frames per second)\n",
    "    fs = 44100\n",
    "    \n",
    "    piano_roll_length = int(frames)\n",
    "    \n",
    "    # Initialize the piano roll array\n",
    "    piano_roll = np.zeros((64, piano_roll_length))\n",
    "    \n",
    "    # Fill in the piano roll array\n",
    "    for note in transcription.instruments[0].notes:\n",
    "        # Convert start and end times to frame indices\n",
    "        start_frame = int(np.floor(note.start * fs))\n",
    "        end_frame = int(np.ceil(note.end * fs))\n",
    "        \n",
    "        # Set the corresponding frames to 1 (or note.velocity for a velocity-sensitive representation)\n",
    "        piano_roll[note.pitch, start_frame:end_frame] = 1  # Or use note.velocity\n",
    "        \n",
    "    roll = np.vstack([piano_roll[35:36, :], piano_roll[38:39, :], piano_roll[42:43, :], piano_roll[47:48, :], piano_roll[49:50, :]])\n",
    "    return roll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20988d30-a95e-4dca-ac19-a6f7cea95717",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72a57b5-7115-421d-b26e-3ca3d21fef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrumDemucs(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(DrumDemucs, self).__init__()\n",
    "\n",
    "        self.loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
    "                    fft_sizes=[1024, 2048, 4096],\n",
    "                    hop_sizes=[256, 512, 1024],\n",
    "                    win_lengths=[1024, 2048, 4096],\n",
    "                    scale=\"mel\", \n",
    "                    n_bins=150,\n",
    "                    sample_rate=44100,\n",
    "                    device=\"cuda\"\n",
    "                )\n",
    "\n",
    "        self.loss_fn_2 = auraloss.time.SISDRLoss()\n",
    "\n",
    "        self.loss_fn_3 = torch.nn.L1Loss()\n",
    "\n",
    "        self.loss_used = 0\n",
    "\n",
    "        sources = ['drum',\n",
    "                   'noise',\n",
    "                   ]\n",
    "        \n",
    "        self.demucs_mixer =  torchaudio.models.HDemucs(\n",
    "            sources=sources,\n",
    "            audio_channels=7,\n",
    "            depth=6,\n",
    "        )\n",
    "\n",
    "        self.out_conv = nn.Conv1d(in_channels=7, out_channels=2, kernel_size=1)\n",
    "        self.out = nn.Conv1d(in_channels=2, out_channels=2, kernel_size=1)      \n",
    "\n",
    "\n",
    "    def compute_loss(self, outputs, ref_signals):\n",
    "        loss = self.loss_fn(outputs, ref_signals) + self.loss_fn_2(outputs, ref_signals) +  self.loss_fn_3(outputs, ref_signals)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, audio, drumroll):\n",
    "        to_mix = torch.cat([audio, drumroll], axis=1)\n",
    "        out = self.demucs_mixer(to_mix)\n",
    "        out_2 = self.out_conv(out[:, 0, :, :])\n",
    "        out_2 = self.out(out_2)\n",
    "        # out_2 = torch.tanh(out_2)\n",
    "\n",
    "        return out_2\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        audio, drum, drumroll = batch\n",
    "        \n",
    "        outputs = self.forward(audio, drumroll)\n",
    "        # print(outputs.size())\n",
    "\n",
    "        if batch_idx % 64 == 0:\n",
    "            input_signal = audio[0].cpu().detach().numpy().T\n",
    "            generated_signal = outputs[0].cpu().detach().numpy().T\n",
    "            drum_signal = drum[0].cpu().detach().numpy().T \n",
    "            wandb.log({'audio_input': [wandb.Audio(input_signal, caption=\"Input\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_reference': [wandb.Audio(drum_signal, caption=\"Reference\", sample_rate=44100)]})\n",
    "            wandb.log({'audio_output': [wandb.Audio(generated_signal, caption=\"Output\", sample_rate=44100)]})\n",
    "             \n",
    "            for i in range(5):\n",
    "                wandb.log({f'drum_{i + 1}': [wandb.Audio(drumroll[0].cpu().detach().numpy()[i, :], caption=\"Output\", sample_rate=44100)]})\n",
    "\n",
    "\n",
    "        loss = self.compute_loss(outputs, drum)         \n",
    "\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define your optimizer and optionally learning rate scheduler here\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
    "        return [optimizer], [scheduler]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79d11190-4e68-41f8-ae19-36d8199c1643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(path):\n",
    "    audio_tensors = []\n",
    "    waveform, _ = torchaudio.load(path)\n",
    "    return waveform\n",
    "\n",
    "def load_roll(path, frames):\n",
    "    transcription = pretty_midi.PrettyMIDI(path)\n",
    "    roll = turn_transcription_into_roll(transcription, frames)\n",
    "\n",
    "    return torch.from_numpy(roll).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a00457-307d-4743-9784-9f4cdae73730",
   "metadata": {},
   "source": [
    "# SISNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc36a6ea-fdf1-49f4-a1b1-d5971933eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(x, out_size=44100*4, step=4410):\n",
    "    output_tensor = torch.zeros((5, out_size))\n",
    "    for i in range(x.shape[1]):  # Iterate over the second dimension\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        output_tensor[:, start_idx:end_idx] = x[:, i].unsqueeze(1)\n",
    "    return output_tensor\n",
    "\n",
    "def compress(x, original_shape=(5, 40), step=4410):\n",
    "    \"\"\"\n",
    "    Compresses a tensor from a larger size to its original smaller size by averaging blocks of values.\n",
    "    \n",
    "    Args:\n",
    "    - x (Tensor): The input tensor to be compressed, expected to have the shape (5, 44100) or similar.\n",
    "    - original_shape (tuple): The shape of the output tensor, default is (5, 40).\n",
    "    - step (int): The size of the block to average over, default is 4410.\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor: The compressed tensor with shape specified by `original_shape`.\n",
    "    \"\"\"\n",
    "    output_tensor = torch.zeros(original_shape)\n",
    "    for i in range(original_shape[1]):  # Iterate over the second dimension of the target shape\n",
    "        start_idx = i * step\n",
    "        end_idx = start_idx + step\n",
    "        # Take the mean of each block and assign it to the corresponding position in the output tensor\n",
    "        output_tensor[:, i] = x[:, start_idx:end_idx].mean(dim=1)\n",
    "    return output_tensor\n",
    "\n",
    "def tournament_selection(population, losses, tournament_size=3):\n",
    "    \"\"\"\n",
    "    Selects two parents using tournament selection.\n",
    "\n",
    "    Args:\n",
    "    - population (list of Tensors): The population from which to select parents.\n",
    "    - losses (list of floats): The loss associated with each individual in the population, serving as a measure of fitness.\n",
    "    - tournament_size (int): The number of individuals to sample for each tournament.\n",
    "\n",
    "    Returns:\n",
    "    - parent1, parent2 (tuple of Tensors): Two selected parents from the population.\n",
    "    \"\"\"\n",
    "    population_size = len(population)\n",
    "\n",
    "    # Tournament 1\n",
    "    indices = np.random.choice(range(population_size), size=tournament_size, replace=False)\n",
    "    tournament_losses = [losses[i] for i in indices]\n",
    "    winner_index = indices[np.argmin(tournament_losses)]\n",
    "    parent1 = population[winner_index]\n",
    "\n",
    "    # Tournament 2\n",
    "    indices = np.random.choice(range(population_size), size=tournament_size, replace=False)\n",
    "    tournament_losses = [losses[i] for i in indices]\n",
    "    winner_index = indices[np.argmin(tournament_losses)]\n",
    "    parent2 = population[winner_index]\n",
    "\n",
    "    return parent1, parent2\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    mask = torch.randint(0, 2, size=parent1.shape, dtype=torch.bool)\n",
    "    offspring = torch.where(mask, parent1, parent2)\n",
    "    return offspring\n",
    "\n",
    "def adaptive_mutation_rate(current_iteration, max_iterations, start_rate=0.75, end_rate=0.25):\n",
    "    \"\"\"\n",
    "    Calculates an adaptive mutation rate that decreases from start_rate to end_rate over time.\n",
    "\n",
    "    Args:\n",
    "    - current_iteration (int): The current iteration number (should start from 0).\n",
    "    - max_iterations (int): The total number of iterations the algorithm will run.\n",
    "    - start_rate (float): The initial mutation rate at the start of the algorithm.\n",
    "    - end_rate (float): The final mutation rate at the end of the algorithm.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated mutation rate for the current iteration.\n",
    "    \"\"\"\n",
    "    # Linear decay\n",
    "    rate = start_rate - ((start_rate - end_rate) * (current_iteration / max_iterations))\n",
    "    \n",
    "    # Ensure the rate never falls below the end_rate\n",
    "    return max(rate, end_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b97a63a5-d58e-47db-9768-e063efd5da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioData:\n",
    "    def __init__(self, audio):\n",
    "        self.audio = audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaf6345c-e063-4fe5-9bb3-15b218c080e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best(mixture_tensor_, drum_tensor_):\n",
    "    with torch.no_grad():\n",
    "        n_iters = 100\n",
    "        population_size = 32\n",
    "        batch_size = population_size\n",
    "        elite_size = 2  # Number of elites to carry over to the next generation\n",
    "        shape = (5,40)\n",
    "    \n",
    "        solution = torch.randn(shape).clamp(0, 1)\n",
    "        solution = torch.where(solution < 0.5, torch.tensor(1), torch.tensor(0))\n",
    "    \n",
    "        population = []\n",
    "    \n",
    "        for i in range(population_size - len(population)):\n",
    "            candidates = torch.randint_like(solution, low=0, high=1)\n",
    "            population.append(candidates)\n",
    "            \n",
    "        best_loss = 10000000000\n",
    "        best_solution = []\n",
    "    \n",
    "        for iteration in range(n_iters):\n",
    "            \n",
    "            losses = []\n",
    "            batch = []\n",
    "            mix = []\n",
    "            drums = []\n",
    "    \n",
    "            for j in range(batch_size):\n",
    "                try:\n",
    "                    proposed = torch.where(population[j] < 0.5, torch.tensor(1), torch.tensor(0))\n",
    "                    proposed = expand(proposed).unsqueeze(0)\n",
    "                    batch.append(proposed)\n",
    "                    mix.append(mixture_tensor_)\n",
    "                    drums.append(drum_tensor_)\n",
    "                except Exception as e:\n",
    "                    print('error', e)\n",
    "            \n",
    "            batch_candidates = torch.cat(batch, axis=0).to(model.device)\n",
    "            mix = torch.cat(mix, axis=0)\n",
    "            drums = torch.cat(drums, axis=0)\n",
    "            sep = model(mix, batch_candidates)\n",
    "    \n",
    "            for j in range(batch_size):\n",
    "                # try:\n",
    "                    # print(loss)\n",
    "                loss_item = nn.L1Loss()(sep[j, : ,:].unsqueeze(0), drum_tensor_).item()\n",
    "                losses.append(loss_item)\n",
    "                # except Exception as e:\n",
    "                #     print('error', e) \n",
    "    \n",
    "            sorted_indices = np.argsort(losses)\n",
    "            sorted_population = [population[i] for i in sorted_indices]\n",
    "            sorted_losses = [losses[i] for i in sorted_indices]\n",
    "    \n",
    "            # Update best solution if found\n",
    "            if sorted_losses[0] < best_loss:\n",
    "                best_loss = sorted_losses[0]\n",
    "                best_solution = sorted_population[0]\n",
    "                # print(f\"Iteration {iteration}, Loss: {best_loss}\")\n",
    "    \n",
    "    \n",
    "            # Elitism: Carry over the best solutions unchanged\n",
    "            new_population = sorted_population[:elite_size]\n",
    "            \n",
    "            # Fill the rest of the new population\n",
    "            while len(new_population) < population_size:\n",
    "                # Tournament selection for parent selection\n",
    "                parent1, parent2 = tournament_selection(sorted_population, sorted_losses)\n",
    "    \n",
    "                # Crossover to produce offspring\n",
    "                offspring1 = crossover(parent1, parent2)\n",
    "                offspring2 = crossover(parent2, parent1)\n",
    "    \n",
    "                # Adaptive mutation rate\n",
    "                mutation_rate = adaptive_mutation_rate(iteration, n_iters)\n",
    "    \n",
    "                # Mutation for offspring\n",
    "                for offspring in [offspring1, offspring2]:\n",
    "                    if len(new_population) < population_size:  # Check if there's still space in the new population\n",
    "                        if torch.rand(1) < mutation_rate:\n",
    "                            mutation = torch.randint(-1, 2, size=offspring.shape)\n",
    "                            mutated_offspring = offspring + mutation\n",
    "                            mutated_offspring = mutated_offspring.clamp(0, 1)\n",
    "                            new_population.append(mutated_offspring)\n",
    "    \n",
    "            # Update population for the next iteration\n",
    "            population = new_population\n",
    "    \n",
    "        return population[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74b2165f-3a25-4ffb-90f2-d8c1ad6879ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall_torch(transcription, prediction):\n",
    "    TPs = torch.sum((transcription == 1) & (prediction == 1), dim=1)\n",
    "    FPs = torch.sum((transcription == 0) & (prediction == 1), dim=1)\n",
    "    FNs = torch.sum((transcription == 1) & (prediction == 0), dim=1)\n",
    "\n",
    "    precision = TPs.float() / (TPs + FPs).float()\n",
    "    recall = TPs.float() / (TPs + FNs).float()\n",
    "\n",
    "    # Handle potential division by zero for precision and recall\n",
    "    precision[torch.isnan(precision)] = 0\n",
    "    recall[torch.isnan(recall)] = 0\n",
    "\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f9974d7-61e7-483f-83d0-733710827e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f_measure(precision, recall, beta=1):\n",
    "    \"\"\"\n",
    "    Calculate the F-measure for each class and the average F-measure.\n",
    "\n",
    "    Parameters:\n",
    "    - precision: Tensor of precision values per class.\n",
    "    - recall: Tensor of recall values per class.\n",
    "    - beta: Weight of recall in the harmonic mean.\n",
    "\n",
    "    Returns:\n",
    "    - f_measure: Tensor of F-measure for each class.\n",
    "    - average_f_measure: Scalar, average F-measure across all classes.\n",
    "    \"\"\"\n",
    "    numerator = (1 + beta**2) * precision * recall\n",
    "    denominator = (beta**2 * precision) + recall\n",
    "\n",
    "    # Avoid division by zero\n",
    "    denominator[denominator == 0] = 1\n",
    "\n",
    "    f_measure = numerator / denominator\n",
    "\n",
    "    # Handle potential NaN values\n",
    "    f_measure[torch.isnan(f_measure)] = 0\n",
    "\n",
    "    average_f_measure = torch.mean(f_measure)\n",
    "\n",
    "    return f_measure, average_f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ada2d-30b5-4c4f-b03f-3b3c069aa549",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'epoch_285'\n",
    "#try:\n",
    "\n",
    "out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_ht_{name}/\"\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "out_dir = f\"D:/Github/phd-drum-sep/model-as-adt/results_ht_{name}/adt/\"\n",
    "try:\n",
    "    os.mkdir(out_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = DrumDemucs.load_from_checkpoint(f'D:/Github/phd-drum-sep/analysis/demucs_small_model_analysis/checkpoint/{name}.ckpt')\n",
    "model = model.eval()\n",
    "\n",
    "results = museval.EvalStore(frames_agg='median', tracks_agg='median')\n",
    "for track in tqdm(all_tracks):\n",
    "\n",
    "    mixture_tensor = load_audio(track.mix_path).unsqueeze(0).to(model.device)\n",
    "    snippet_length = (mixture_tensor.shape[2] // (44100 * 4)) * (44100 * 4)\n",
    "    mixture_tensor = mixture_tensor[:,:, :snippet_length]\n",
    "\n",
    "    drum_tensor = load_audio(track.drum_path).unsqueeze(0)\n",
    "    drum_tensor = torch.cat([drum_tensor, drum_tensor], dim=1).to(model.device)\n",
    "    drum_tensor = drum_tensor[:,:, :snippet_length]\n",
    "\n",
    "    shape = mixture_tensor.shape[2]\n",
    "    roll_tensor = load_roll(track.midi_path, shape).unsqueeze(0).to(model.device)\n",
    "    roll_tensor = roll_tensor[:,:, :snippet_length]\n",
    "\n",
    "    proposed_answers = []\n",
    "    \n",
    "    device = mixture_tensor.device\n",
    "    batch, channels, length = mixture_tensor.shape\n",
    "    chunk_len = int(44100 * 4)\n",
    "\n",
    "    for start in tqdm(range(0, length, chunk_len)):\n",
    "        end = start + chunk_len\n",
    "        answer = find_best(mixture_tensor[:,:,start:end], drum_tensor[:,:, start:end])\n",
    "        proposed_answers.append(answer)\n",
    "\n",
    "    expanded = [expand(p).unsqueeze(0) for p in proposed_answers]\n",
    "    # expanded = torch.cat(expanded, dim=2)\n",
    "    pres = [[],[],[],[],[]]\n",
    "    recs = [[],[],[],[],[]]\n",
    "    for idx, val in enumerate(expanded):\n",
    "        segment = 44100 * 4\n",
    "        start = idx * segment\n",
    "        end = start + segment\n",
    "        slice = roll_tensor[:, :, start:end].to(model.device).squeeze(0)\n",
    "        pred = val.to(model.device).squeeze(0)\n",
    "        pre, rec = calculate_precision_recall_torch(slice, pred)\n",
    "    \n",
    "        for drum in range(5):\n",
    "            pres[drum].append(pre[drum].unsqueeze(0))\n",
    "            recs[drum].append(rec[drum].unsqueeze(0))\n",
    "            \n",
    "    for p in range(len(pres)):\n",
    "        for q in range(len(pres[p])):\n",
    "            try:\n",
    "                pres[p][q] = pres[p][q].item()\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    for p in range(len(recs)):\n",
    "        for q in range(len(recs[p])):\n",
    "            try:\n",
    "                recs[p][q] = recs[p][q].item()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(f'{out_dir}{track.name}')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df = pd.DataFrame(pres)\n",
    "    df = df.T\n",
    "    df.to_csv(f'{out_dir}{track.name}/precision.csv')\n",
    "\n",
    "    df = pd.DataFrame(recs)\n",
    "    df = df.T\n",
    "    df.to_csv(f'{out_dir}{track.name}/recall.csv')\n",
    "\n",
    "    for idx, val in enumerate(proposed_answers):\n",
    "        adf = pd.DataFrame(val.numpy())\n",
    "        adf.to_csv(f'{out_dir}{track.name}/{idx}.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b03b8-777f-432c-82d4-722e34f6dbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5005b-6cf4-4a3a-a7c3-71ef9cb2433a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb30c5a-f19d-4d83-8973-0cd32e8ba903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
